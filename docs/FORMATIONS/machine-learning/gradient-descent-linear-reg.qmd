---
title: "Mise en ≈ìuvre de l'algorithme de la descente de gradient : Cas de la r√©gression lin√©aire"
author: "Djamal TOE"
date: "June 8, 2025"
link-citations: true
subparagraph: yes
fig-cap-location: bottom
fig-caption: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE)
library(reticulate)
use_python("C:/Users/Djamal TOE/AppData/Local/Programs/Python/Python311")
```

```{r chargement, message=FALSE, echo=FALSE, include=FALSE, warning=FALSE}
rm(list=ls())

###--- package √† installer

packages <- c("dplyr", "kableExtra")

###--- Boucle pour installer et charger les packages
for (pkg in packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg, dependencies = T)
  }
  library(pkg, character.only = TRUE)
}

display_table <- function(data, cap_, nrow_ = 10){
  tab <- kable(head(data, nrow_), booktabs = TRUE, linesep = "", caption = cap_, align = c("l", rep("r", ncol(data))), format = "simple", escape = FALSE)
  return(tab)
}

```

```{python, echo = TRUE}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import math
plt.style.use('deeplearning.mplstyle')
```



## R√©sum√©

|       √Ä la suite de mes cours d'optimisation et de calcul num√©rique, j'ai souhait√© impl√©menter moi-m√™me l‚Äôalgorithme de la descente de gradient. J‚Äôai choisi comme premier cas d‚Äôapplication la **`r√©gression lin√©aire`**, dans le but d‚Äôestimer les param√®tres optimaux (`w`, `b`) qui minimisent la fonction de co√ªt (g√©n√©ralement l‚Äôerreur quadratique moyenne).

M√™me si, dans le cas de la r√©gression lin√©aire, il existe une solution analytique explicite par la m√©thode des moindres carr√©s, cette situation est id√©ale pour comprendre et tester l'efficacit√© de la descente de gradient. En revanche, pour des mod√®les plus complexes comme la r√©gression logistique, une solution analytique n‚Äôest plus disponible. Dans ces cas, on utilise syst√©matiquement des **`m√©thodes num√©riques`** comme la descente de gradient.

---

## Abstract

|       Following my coursework in optimization and numerical methods, I implemented the **`gradient descent algorithm`** from scratch, starting with the case of **`linear regression`**. While a closed-form solution exists for linear models, using gradient descent allows for a deeper understanding of iterative optimization. This approach becomes essential when working with more complex models, such as **`logistic regression`**, where no explicit analytical solution is available.

---

## Introduction

|       L‚Äôobjectif de ce document est de pr√©senter l‚Äôimpl√©mentation de l‚Äôalgorithme de descente de gradient dans le cas de la r√©gression lin√©aire multiple, afin de mieux comprendre le principe d‚Äôoptimisation it√©rative. On commence par un rappel de la r√©gression lin√©aire classique, avant de d√©river la fonction de co√ªt et ses gradients par rapport aux param√®tres. Ensuite, l‚Äôalgorithme est appliqu√© √† un jeu de donn√©es simul√© pour illustrer sa convergence et les effets du taux d‚Äôapprentissage.

Dans une deuxi√®me partie, nous discuterons des limites de l‚Äôapproche analytique, notamment dans des contextes o√π la descente de gradient devient incontournable (r√©gression logistique, r√©seaux de neurones, etc.).


---

## Un peu de formalisme

|       Nous souhaitons minimiser une fonction objective, appel√©e √©galement **`fonction de co√ªt`**. Avant de chercher une m√©thode num√©rique pour effectuer cette minimisation, il est essentiel de s‚Äôassurer que la fonction admet bien un minimum, et que celui-ci est **`unique`**.

En effet, une fonction peut pr√©senter **`plusieurs minima locaux`**, et l‚Äôobjectif est g√©n√©ralement d‚Äôatteindre le **`minimum global`**. Pour garantir l‚Äôexistence d‚Äôun minimum global, on fait appel au **`th√©or√®me de Weierstrass`**, qui stipule qu‚Äôune fonction continue sur un ensemble compact atteint un minimum (et un maximum).

Mais pour garantir l‚Äôunicit√© du minimum, on utilise la notion de **`convexit√©`**. Une fonction strictement convexe sur un domaine convexe poss√®de un unique minimum qui est donc le minimum global.  

> üí° Si votre fonction de co√ªt est **`concave`** (au lieu d‚Äô√™tre convexe), il suffit d‚Äôen prendre l‚Äôoppos√©. La maximisation d‚Äôune fonction concave revient √† minimiser son oppos√©e, qui sera convexe.

Ainsi, **la stricte convexit√©** de la fonction objective est une propri√©t√© cruciale : elle permet de garantir l‚Äôunicit√© du minimum et donc la convergence de l‚Äôalgorithme vers une solution bien d√©finie.

Je n'entrerai pas dans trop de details math√©matiques. Si vous voulez en savoir plus (condition d'application du th√©or√®me de weierstrass -> s√©mi-continuit√© + espace de contraintes born√© ou semi-continuit√© + coercivit√© + espace de contraintes ferm√©) consultez cette page : <<https://en.wikipedia.org/wiki/Extreme_value_theorem>> et faites des recherches suppl√©mentaires.


---

## Algorithme de la descente de gradient : cas de la regression lin√©aire

- **Fonction de co√ªt de la regression lin√©aire**

La fonction de co√ªt utilis√©e pour la r√©gression lin√©aire est la \textbf{fonction de co√ªt quadratique moyenne} (ou `Mean Squared Error`, MSE). Elle s'√©crit comme suit :

$$
\begin{equation}
    J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^2
\end{equation}
$$

o√π :

\begin{itemize}
    \item $m$ est le nombre d'exemples d'entra√Ænement,
    \item $y^{(i)}$ est la vraie √©tiquette de l'exemple $i$,
    \item $\hat{y}^{(i)} = w^T x^{(i)} + b$ est la pr√©diction du mod√®le pour l'exemple $i$,
    \item $w$ est le vecteur de poids, $b$ le biais.
\end{itemize}

\bigskip

En ***notation vectorielle***, on peut r√©√©crire la fonction de co√ªt sous la forme :

$$
\begin{equation}
    J(w, b) = \frac{1}{2m} (\hat{Y} - Y)^T (\hat{Y} - Y)
\end{equation}
$$

o√π :

\begin{itemize}
    \item $\hat{Y} = Xw + b$ est le vecteur des pr√©dictions (de taille $m \times 1$),
    \item $Y$ est le vecteur des vraies valeurs (de taille $m \times 1$),
    \item la norme au carr√© peut s‚Äô√©crire :
\end{itemize}

$$
\begin{equation}
    J(w, b) = \frac{1}{2m} \left\| \hat{Y} - Y \right\|^2
\end{equation}
$$

- üîª **Descente de Gradient**

L‚Äôobjectif de la descente de gradient est de d√©terminer les param√®tres optimaux $w$ et $b$ qui minimisent la fonction de co√ªt. Pour cela, on utilise un algorithme **it√©ratif** qui met progressivement √† jour ces param√®tres **dans le sens oppos√© au gradient**.

√Ä chaque it√©ration, les nouvelles valeurs de $w$ et $b$ doivent id√©alement conduire √† une **diminution de la fonction de co√ªt**. Si la fonction **augmente**, cela peut √™tre d√ª √† :

- un **taux d‚Äôapprentissage** ($\alpha$) trop **√©lev√©**, provoquant une **divergence** ;
- une **erreur de code**, par exemple un mauvais calcul du gradient.

En revanche, si la fonction de co√ªt diminue **trop lentement**, cela peut signifier :

- un taux d‚Äôapprentissage trop **faible**, causant une **convergence lente** ou incompl√®te.

> ‚ö†Ô∏è Dans certains cas, la fonction de co√ªt peut **onduler** √† chaque it√©ration (oscillations), souvent √† cause d‚Äôune mauvaise normalisation ou d‚Äôun $\alpha$ mal ajust√©.

---

## üîé V√©rification de la convergence

Pour s‚Äôassurer que l‚Äôalgorithme converge correctement, on peut :

- **Tracer graphiquement** la valeur de la fonction de co√ªt √† chaque it√©ration.
- Fixer un **seuil de tol√©rance** :

$$
\text{Si } \|\nabla J(w, b)\| < \varepsilon, \text{ alors on arr√™te l'algorithme.}
$$

---

## ‚öôÔ∏è Mise √† jour des param√®tres

La descente de gradient met √† jour **simultan√©ment** les poids et le biais selon la r√®gle :

$$
w := w - \alpha \cdot \frac{\partial J(w, b)}{\partial w}
$$

$$
b := b - \alpha \cdot \frac{\partial J(w, b)}{\partial b}
$$

o√π :

- $J(w, b)$ est la fonction de co√ªt,
- $\alpha$ est le **learning rate**.

---

## üß™ Remarques pratiques

- Un **taux d‚Äôapprentissage dynamique** (adaptatif) peut am√©liorer la convergence.
- Des techniques comme **momentum**, **RMSprop** ou **Adam** sont des variantes plus stables.


```{python, echo=TRUE, fig.align='center', out.width='90%', out.height='80%'}
#| label: fig-fiting
#| fig-cap: Exemple d'apprentissage

# G√©n√©ration des it√©rations
np.random.seed(42)
iterations = np.arange(0, 100)

# Simulations de comportements de co√ªt
convergence_bonne = np.exp(-0.05 * iterations) + 0.05 * np.random.randn(100)
divergence = 0.1 * iterations + 0.5 * np.random.rand(100)

# Trac√©
plt.figure(figsize=(8, 4))
plt.plot(iterations, convergence_bonne, label='Bonne convergence', color='green', linewidth=1.2)
plt.plot(iterations, divergence, label='Divergence', color='red', linewidth=1.2)

plt.xlabel("It√©rations")
plt.ylabel("Co√ªt simul√©")
plt.title("Diff√©rents comportements de la fonction de co√ªt", fontsize=12)
plt.legend()
plt.tight_layout()
plt.show()

```

::: {.callout-note title="Descente de Gradient et R√©gularisation"}

L‚Äô**algorithme de descente de gradient** est une m√©thode d‚Äôoptimisation utilis√©e pour ajuster les param√®tres d‚Äôun mod√®le en minimisant une fonction de co√ªt. Il repose sur le calcul du gradient (ou pente) de cette fonction par rapport aux param√®tres, et sur une mise √† jour it√©rative jusqu‚Äô√† convergence.

Cependant, en pratique, on rencontre souvent le **probl√®me de surapprentissage (overfitting)**, notamment lorsque :

- Le nombre de variables est √©lev√© par rapport au nombre d‚Äôobservations.
- Certaines variables explicatives n‚Äôapportent que peu ou pas d'information utile.
- Le mod√®le devient trop complexe, capturant le bruit au lieu du signal.

Pour pallier cela, plusieurs strat√©gies existent :

- üîÑ **Augmenter la taille du jeu de donn√©es** : plus de donn√©es permet de mieux g√©n√©raliser.
- üß† **S√©lectionner judicieusement les variables** : par des techniques comme le *feature selection*, on garde seulement les plus pertinentes.
- üõ°Ô∏è **Appliquer une r√©gularisation** (comme Lasso ou Ridge) : on p√©nalise la complexit√© du mod√®le pour √©viter l‚Äôajustement excessif.

üëâ Ces aspects seront peut-√™tre explor√©s plus en d√©tail dans une prochaine publication √† travers un **mod√®le de r√©gression logistique appliqu√© √† des donn√©es r√©elles**, o√π la s√©lection de variables et la r√©gularisation joueront un r√¥le central.

:::

::: {.callout-tip title="La R√©gression Polynomiale"}

La **r√©gression polynomiale** est une extension de la r√©gression lin√©aire o√π l'on introduit des puissances suppl√©mentaires des variables explicatives pour capturer des relations **non lin√©aires** entre les variables.

Exemple :

$$
y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \dots + w_d x^d + \varepsilon
$$

Cela permet au mod√®le de s‚Äôadapter √† des courbes complexes, mais augmente √©galement le **risque de surapprentissage**. Plus le degr√© \( d \) est √©lev√©, plus le mod√®le est flexible, mais moins il g√©n√©ralise bien si les donn√©es ne sont pas suffisantes.

‚û°Ô∏è Il est donc **essentiel de combiner cette approche avec des techniques de validation crois√©e et de r√©gularisation**, pour trouver le bon compromis entre biais et variance.

:::


## Applications

|       Dans cette section, nous coommen√ßons d'abord par simuler un jeu de donn√©es synth√©tiques repr√©sentant des individus caract√©ris√©s par leur √¢ge, leur niveau d'√©ducation, leur sexe, ainsi que leur salaire annuel. Le salaire est g√©n√©r√© √† partir d‚Äôun mod√®le probabiliste bas√© sur des salaires de base associ√©s √† chaque niveau d'√©ducation, auxquels s‚Äôajoute un effet lin√©aire de l‚Äô√¢ge, et une part d‚Äôal√©a simul√©e √† l‚Äôaide d‚Äôune distribution normale.

```{python, echo = TRUE}
#| label: data-gen


# generating data
# fixing generator seed
np.random.seed(42)

# number of observation
size = 1000

# creating age variable
ages = np.random.randint(low=21, high=66, size=size)

# creating education level variable
levels = ['BAC', 'Bachelor degree', 'Msc', 'PhD']
educ_level = np.random.choice(levels, size=size, p=[0.1, 0.2, 0.3, 0.4])

# creating sexe variable
sexes = ['M', 'F']
sexe = np.random.choice(sexes, size=size)


# creating annual salary (Euro) based on the different informations above
base_salary = {
    'BAC': 25000,
    'Bachelor degree': 30000,
    'Msc': 45000,
    'PhD': 75000
}

salary = np.asarray([
  round(np.random.normal(base_salary[edu_lvl] + (age*2), 100)) 
  for edu_lvl, age in zip(educ_level, ages)
])

df = pd.DataFrame(
    {
        'salary': salary,
        'ages': ages,
        'sex': sexe,
        'education': educ_level
    }
)
```


- **Algorithme du calcul de la fonction de co√ªt**

```{python, echo = TRUE}
#| label: cost-fun


def compute_cost_fn(x:np.ndarray, y:np.ndarray, w:np.array, b:float) -> float:
    """
        Calculates the cost function (MSE divided by 2)
        for multivariate linear regression.

        Args:
            x (np.ndarray): Matrix of explanatory variables (shape: [m, n])
            y (np.ndarray): Vector of target values (shape: [m,])
            w (np.ndarray): Weight vector (shape: [n,])
            b (float): Bias (scalar)

        Returns:
            float: Value of the cost function
    """
    m = x.shape[0]
    y_hat = x@w + b
    square_error = np.sum((y - y_hat)**2)
    cost_fn = square_error/(2*m)
    return cost_fn
```


- **Algorithme du calcul du gradient de la fonction de co√ªt**


```{python, echo = TRUE}
#| label: gradient-fun

def compute_gradient(x:np.ndarray, y:np.ndarray, w:np.array, b:float) -> tuple:
    """_summary_

    Args:
        x (np.ndarray): _description_
        y (np.ndarray): _description_
        w (np.array): _description_
        b (float): _description_

    Returns:
        tuple: _description_
    """
    m = x.shape[0]
    y_hat = x @ w + b
    square_error = y_hat - y
    fw_prime = (x.T @ square_error)/m
    fb_prime = np.sum(square_error)/m
    return fw_prime, fb_prime
```


- **Algorithme de la descente de gradient**

```{python, echo = TRUE}
#| label: gradient-descent-fun

from typing import Callable
from copy import deepcopy
def gradient_descent(
    x: np.ndarray,
    y: np.ndarray,
    alpha: float,
    w_in: np.array,
    b_in: float,
    max_iter: int,
    tolerance: float,
    cost_fn: Callable,
    gradient_compute_fn: Callable) -> dict:
    """
    Performs gradient descent to adjust
    the parameters w and b.

    Args:
        x (np.ndarray): Input data (m, n)
        y (np.ndarray): Target (m,)
        w_in (np.ndarray): Initial weight (n,)
        b_in (float): Initial bias
        alpha (float): Learning rate
        max_iter (int): Maximum number of iterations
        tolerance (float): Convergence threshold
        cost_fn (Callable): Cost function
        gradient_compute_fn (Callable): Gradient calculation function

    Returns:
        dict: History of parameters and cost at each iteration
    """

    w = deepcopy(w_in)
    b = b_in
    cost_fn_and_params_hist = {}
    for i in range(max_iter):
        fw_i, fb_i = gradient_compute_fn(x, y, w, b)
        w = w - alpha*fw_i
        b = b - alpha*fb_i
        cost_fn_i = cost_fn(x, y, w, b)
        cost_fn_and_params_hist[i] = {
            'w': w.copy(),
            'b': b,
            'cost_fn': cost_fn_i
        }
        if i % 15000 == 0: # vous pouvez ajuster: moi je ne veux pas afficher toutes les it√©rations
            print(
                f'Iteraion {i} : \n|(w, b) = ({w}, {b:4f}) | cost_fn = {cost_fn_i:4f}|'
            )
        if np.linalg.norm(fw_i) < tolerance and abs(fb_i) < tolerance:
            print(
                f'The algorithm converges at the {i}-th iteration'
                f'\nThen we have (w, b) = ({w}, {b:4f}) & cost_fn = {cost_fn_i:4f}'
                )
            break
    return cost_fn_and_params_hist
```


---


## Analyse exploratoire (rapide)

Ici on veut predire le salaire d'un individu en fonction de son niveau d'√©ducation, de son √¢ge et de son sexe.

- **Premi√®res lignes de la table**

```{python, echo=TRUE}
#| label: first-row
df_5 = df.loc[:5, :]
```

```{r, echo=TRUE}
#| label: tbl-first
#| tbl-cap-location: top

df_5_r = py$df_5
display_table(df_5_r, "Les premi√®res lignes de notre base de donn√©es", nrow_ = 5)
```



- **R√©sum√© statistique de la table**

```{python, echo=TRUE}
#| label: statistical-summary

df.describe() # affcihe le r√©sum√© statistique des variables num√©riques
```

- **Distribution du salaire en fonction des features**

```{python, echo=TRUE}
#| label: features-selection

X_features = df.drop('salary', axis=1).columns.to_list()
X_features
```


```{python, fig.align='center', echo= TRUE, out.width='90%', out.height='80%'}
#| label: fig-dist-plot
#| fig-cap: Distribution des variables en fonction du salaire

fig, axes = plt.subplots(1, 3, figsize = (12, 8))
for i in range(len(axes)):
    axes[i].scatter(y=df['salary'], x=df[df.columns.to_list()[i + 1]])
    axes[i].set_xlabel(X_features[i].capitalize(), fontsize=10)
    axes[i].set_ylabel('Salary', size=10)
    axes[i].set_xlabel(X_features[i].capitalize(), size=10)
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].set_title(f'Relation between salary and {X_features[i].capitalize()}', size=10)
plt.legend()
plt.tight_layout()
plt.show()

```


|       De prime abord on pour se dire qu'il existe un lien non lin√©aire entre les variables, ce qui n'est pas totalement juste car ... 


---


## Data preprocessing

::: {.callout-important}
## Code pour partitionner les donn√©es

La fonction suivante permet de diviser vos donn√©es en ensembles d‚Äôentra√Ænement et de test pour la validation du mod√®le. Il est important de s‚Äôassurer que les deux ensembles contiennent toutes les cat√©gories ou modalit√©s des variables cat√©gorielles (***indice : stratification***). La fonction ci-dessous ne prend pas cela en charge automatiquement. Cependant, gr√¢ce √† la taille de notre jeu de donn√©es et √† l‚Äôutilisation de la graine al√©atoire fix√©e √† 42, nous pouvons garantir que ces caract√©ristiques sont bien pr√©sentes dans les deux ensembles.
:::

```{.python}
>>> train['education'].unique()
array(['Bachelor degree', 'PhD', 'Msc', 'BAC'], dtype=object)
>>> test['education'].unique()
array(['Msc', 'PhD', 'BAC', 'Bachelor degree'], dtype=object)
>>>
```


```{python, echo = TRUE}
#| label: data-partition

def create_data_partition(df, train_ratio):
    """
        Creates a random partition of the DataFrame into training and test sets.

        Args:
            df (pd.DataFrame): the complete DataFrame
            train_ratio (float): the proportion of rows to be used for training (ex: 0.8)

        Returns:
            tuple: (train_df, test_df)
    """
    nrow_df = df.shape[0]
    nrow_train = round(nrow_df*train_ratio)
    train_idx = np.random.choice(df.index, size=nrow_train, replace=False)
    test_idx = df.index.difference(train_idx)
    train_df = df.iloc[train_idx]
    test_df = df.iloc[test_idx]

    return (train_df, test_df)

```

|       Une fois les donn√©es simul√©es, nous proc√©dons √† leur s√©paration en deux sous-ensembles distincts : un ensemble d'entra√Ænement, utilis√© pour ajuster le mod√®le, et un ensemble de test, destin√© √† l‚Äô√©valuer. La fonction `create_data_partition` r√©alise une partition al√©atoire selon un ratio d√©fini (ici 80% pour l‚Äôentra√Ænement, 20% pour le test). Ensuite, les variables explicatives (`ages`, `education`, `sex`) sont trait√©es via une pipeline de pr√©traitement : les variables num√©riques sont standardis√©es (centr√©es-r√©duites) tandis que les variables cat√©gorielles sont transform√©es en indicatrices (encodage one-hot, sans la premi√®re modalit√©). Enfin, la variable cible (`salary`) est √©galement standardis√©e pour assurer une convergence efficace de l'algorithme de descente de gradient.


```{python, echo = TRUE}
#| label: data-ing

# data preprocessing

# geting train and test datasets
np.random.seed(42)
train, test = create_data_partition(df, 0.8)


# selecting features
X = train.drop('salary', axis=1)

# selecting target variable
y = train['salary']

# standardization of the target variable
scaler_y = StandardScaler()
onehot_encoder_educ = OneHotEncoder()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

# one-hot encodings & standardization : pipeline treatment
numeric_features = ['ages']
categorical_features = ['education', 'sex']

preprocessor = ColumnTransformer(transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(drop='first'), categorical_features)
])

# transform numerical values by standardize them and categorical one by dummy them
X_processed = preprocessor.fit_transform(X=X)
```

:::{.callout-important}
## ‚ö†Ô∏è Important : `fit_transform()` vs `transform()`

Toujours utiliser `fit_transform()` **uniquement sur les donn√©es d'entra√Ænement**, et `transform()` sur les donn√©es de validation ou de test.

Cela s‚Äôapplique √† **tous les types de preprocessing**, notamment :

- üü¶ **StandardScaler** : la moyenne et l‚Äô√©cart-type doivent √™tre appris sur le *train* uniquement.
- üüß **OneHotEncoder** : les cat√©gories doivent √™tre identifi√©es √† partir du *train* et appliqu√©es de mani√®re coh√©rente au *test*.

‚ùå Ne jamais faire `fit_transform()` sur le test, car cela introduit du **data leakage** (les donn√©es de test influencent le mod√®le).
:::

---

## Application de l'algorithme de la descente de gradient

|       Comme mentionn√© pr√©c√©demment, le choix du `learning rate` (ou taux d‚Äôapprentissage) est un param√®tre crucial dans l‚Äôalgorithme de descente de gradient. Un taux trop √©lev√© peut emp√™cher la convergence du mod√®le, tandis qu‚Äôun taux trop faible peut rendre l‚Äôapprentissage extr√™mement lent. Afin d‚Äôidentifier un taux optimal, plusieurs valeurs sont test√©es, et celle qui permet de minimiser au mieux la fonction de co√ªt est retenue.


```{python, echo=TRUE}
#| label: run-gradient


alphas = [1e-1, 1e-2] # , 1e-3, 1e-4, 1e-5 √† ajouter si vous voulez, je veux juste reduire l'affichage
n_features = X_processed.shape[1]
w = np.zeros(n_features)
b = np.random.randint(100, size=1)[0]
b = int(b)
tolerance = 1e-3
max_iters = 50000
cost_fn = compute_cost_fn
gradient_fn = compute_gradient
all_histories = {}
for alpha in alphas:
    print(f'Runing the algorithm for alpha : {alpha}\n')
    w = np.zeros(n_features)
    b = int(np.random.randint(100, size=1)[0])
    history = gradient_descent(
        X_processed,
        y_scaled,
        alpha=alpha,
        w_in=w,
        b_in=b,
        max_iter=max_iters,
        tolerance=tolerance,
        cost_fn=cost_fn,
        gradient_compute_fn=gradient_fn)
    print('\n')
    all_histories[str(alpha)] = history
```

|       Ici, on constate directement que notre algorithme a converg√© (crit√®re de *tol√©rance*) pour chacun des taux d‚Äôapprentissage (`learning rate`) test√©s.

Apr√®s cet entra√Ænement, il est int√©ressant de d√©terminer quel `learning rate` a permis d‚Äôobtenir le meilleur r√©sultat. Le code suivant est particuli√®rement utile lorsqu‚Äôon teste plusieurs valeurs de `learning rate` (plus de deux).


```{python, echo=TRUE}
#| label: see-good-alpha

best_alpha = None
min_cost = float('inf')

for alpha, hist in all_histories.items():
    costs = [v['cost_fn'] for v in hist.values()]
    min_cost_alpha = min(costs)
    print(f"Alpha {alpha} ‚û§ Min cost: {min_cost_alpha:.5f}")
    if min_cost_alpha < min_cost:
        min_cost = min_cost_alpha
        best_alpha = alpha

print(f"\n‚úÖ Best alpha: {best_alpha} with min cost: {min_cost:.2f}")
```

Avec ces deux `learning rates`, la fonction de co√ªt atteint 0, ce qui signifie que l‚Äôalgorithme a bien converg√© puisque la fonction de co√ªt est toujours positive.


```{python, echo=TRUE}
#| label: get-good-alpha

def get_best_w_b(cost_fn_and_params_hist: dict):
    hist_len = len(cost_fn_and_params_hist)
    best_of_hist = cost_fn_and_params_hist[hist_len-1]
    return best_of_hist
  
def predict(w, b, X):
    return X @ w + b
  
hist = all_histories['0.01']
best = get_best_w_b(hist)
```

|       On peut maintenant calculer nos pr√©dictions et v√©rifier leur qualit√© par rapport aux donn√©es r√©elles. Rappelons que les donn√©es ont √©t√© standardis√©es, il faudra donc ramener les pr√©dictions √† leur √©chelle d‚Äôorigine pour une interpr√©tation correcte.


```{python, echo=TRUE}
#| label: getting-pred

# predictions standardis√©
ypred_scaled = predict(best['w'], best['b'], X_processed).reshape((-1, 1))
print(f'Donn√©es standardis√©es (3 premi√®res valeurs): {ypred_scaled[:4]}\n\n')

# echelle normale
y_pred = scaler_y.inverse_transform(ypred_scaled)
print(f'Donn√©es avec leur √©chelle normale (3 premi√®res valeurs): {y_pred[:4]}\n\n')

# affichange des parametres optimaux
print(f'Param√®tres optimaux : {best}')
best
```


```{python, fig.align='center', echo= TRUE, out.width='90%', out.height='80%'}
#| label: fig-plot-prediction
#| fig-cap: Predcitions vs Real data

fig, axes = plt.subplots(1, 3, figsize = (12, 8))
for i in range(len(axes)):
    axes[i].scatter(train[X_features[i]], train['salary'], label='target')
    axes[i].scatter(train[X_features[i]], y_pred, c='orange', label='predicted')
    axes[i].set_ylabel('Salary', size=10)
    axes[i].set_xlabel(X_features[i].capitalize(), size=10)
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].set_title(f'Relation between salary and {X_features[i].capitalize()}', size=10)
plt.legend()
plt.tight_layout()
plt.show()
```



|       On peut voir que l'ajustement est tr√®s satisfaisant, mais il reste  √† d√©terminer s'il ne s'agit pas d'un `surapprentissage`. Pour cela nous allons utiliser les donn√©es de test, pour √©valuer le mod√®le entrain√©. 

## Evaluation du mod√®le

|       Pour √©valuer le mod√®le nous utilisons la **validation crois√©e** car elle est robuste en terme d'√©valuation de performance d'un mod√®le.

- **But principal** : √âvaluer la performance d‚Äôun mod√®le de mani√®re fiable et robuste, en r√©duisant le biais d√ª √† une simple s√©paration train/test.

- **√âtape 1 : Partition des donn√©es**  
  Diviser l‚Äôensemble des donn√©es en *k* sous-ensembles (ou ¬´ folds ¬ª) de taille √† peu pr√®s √©gale.

- **√âtape 2 : Boucle sur les folds**  
  Pour chaque fold (de 1 √† k) :  
  - Utiliser ce fold comme ensemble de test (validation).  
  - Utiliser les *k-1* autres folds comme ensemble d‚Äôentra√Ænement.

- **√âtape 3 : Entra√Ænement**  
  Entra√Æner le mod√®le uniquement sur les donn√©es d‚Äôentra√Ænement (les *k-1* folds).

- **√âtape 4 : √âvaluation**  
  Tester le mod√®le entra√Æn√© sur le fold de test (le fold laiss√© de c√¥t√©), calculer une m√©trique de performance (ex : erreur quadratique moyenne).

- **√âtape 5 : Agr√©gation**  
  R√©p√©ter les √©tapes 2 √† 4 pour chaque fold, puis calculer la moyenne (et √©ventuellement l‚Äô√©cart-type) des performances obtenues sur chaque fold.

- **Avantages** :  
  - Meilleure estimation de la g√©n√©ralisation du mod√®le sur des donn√©es nouvelles.  
  - R√©duit le sur-apprentissage li√© √† un seul d√©coupage train/test.  
  - Utilise efficacement toutes les donn√©es pour entra√Ænement et validation.

- **Inconv√©nients** :  
  - Co√ªt computationnel plus √©lev√©, car le mod√®le est entra√Æn√© *k* fois.  
  - Peut √™tre sensible au choix de *k* (souvent 5 ou 10).



```{r, fig.align='center', echo= FALSE, out.width='90%', out.height='80%'}
#| label: fig-kfold-illustration
#| fig-cap: Illustration du processus de la validation crois√©e

knitr::include_graphics('k_fold.png')
```




```{python, echo = FALSE}
#| label: gradient-descent-fun-without-print

from typing import Callable
from copy import deepcopy
def gradient_descent(
    x: np.ndarray,
    y: np.ndarray,
    alpha: float,
    w_in: np.array,
    b_in: float,
    max_iter: int,
    tolerance: float,
    cost_fn: Callable,
    gradient_compute_fn: Callable) -> dict:
    """
    Effectue la descente de gradient pour ajuster
    les param√®tres w et b.

    Args:
        x (np.ndarray): Donn√©es d'entr√©e (m, n)
        y (np.ndarray): Cible (m,)
        w_in (np.ndarray): Poids initial (n,)
        b_in (float): Biais initial
        alpha (float): Taux d'apprentissage
        max_iter (int): Nombre maximal d'it√©rations
        tolerance (float): Seuil de convergence
        cost_fn (Callable): Fonction de co√ªt
        gradient_compute_fn (Callable): Fonction de calcul du gradient

    Returns:
        dict: Historique des param√®tres et co√ªt √† chaque it√©ration
    """

    w = deepcopy(w_in)
    b = b_in
    cost_fn_and_params_hist = {}
    for i in range(max_iter):
        fw_i, fb_i = gradient_compute_fn(x, y, w, b)
        w = w - alpha*fw_i
        b = b - alpha*fb_i
        cost_fn_i = cost_fn(x, y, w, b)
        cost_fn_and_params_hist[i] = {
            'w': w.copy(),
            'b': b,
            'cost_fn': cost_fn_i
        }
        if np.linalg.norm(fw_i) < tolerance and abs(fb_i) < tolerance:
            print(
                f'The algorithm converges at the {i}-th iteration'
                f'\nThen we have (w, b) = ({w}, {b:4f}) & cost_fn = {cost_fn_i:4f}'
                )
            break
    return cost_fn_and_params_hist
```


```{python, echo=TRUE}
#| label: k-fold

def k_fold_cross_validation(df, k, alpha, max_iters, tolerance, cost_fn, gradient_fn):
    """
    Perform k-fold cross-validation using gradient descent.

    Args:
        df (pd.DataFrame): full dataset (training set)
        k (int): number of folds
        alpha (float): learning rate
        max_iters (int): maximum number of iterations for gradient descent
        tolerance (float): convergence threshold
        cost_fn (Callable): cost function
        gradient_fn (Callable): gradient computation function

    Returns:
        list: list of cost values on each fold's test set
    """
    # Shuffle the DataFrame index
    df = df.sample(frac=1).reset_index(drop=True)
    fold_size = len(df) // k
    sqrt_costs = []
    
    for fold in range(k):
        # Define start and end indices for the test fold
        start = fold * fold_size
        if fold != k - 1:
            end = (fold + 1) * fold_size
        else:  # last fold takes the remaining data
            end = len(df)
        
        # Split the data into test and train folds
        test_df = df.iloc[start:end]
        train_df = pd.concat([df.iloc[:start], df.iloc[end:]], axis=0)
        
        # Prepare features and target for train and test
        X_train = train_df.drop('salary', axis=1)
        y_train = train_df['salary']
        X_test = test_df.drop('salary', axis=1)
        y_test = test_df['salary']
        
        print('-'*10)
        print(f"\nFold {fold+1}:")
        print("Train unique education:", train_df['education'].unique())
        print("Test unique education:", test_df['education'].unique())
        print("Train unique sex:", train_df['sex'].unique())
        print("Test unique sex:", test_df['sex'].unique())
        print()
        
        
        
        numeric_features = ['ages']
        categorical_features = ['education', 'sex']

        preprocessor = ColumnTransformer(transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(drop='first'), categorical_features)
        ])
        
        # Preprocess the data using our pipeline
        X_train_processed = preprocessor.fit_transform(X_train)
        y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()
        
        X_test_processed = preprocessor.transform(X_test)
        y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()
        
        # Initialize parameters
        n_features = X_train_processed.shape[1]
        w = np.zeros(n_features)
        b = 0
        
        # Run gradient descent on training data
        history = gradient_descent(
            X_train_processed,
            y_train_scaled,
            alpha=alpha,
            w_in=w,
            b_in=b,
            max_iter=max_iters,
            tolerance=tolerance,
            cost_fn=cost_fn,
            gradient_compute_fn=gradient_fn
        )
        
        # Retrieve best parameters from last iteration
        last_iter = max(history.keys())
        w_best = history[last_iter]['w']
        b_best = history[last_iter]['b']
        
        # Compute root mean square cost on the test fold
        cost_test = np.sqrt(cost_fn(X_test_processed, y_test_scaled, w_best, b_best))
        print(f'Fold {fold + 1}/{k} - Test fold square root cost: {np.sqrt(cost_test):.5f}')
        print('-'*10)
        print('\n'*3)
        sqrt_costs.append(np.sqrt(cost_test))
    
    print(f'\nAverage root mean square of the cost func over {k} folds: {np.mean(sqrt_costs):.5f}')
    
    return sqrt_costs

```


```{python, echo = TRUE}
#| label: testing-kfold model

np.random.seed(42)
k = 5
alpha = 0.01
max_iters = 10000
tolerance = 1e-4

costs = k_fold_cross_validation(train, k, alpha, max_iters, tolerance, compute_cost_fn, compute_gradient)
```

|       On peut remaquer qu'√† chaque fold nos modalit√©s sont pr√©sentes et dans les donn√©es de test et dans celles de l'apprentissage. 


La validation crois√©e en 5 plis (K-Fold Cross-Validation) a √©t√© utilis√©e pour √©valuer la performance du mod√®le. Les valeurs ci-dessous correspondent √† la racine carr√©e de la fonction de co√ªt (Root Mean Square Error, RMSE), **calcul√©e sur des donn√©es de sortie standardis√©es**.

### R√©sultats par pli

- ‚úÖ Fold 1 : RMSE = 0.15381  
- ‚úÖ Fold 2 : RMSE = 0.13553  
- ‚úÖ Fold 3 : RMSE = 0.13553  
- ‚úÖ Fold 4 : RMSE = 0.18426  
- ‚úÖ Fold 5 : RMSE = 0.14942  

**Moyenne des RMSE sur les 5 plis** : `0.15442`


### Interpr√©tation

- Le mod√®le a √©t√© test√© sur 5 sous-ensembles diff√©rents, ce qui donne une estimation plus robuste de sa performance.

- La **RMSE moyenne de 0.15442** signifie que, **en moyenne, les pr√©dictions s‚Äô√©cartent de la v√©rit√© d‚Äôenviron 0.15 unit√© sur l‚Äô√©chelle standardis√©e**.

- La **faible variabilit√©** des scores sugg√®re que le mod√®le reste globalement stable.

### Visualisation des erreurs par pli

```{python, fig.align='center', echo= TRUE, out.width='90%', out.height='80%'}
#| label: fig-rmse-value-plot
#| fig-cap: RMSE par pli (validation crois√©e 5-fold)

# RMSE values
rmse_values = [0.15381, 0.14906, 0.13553, 0.18426, 0.14942]
folds = [f'Fold {i+1}' for i in range(5)]
rmse_mean = sum(rmse_values) / len(rmse_values)

plt.figure(figsize=(12, 8))
bars = plt.bar(folds, rmse_values, color='#3182bd')
plt.axhline(y=rmse_mean, color='r', linestyle='--', linewidth=1.5, label=f'Mean RMSE = {rmse_mean:.5f}')
plt.title('RMSE by fold (crossed validation 5-fold)', fontsize=10)
plt.ylabel('RMSE (standardized)', fontsize=10)
plt.xlabel('Fold number',fontsize=10)
plt.ylim(0, max(rmse_values) * 1.15)

# Ajouter les valeurs au-dessus de chaque barre
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.005, f'{yval:.3f}', ha='center', va='bottom')

plt.legend(fontsize=10)
plt.tight_layout()
plt.show()
```

## Evaluation du mod√®le final sur les donn√©es de test


```{python, echo=TRUE}
#| label: testing-final model

# selecting test features
X_test = test.drop('salary', axis=1)

# transforming test features (one hot encodings and standardization)
X_test_processed = preprocessor.transform(X=X_test)

# predictions on standardized test features
y_pred_test_scaled = predict(best['w'], best['b'], X_test_processed).reshape((-1, 1))

# reverse scaling
y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled).reshape(200,)


# getting real y and changing the shape
y_true_test = test['salary'].values.reshape(200,)
```

Maintenant calculons le `RMSE` des erreurs commises en pr√©disant. D'abord essayons de visualiser cela.
D'abord de mani√®re s√©par√©e car il me vient √† l'id√©e d'ordonner les diff√©rents arrays obtenus, mais avant faut que je sois s√ªre que les salaires r√©els (pas dans le sens √©conomique du terme mais pour dire salaire observ√©) et ceux pr√©dits ont la m√™me allure.

```{python, fig.align='center', echo= TRUE, out.width='90%', out.height='80%'}
#| label: fig-testing-final-plot
#| fig-cap: 'Observed Slaries and Predicted Salaries visualisation'

fig, axes = plt.subplots(1, 2, figsize=(12, 8))
abs_ = np.arange(1, 201, 1)

axes[0].plot(abs_, y_true_test, c='g', label='Observed salaries', linewidth=1)
axes[0].set_title('Observed Salaries', fontsize=10)
axes[0].set_xlabel('Data points', fontsize=10)
axes[0].set_ylabel('Salary', fontsize=10)

axes[1].plot(abs_, y_pred_test, c='b', label='Predicted salaries', linewidth=1)
axes[1].set_title('Predicted Salaries', fontsize=10)
axes[1].set_xlabel('Data points', fontsize=10)
axes[1].set_ylabel('Salary', fontsize=10)

plt.tight_layout()
plt.show()
```

|       En effet les deux courbe ont pratiquement la m√™me allure, essayons de voir ce que √ßa donne quand on les arrange de mani√®re croissante.


```{python, fig.align='center', echo= TRUE, out.width='90%', out.height='80%'}
#| label: fig-testing-final-sorted-plot
#| fig-cap: 'Observed Slaries VS Predicted Salaries'


y_pred_test_sorted = np.sort(y_pred_test)
y_true_test_sorted = np.sort(y_true_test)
plt.figure(figsize=(12, 8))
abs_ = np.arange(1, 201, 1)
plt.plot(abs_,y_true_test_sorted , c='g', label='Observed values of the salary', linewidth=1)
plt.plot(abs_, y_pred_test_sorted, c='b',label='Predicted values of the salary', linewidth=1)
plt.ylabel('Salary')
plt.xlabel('Data points')
plt.legend(fontsize=10)
plt.tight_layout()
plt.show()
```

::: {.callout-note}
## Analyse graphique des salaires pr√©dits vs observ√©s

Les courbes des salaires pr√©dits et observ√©s montrent une forte similarit√©.  
Cela sugg√®re que le mod√®le est capable de bien capturer la relation entre les variables explicatives et le salaire, indiquant ainsi une **bonne capacit√© de g√©n√©ralisation** sur les donn√©es de test.
:::


- **Calcul de la racine de l'erreur quadratique moyenne**

```{python, echo = TRUE}
#| label: testing-final-rmse
# RMSE sur les salaires standardis√©s

# RMSE sur les salaires r√©els
rmse_original = np.sqrt(np.mean((y_pred_test - y_true_test)**2))
print(f"original RMSE : {rmse_original:.2f}")

```


- **Interpr√©tation de la RMSE en valeur r√©elle**

|       Les salaires dans notre dataset varient entre environ **24 933 ‚Ç¨** et **75 318 ‚Ç¨**, ce qui explique qu‚Äôune `RMSE` d‚Äôenviron 284 ‚Ç¨ soit coh√©rente.

Pour mieux comprendre ce que signifie cette erreur moyenne, consid√©rons l‚Äô√©chelle des salaires :  
la diff√©rence entre le minimum et le maximum est d‚Äôenviron 60 000 ‚Ç¨.

Ainsi, une `RMSE` de `284 ‚Ç¨` correspond √† une erreur moyenne relative d‚Äôenviron :

$$
\frac{284}{50 385} \approx 0.0056 \quad \text{soit} \quad 0{,}56\%
$$

---

## Conclusion g√©n√©rale

L‚Äôanalyse men√©e √† travers la g√©n√©ration, la mod√©lisation et la validation crois√©e de notre dataset salarial montre que le mod√®le d√©velopp√© est performant et robuste.

- La **validation crois√©e en 5 plis** a permis d‚Äô√©valuer la stabilit√© du mod√®le, avec une **RMSE moyenne standardis√©e autour de 0.15**, ce qui indique une bonne capacit√© de g√©n√©ralisation sur des donn√©es non vues.

- En ramenant la RMSE √† l‚Äô√©chelle r√©elle des salaires, on obtient une erreur moyenne d‚Äôenviron **284 ‚Ç¨**, soit **0,56 %** de la moyenne salariale. Cette pr√©cision relative est tr√®s satisfaisante compte tenu de la variabilit√© naturelle des salaires.

- La convergence rapide de l‚Äôalgorithme de descente de gradient, test√©e sur plusieurs valeurs du taux d‚Äôapprentissage, montre que le mod√®le est bien param√©tr√© et que la fonction de co√ªt est efficacement minimis√©e.

- Il est important de noter que pour √©viter le ph√©nom√®ne d‚Äô**overfitting**, plusieurs pistes peuvent √™tre explor√©es dans de futurs travaux, telles que l‚Äôaugmentation de la taille des donn√©es, une s√©lection judicieuse des variables explicatives, ou l‚Äôintroduction de r√©gularisations dans le mod√®le.

- Enfin, ce travail pose les bases solides pour des mod√®les plus avanc√©s, notamment la r√©gression logistique pour des probl√®mes de classification, ou des r√©gressions polynomiales pour capter des relations non lin√©aires, qui seront explor√©s dans des publications futures.

En somme, ce projet illustre clairement l‚Äôimportance d‚Äôune pr√©paration rigoureuse des donn√©es, d‚Äôun choix appropri√© des hyperparam√®tres, et d‚Äôune validation syst√©matique pour construire des mod√®les pr√©dictifs fiables et interpr√©tables. J'ai √©t√© ravis d'avoir passer 20 h √† pr√©parer et √©crire cette publication, cela m'a permis de consolid√© mes acquis en **Python**, **Machine Learning (Th√©orie+ pratique)** et en rigeur m√©thodologique.

---

## Remerciements et retour d‚Äôexp√©rience

J‚Äôai √©t√© ravi d‚Äôavoir consacr√© pr√®s de **20 heures** √† la pr√©paration et √† la r√©daction de cette publication. 

Cet effort m‚Äôa permis de :

- consolider mes acquis en **Python**,  
- approfondir mes connaissances en **Machine Learning**, tant sur le plan **th√©orique que pratique**,  
- renforcer ma **rigueur m√©thodologique** dans le traitement des donn√©es, l‚Äôexp√©rimentation et l‚Äôanalyse des r√©sultats.

Ce projet m‚Äôa √©galement offert une excellente opportunit√© de structurer une d√©marche compl√®te de mod√©lisation, depuis la g√©n√©ration des donn√©es jusqu‚Äô√† l‚Äôinterpr√©tation finale des performances du mod√®le.

Je suis enthousiaste √† l‚Äôid√©e de poursuivre cette exploration dans de futures publications de ce genre (travailler √† la mano), notamment sur des cas r√©els avec des mod√®les plus avanc√©s comme la **r√©gression logistique** ou des approches **r√©gularis√©es**.

Mon cours d'optimisation et de m√©thode de calcul num√©riques m'a √©t√© d'une grande utilit√© en 1A √† l'ENSAI.


---

## Annexes

### 1. Standardisation des variables

|       La **standardisation** permet de centrer et r√©duire les variables num√©riques pour qu‚Äôelles aient une moyenne nulle et un √©cart-type unitaire.

Pour une variable continue $x$, la standardisation est donn√©e par :

$$
x_{\text{std}} = \frac{x - \mu_x}{\sigma_x}
$$

o√π :

- $\mu_x = \dfrac{1}{n} \sum_{i=1}^n x_i$ est la moyenne,
- $\sigma_x = \sqrt{\dfrac{1}{n} \sum_{i=1}^n (x_i - \mu_x)^2}$ est l‚Äô√©cart-type.

Apr√®s avoir entra√Æn√© le mod√®le sur les donn√©es standardis√©es, on peut revenir √† l‚Äô√©chelle r√©elle avec :

$$
\hat{y} = \hat{y}_{\text{std}} \cdot \sigma_y + \mu_y
$$

o√π $\mu_y$ et $\sigma_y$ sont la moyenne et l‚Äô√©cart-type de la variable cible $y$.

---

### 2. Codage One-Hot

Le **One-Hot Encoding** transforme une variable cat√©gorielle √† $k$ modalit√©s en $k$ colonnes binaires.

Soit une variable cat√©gorielle :

$$
\text{cat} \in \{c_1, c_2, \ldots, c_k\}
$$

On cr√©e un vecteur :

$$
\mathbf{v} = (v_1, v_2, \ldots, v_k) \quad \text{o√π} \quad
v_j =
\begin{cases}
1 & \text{si } \text{cat} = c_j \\
0 & \text{sinon}
\end{cases}
$$

Afin d‚Äô√©viter la redondance, on supprime une modalit√© (ex. via `drop='first'`) pour √©viter le pi√®ge des variables muettes (dummy variable trap).

---

### 3. Calcul de la RMSE

La **Root Mean Square Error** (RMSE) mesure l'√©cart quadratique moyen entre les valeurs pr√©dites $\hat{y}_i$ et observ√©es $y_i$ :

$$
\text{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 }
$$

En notation vectorielle, si $\mathbf{y}$ est le vecteur des valeurs observ√©es et $\hat{\mathbf{y}}$ celui des valeurs pr√©dites :

$$
\text{RMSE} = \sqrt{ \frac{1}{n} (\mathbf{y} - \hat{\mathbf{y}})^T (\mathbf{y} - \hat{\mathbf{y}}) }
$$

Cette m√©trique est exprim√©e dans l‚Äôunit√© de la variable cible (ici, les euros), ce qui la rend facile √† interpr√©ter dans un contexte r√©el.

---

Ces op√©rations sont fondamentales dans toute pipeline de traitement pour la r√©gression ou tout autre algorithme supervis√©.

