---
title: "Mise en ≈ìuvre de l'algorithme de la descente de gradient : Cas de la r√©gression lin√©aire"
author: "Djamal TOE"
date: "June 8, 2025"
link-citations: true
subparagraph: yes
fig-cap-location: bottom
fig-caption: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE)
library(reticulate)
use_python("C:/Users/Djamal TOE/AppData/Local/Programs/Python/Python311")
```

```{r chargement, message=FALSE, echo=FALSE, include=FALSE, warning=FALSE}
rm(list=ls())

###--- package √† installer

packages <- c("dplyr", "kableExtra")

###--- Boucle pour installer et charger les packages
for (pkg in packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg, dependencies = T)
  }
  library(pkg, character.only = TRUE)
}

display_table <- function(data, cap_, nrow_ = 10){
  tab <- kable(head(data, nrow_), booktabs = TRUE, linesep = "", caption = cap_, align = c("l", rep("r", ncol(data))), format = "simple", escape = FALSE)
  return(tab)
}

```

```{python, echo = TRUE}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import math
plt.style.use('deeplearning.mplstyle')
```



## R√©sum√©

|       √Ä la suite de mes cours d'optimisation et de calcul num√©rique, j'ai souhait√© impl√©menter moi-m√™me l‚Äôalgorithme de la descente de gradient. J‚Äôai choisi comme premier cas d‚Äôapplication la **`r√©gression lin√©aire`**, dans le but d‚Äôestimer les param√®tres optimaux (`w`, `b`) qui minimisent la fonction de co√ªt (g√©n√©ralement l‚Äôerreur quadratique moyenne).

M√™me si, dans le cas de la r√©gression lin√©aire, il existe une solution analytique explicite par la m√©thode des moindres carr√©s, cette situation est id√©ale pour comprendre et tester l'efficacit√© de la descente de gradient. En revanche, pour des mod√®les plus complexes comme la r√©gression logistique, une solution analytique n‚Äôest plus disponible. Dans ces cas, on utilise syst√©matiquement des **`m√©thodes num√©riques`** comme la descente de gradient.
En pratique, il faut s‚Äôassurer que les donn√©es soient r√©parties de mani√®re √©quilibr√©e selon des crit√®res comme le sexe ou l‚Äô√©ducation entre les jeux d‚Äôapprentissage, de validation (avec la validation crois√©e) et de test, pour √©viter que l‚Äô√©valuation du mod√®le soit fauss√©e. 

---

## Abstract

|       Following my courses in optimisation and numerical computation, I wanted to implement the gradient descent algorithm myself. My first application was to linear regression, with the aim of estimating the optimal parameters (`w`, `b`) that minimise the cost function (generally the mean square error).

Even though, in the case of linear regression, there is an explicit analytical solution using the method of least squares, this situation is ideal for understanding and testing the effectiveness of gradient descent. However, for more complex models such as logistic regression, an analytical solution is no longer available. In these cases, numerical methods such as gradient descent are systematically used.
In practice, care must be taken to ensure that the data is distributed evenly according to criteria such as gender or education between the training, validation (with cross-validation) and test sets, to avoid distorting the evaluation of the model.

---

## Introduction

|       L‚Äôobjectif de ce document est de pr√©senter l‚Äôimpl√©mentation de l‚Äôalgorithme de descente de gradient dans le cas de la r√©gression lin√©aire multiple, afin de mieux comprendre le principe d‚Äôoptimisation it√©rative. On commence par un rappel de la r√©gression lin√©aire classique, avant de d√©river la fonction de co√ªt et ses gradients par rapport aux param√®tres. Ensuite, l‚Äôalgorithme est appliqu√© √† un jeu de donn√©es simul√© pour illustrer sa convergence et les effets du taux d‚Äôapprentissage.

Dans une deuxi√®me partie, nous discuterons des limites de l‚Äôapproche analytique, notamment dans des contextes o√π la descente de gradient devient incontournable (r√©gression logistique, r√©seaux de neurones, etc.).


---

## Un peu de formalisme

|       Nous souhaitons minimiser une fonction objective, appel√©e √©galement **`fonction de co√ªt`**. Avant de chercher une m√©thode num√©rique pour effectuer cette minimisation, il est essentiel de s‚Äôassurer que la fonction admet bien un minimum, et que celui-ci est **`unique`**.

En effet, une fonction peut pr√©senter **`plusieurs minima locaux`**, et l‚Äôobjectif est g√©n√©ralement d‚Äôatteindre le **`minimum global`**. Pour garantir l‚Äôexistence d‚Äôun minimum global, on fait appel au **`th√©or√®me de Weierstrass`**, qui stipule qu‚Äôune fonction continue sur un ensemble compact atteint un minimum (et un maximum).

Mais pour garantir l‚Äôunicit√© du minimum, on utilise la notion de **`convexit√©`**. Une fonction strictement convexe sur un domaine convexe poss√®de un unique minimum qui est donc le minimum global.  

> üí° Si votre fonction de co√ªt est **`concave`** (au lieu d‚Äô√™tre convexe), il suffit d‚Äôen prendre l‚Äôoppos√©. La maximisation d‚Äôune fonction concave revient √† minimiser son oppos√©e, qui sera convexe.

Ainsi, **la stricte convexit√©** de la fonction objective est une propri√©t√© cruciale : elle permet de garantir l‚Äôunicit√© du minimum et donc la convergence de l‚Äôalgorithme vers une solution bien d√©finie.

Je n'entrerai pas dans trop de details math√©matiques. Si vous voulez en savoir plus (condition d'application du th√©or√®me de weierstrass -> s√©mi-continuit√© + espace de contraintes born√© ou semi-continuit√© + coercivit√© + espace de contraintes ferm√©) consultez cette page : <<https://en.wikipedia.org/wiki/Extreme_value_theorem>> et faites des recherches suppl√©mentaires.


---

## Algorithme de la descente de gradient : cas de la regression lin√©aire

- **Fonction de co√ªt de la regression lin√©aire**

La fonction de co√ªt utilis√©e pour la r√©gression lin√©aire est la \textbf{fonction de co√ªt quadratique moyenne} (ou `Mean Squared Error`, MSE). Elle s'√©crit comme suit :

$$
\begin{equation}
    J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^2
\end{equation}
$$

o√π :

\begin{itemize}
    \item $m$ est le nombre d'exemples d'entra√Ænement,
    \item $y^{(i)}$ est la vraie √©tiquette de l'exemple $i$,
    \item $\hat{y}^{(i)} = w^T x^{(i)} + b$ est la pr√©diction du mod√®le pour l'exemple $i$,
    \item $w$ est le vecteur de poids, $b$ le biais.
\end{itemize}

\bigskip

En ***notation vectorielle***, on peut r√©√©crire la fonction de co√ªt sous la forme :

$$
\begin{equation}
    J(w, b) = \frac{1}{2m} (\hat{Y} - Y)^T (\hat{Y} - Y)
\end{equation}
$$

o√π :

\begin{itemize}
    \item $\hat{Y} = Xw + b$ est le vecteur des pr√©dictions (de taille $m \times 1$),
    \item $Y$ est le vecteur des vraies valeurs (de taille $m \times 1$),
    \item la norme au carr√© peut s‚Äô√©crire :
\end{itemize}

$$
\begin{equation}
    J(w, b) = \frac{1}{2m} \left\| \hat{Y} - Y \right\|^2
\end{equation}
$$

- üîª **Descente de Gradient**

L‚Äôobjectif de la descente de gradient est de d√©terminer les param√®tres optimaux $w$ et $b$ qui minimisent la fonction de co√ªt. Pour cela, on utilise un algorithme **it√©ratif** qui met progressivement √† jour ces param√®tres **dans le sens oppos√© au gradient**.

√Ä chaque it√©ration, les nouvelles valeurs de $w$ et $b$ doivent id√©alement conduire √† une **diminution de la fonction de co√ªt**. Si la fonction **augmente**, cela peut √™tre d√ª √† :

- un **taux d‚Äôapprentissage** ($\alpha$) trop **√©lev√©**, provoquant une **divergence** ;
- une **erreur de code**, par exemple un mauvais calcul du gradient.

En revanche, si la fonction de co√ªt diminue **trop lentement**, cela peut signifier :

- un taux d‚Äôapprentissage trop **faible**, causant une **convergence lente** ou incompl√®te.

> ‚ö†Ô∏è Dans certains cas, la fonction de co√ªt peut **onduler** √† chaque it√©ration (oscillations), souvent √† cause d‚Äôune mauvaise normalisation ou d‚Äôun $\alpha$ mal ajust√©.

---

## üîé V√©rification de la convergence

Pour s‚Äôassurer que l‚Äôalgorithme converge correctement, on peut :

- **Tracer graphiquement** la valeur de la fonction de co√ªt √† chaque it√©ration.
- Fixer un **seuil de tol√©rance** :

$$
\text{Si } \|\nabla J(w, b)\| < \varepsilon, \text{ alors on arr√™te l'algorithme.}
$$

---

## ‚öôÔ∏è Mise √† jour des param√®tres

La descente de gradient met √† jour **simultan√©ment** les poids et le biais selon la r√®gle :

$$
w := w - \alpha \cdot \frac{\partial J(w, b)}{\partial w}
$$

$$
b := b - \alpha \cdot \frac{\partial J(w, b)}{\partial b}
$$

o√π :

- $J(w, b)$ est la fonction de co√ªt,
- $\alpha$ est le **learning rate**.

---

## üß™ Remarques pratiques

- Un **taux d‚Äôapprentissage dynamique** (adaptatif) peut am√©liorer la convergence.
- Des techniques comme **momentum**, **RMSprop** ou **Adam** sont des variantes plus stables.


```{python, echo=FALSE, fig.align='center', out.width='90%', out.height='80%'}
#| label: fig-fiting
#| fig-cap: Learning example

# Generating iterations
np.random.seed(42)
iterations = np.arange(0, 100)

# Simulation of cost behaviour
convergence_bonne = np.exp(-0.05 * iterations) + 0.05 * np.random.randn(100)
divergence = 0.1 * iterations + 0.5 * np.random.rand(100)

# Layout
plt.figure(figsize=(8, 4))
plt.plot(iterations, convergence_bonne, label='Good convergence', color='green', linewidth=1.2)
plt.plot(iterations, divergence, label='Divergence', color='red', linewidth=1.2)

plt.xlabel("Iterations")
plt.ylabel("Simulated cost")
plt.title("Different behaviours of the cost function", fontsize=12)
plt.legend()
plt.tight_layout()
plt.show()

```

::: {.callout-note title="Descente de Gradient et R√©gularisation"}

L‚Äô**algorithme de descente de gradient** est une m√©thode d‚Äôoptimisation utilis√©e pour ajuster les param√®tres d‚Äôun mod√®le en minimisant une fonction de co√ªt. Il repose sur le calcul du gradient (ou pente) de cette fonction par rapport aux param√®tres, et sur une mise √† jour it√©rative jusqu‚Äô√† convergence.

Cependant, en pratique, on rencontre souvent le **probl√®me de surapprentissage (overfitting)**, notamment lorsque :

- Le nombre de variables est √©lev√© par rapport au nombre d‚Äôobservations.
- Certaines variables explicatives n‚Äôapportent que peu ou pas d'information utile.
- Le mod√®le devient trop complexe, capturant le bruit au lieu du signal.

Pour pallier cela, plusieurs strat√©gies existent :

- üîÑ **Augmenter la taille du jeu de donn√©es** : plus de donn√©es permet de mieux g√©n√©raliser.
- üß† **S√©lectionner judicieusement les variables** : par des techniques comme le *feature selection*, on garde seulement les plus pertinentes.
- üõ°Ô∏è **Appliquer une r√©gularisation** (comme Lasso ou Ridge) : on p√©nalise la complexit√© du mod√®le pour √©viter l‚Äôajustement excessif.

üëâ Ces aspects seront peut-√™tre explor√©s plus en d√©tail dans une prochaine publication √† travers un **mod√®le de r√©gression logistique appliqu√© √† des donn√©es r√©elles**, o√π la s√©lection de variables et la r√©gularisation joueront un r√¥le central.

:::

::: {.callout-tip title="La R√©gression Polynomiale"}

La **r√©gression polynomiale** est une extension de la r√©gression lin√©aire o√π l'on introduit des puissances suppl√©mentaires des variables explicatives pour capturer des relations **non lin√©aires** entre les variables.

Exemple :

$$
y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \dots + w_d x^d + \varepsilon
$$

Cela permet au mod√®le de s‚Äôadapter √† des courbes complexes, mais augmente √©galement le **risque de surapprentissage**. Plus le degr√© \( d \) est √©lev√©, plus le mod√®le est flexible, mais moins il g√©n√©ralise bien si les donn√©es ne sont pas suffisantes.

‚û°Ô∏è Il est donc **essentiel de combiner cette approche avec des techniques de validation crois√©e et de r√©gularisation**, pour trouver le bon compromis entre biais et variance.

:::


## Applications

|       Dans cette section, nous coommen√ßons d'abord par simuler un jeu de donn√©es synth√©tiques repr√©sentant des individus caract√©ris√©s par leur √¢ge, leur niveau d'√©ducation, leur sexe, ainsi que leur salaire annuel. Le salaire est g√©n√©r√© √† partir d‚Äôun mod√®le probabiliste bas√© sur des salaires de base associ√©s √† chaque niveau d'√©ducation, auxquels s‚Äôajoute un effet lin√©aire de l‚Äô√¢ge, et une part d‚Äôal√©a simul√©e √† l‚Äôaide d‚Äôune distribution normale.

```{python, echo = TRUE}
#| label: data-gen


# generating data
# fixing generator seed
np.random.seed(42)

# number of observation
size = 1000

# creating age variable
ages = np.random.randint(low=21, high=66, size=size)

# creating education level variable
levels = ['BAC', 'Bachelor degree', 'Msc', 'PhD']
educ_level = np.random.choice(levels, size=size, p=[0.1, 0.2, 0.3, 0.4])

# creating sexe variable
sexes = ['M', 'F']
sexe = np.random.choice(sexes, size=size)


# creating annual salary (Euro) based on the different informations above
base_salary = {
    'BAC': 25000,
    'Bachelor degree': 30000,
    'Msc': 45000,
    'PhD': 75000
}

salary = np.asarray([
  round(np.random.normal(base_salary[edu_lvl] + (age*2), 100)) 
  for edu_lvl, age in zip(educ_level, ages)
])

df = pd.DataFrame(
    {
        'salary': salary,
        'ages': ages,
        'sex': sexe,
        'education': educ_level
    }
)
```


- **Algorithme du calcul de la fonction de co√ªt**

```{python, echo = TRUE}
#| label: cost-fun


def compute_cost_fn(x:np.ndarray, y:np.ndarray, w:np.array, b:float) -> float:
    """
        Calculates the cost function (MSE divided by 2)
        for multivariate linear regression.

        Args:
            x (np.ndarray): Matrix of explanatory variables (shape: [m, n])
            y (np.ndarray): Vector of target values (shape: [m,])
            w (np.ndarray): Weight vector (shape: [n,])
            b (float): Bias (scalar)

        Returns:
            float: Value of the cost function
    """
    m = x.shape[0]
    y_hat = x@w + b
    square_error = np.sum((y - y_hat)**2)
    cost_fn = square_error/(2*m)
    return cost_fn
```


- **Algorithme du calcul du gradient de la fonction de co√ªt**


```{python, echo = TRUE}
#| label: gradient-fun

def compute_gradient(x:np.ndarray, y:np.ndarray, w:np.array, b:float) -> tuple:
    """
    Calculates the gradient of the cost function (MSE) with respect to weights w and bias b.

    Args:
        x (np.ndarray): Matrix of input features of form (m, n),
        where m is the number of observations
                        and n is the number of explanatory variables.
        y (np.ndarray): Vector of target values (m, ).
        w (np.ndarray): Weight vector (n, ).
        b (float): Scalar bias.

    Returns:
        tuple: A pair (fw_prime, fb_prime) containing :
            - fw_prime (np.ndarray): The gradient of the cost function with respect to w, 
            of the form (n, ).
            - fb_prime (float): The gradient of the cost function with respect to b.
    """
    m = x.shape[0]
    y_hat = x @ w + b
    square_error = y_hat - y
    fw_prime = (x.T @ square_error)/m
    fb_prime = np.sum(square_error)/m
    return fw_prime, fb_prime
```


- **Algorithme de la descente de gradient**

```{python, echo = TRUE}
#| label: gradient-descent-fun

from typing import Callable
from copy import deepcopy
def gradient_descent(
    x: np.ndarray,
    y: np.ndarray,
    alpha: float,
    w_in: np.array,
    b_in: float,
    max_iter: int,
    tolerance: float,
    cost_fn: Callable,
    gradient_compute_fn: Callable) -> dict:
    """
    Performs gradient descent to adjust
    the parameters w and b.

    Args:
        x (np.ndarray): Input data (m, n)
        y (np.ndarray): Target (m,)
        w_in (np.ndarray): Initial weight (n,)
        b_in (float): Initial bias
        alpha (float): Learning rate
        max_iter (int): Maximum number of iterations
        tolerance (float): Convergence threshold
        cost_fn (Callable): Cost function
        gradient_compute_fn (Callable): Gradient calculation function

    Returns:
        dict: History of parameters and cost at each iteration
    """

    w = deepcopy(w_in)
    b = b_in
    cost_fn_and_params_hist = {}
    for i in range(max_iter):
        fw_i, fb_i = gradient_compute_fn(x, y, w, b)
        w = w - alpha*fw_i
        b = b - alpha*fb_i
        cost_fn_i = cost_fn(x, y, w, b)
        cost_fn_and_params_hist[i] = {
            'w': w.copy(),
            'b': b,
            'cost_fn': cost_fn_i
        }
        if i % 15000 == 0: # you can adjust: I don't want to display all the iterations
            print(
                f'Iteraion {i} : \n|(w, b) = ({w}, {b:4f}) | cost_fn = {cost_fn_i:4f}|'
            )
        if np.linalg.norm(fw_i) < tolerance and abs(fb_i) < tolerance:
            print(
                f'The algorithm converges at the {i}-th iteration'
                f'\nThen we have (w, b) = ({w}, {b:4f}) & cost_fn = {cost_fn_i:4f}'
                )
            break
    return cost_fn_and_params_hist
```


---

## üìä Analyse exploratoire : Relation entre salaire et variables explicatives

Ici on veut predire le salaire d'un individu en fonction de son niveau d'√©ducation, de son √¢ge et de son sexe.

- **Premi√®res lignes de la table**

```{python, echo=TRUE}
#| label: first-row
df_5 = df.loc[:5, :]
```

```{r, echo=TRUE}
#| label: tbl-first
#| tbl-cap: The first lines of our database
#| tbl-cap-location: top

df_5_r = py$df_5
display_table(df_5_r, "", nrow_ = 5)
```



- **R√©sum√© statistique de la table**

```{python, echo=TRUE}
#| label: statistical-summary

df.describe() # displays the statistical summary of numerical variables
```

- **Distribution du salaire en fonction des features**

```{python, echo=TRUE}
#| label: features-selection

X_features = df.drop('salary', axis=1).columns.to_list()
X_features
```


```{python, fig.align='center', echo= TRUE, out.width='90%', out.height='80%'}
#| label: fig-dist-plot
#| fig-cap: Distribution of variables as a function of salary

fig, axes = plt.subplots(1, 3, figsize = (12, 8))
for i in range(len(axes)):
    axes[i].scatter(y=df['salary'], x=df[df.columns.to_list()[i + 1]])
    axes[i].set_xlabel(X_features[i].capitalize(), fontsize=10)
    axes[i].set_ylabel('Salary', size=10)
    axes[i].set_xlabel(X_features[i].capitalize(), size=10)
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].set_title(f'Relation between salary and {X_features[i].capitalize()}', size=10)
plt.legend()
plt.tight_layout()
plt.show()

```


### üë¥ Relation entre le salaire et l'√¢ge

- **üîç Observation :**
  - Les salaires sont regroup√©s autour de quelques valeurs fixes (25k, 30k, 45k, 75k), ind√©pendamment de l'√¢ge.
  - La distribution semble en **paliers**, pas de tendance lin√©aire visible.

- **üí° Interpr√©tation :**
  - Il **n‚Äôexiste pas de relation lin√©aire claire** entre l‚Äô√¢ge et le salaire.
  - L‚Äô√¢ge **n‚Äôest pas un facteur explicatif majeur** du salaire dans cet √©chantillon.
  - üëâ Il faudrait explorer d'autres variables comme l‚Äô`√©ducation`, ect..

---

### üöª Relation entre le salaire et le sexe

- **üîç Observation :**
  - Les salaires des femmes (`F`) et des hommes (`M`) sont r√©partis de mani√®re similaire.
  - Aucune **diff√©rence salariale flagrante** n‚Äôappara√Æt sur ce graphique.

- **üí° Interpr√©tation :**
  - Le **sexe ne semble pas avoir d‚Äôinfluence directe** sur le salaire ici.
  - üìå Une analyse plus fine (ex. tests statistiques) serait n√©cessaire pour confirmer l‚Äôabsence d‚Äô√©cart significatif (mais ce n'est pas le but ici, [Cliquez ici pour voir une publication dans laquelle je r√©alise des tests statistiques afin de selectionner les variables essentielles pour la sp√©cification de mon mod√®le](https://djamal2905.github.io/djamal_website/INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html)).

---

### üéì Relation entre le salaire et le niveau d'√©ducation

- **üîç Observation :**
  - Le salaire **augmente avec le niveau de dipl√¥me** :
    - PhD üßëüî¨ > MSc üë®üéì > Bachelor üë®üè´ > BAC üéí
  - Cette progression est claire et **ordonn√©e**.

- **üí° Interpr√©tation :**
  - Le **niveau d‚Äô√©ducation est un facteur pr√©dictif fort** du salaire.
  - Il existe une **relation croissante et logique** entre dipl√¥me obtenu et r√©mun√©ration.

---

### ‚úÖ Conclusion de l'analyse exploratoire rapide

> üß† **De prime abord**, on pourrait penser qu‚Äôil existe une relation non lin√©aire entre les variables explicatives et le salaire.  
> En r√©alit√©, seule la variable **üéì niveau d'√©ducation** montre une **relation coh√©rente et croissante**.  
> Ni **üë¥ l‚Äô√¢ge**, ni **üöª le sexe** ne pr√©sentent d‚Äôinfluence claire sur le salaire.  
> Cette analyse souligne l'importance d‚Äôune **exploration visuelle et statistique** avant toute mod√©lisation.


---


## Data preprocessing

::: {.callout-important}
## Code pour partitionner les donn√©es

La fonction suivante permet de diviser vos donn√©es en ensembles d‚Äôentra√Ænement et de test pour la validation du mod√®le. Il est important de s‚Äôassurer que les deux ensembles contiennent toutes les cat√©gories ou modalit√©s des variables cat√©gorielles (***indice : stratification***). La fonction ci-dessous ne prend pas cela en charge automatiquement. Cependant, gr√¢ce √† la taille de notre jeu de donn√©es et √† l‚Äôutilisation de la graine al√©atoire fix√©e √† 42, nous pouvons garantir que ces caract√©ristiques sont bien pr√©sentes dans les deux ensembles.
:::

```{.python}
>>> train['education'].unique()
array(['Bachelor degree', 'PhD', 'Msc', 'BAC'], dtype=object)
>>> test['education'].unique()
array(['Msc', 'PhD', 'BAC', 'Bachelor degree'], dtype=object)
>>>
```


```{python, echo = TRUE}
#| label: data-partition

def create_data_partition(df, train_ratio):
    """
        Creates a random partition of the DataFrame into training and test sets.

        Args:
            df (pd.DataFrame): the complete DataFrame
            train_ratio (float): the proportion of rows to be used for training (ex: 0.8)

        Returns:
            tuple: (train_df, test_df)
    """
    nrow_df = df.shape[0]
    nrow_train = round(nrow_df*train_ratio)
    train_idx = np.random.choice(df.index, size=nrow_train, replace=False)
    test_idx = df.index.difference(train_idx)
    train_df = df.iloc[train_idx]
    test_df = df.iloc[test_idx]

    return (train_df, test_df)

```


|       Une fois les donn√©es simul√©es, nous proc√©dons √† leur s√©paration en deux sous-ensembles distincts : un ensemble d'entra√Ænement, utilis√© pour ajuster le mod√®le, et un ensemble de test, destin√© √† l‚Äô√©valuer. La fonction `create_data_partition` r√©alise une partition al√©atoire selon un ratio d√©fini (ici 80% pour l‚Äôentra√Ænement, 20% pour le test). Ensuite, les variables explicatives (`ages`, `education`, `sex`) sont trait√©es via une pipeline de pr√©traitement : les variables num√©riques sont standardis√©es (centr√©es-r√©duites) tandis que les variables cat√©gorielles sont transform√©es en indicatrices (encodage one-hot, sans la premi√®re modalit√©). Enfin, la variable cible (`salary`) est √©galement standardis√©e pour assurer une convergence efficace de l'algorithme de descente de gradient.


```{python, echo = TRUE}
#| label: data-ing

# data preprocessing

# geting train and test datasets
np.random.seed(42)
train, test = create_data_partition(df, 0.8)


# selecting features
X = train.drop('salary', axis=1)

# selecting target variable
y = train['salary']

# standardization of the target variable
scaler_y = StandardScaler()
onehot_encoder_educ = OneHotEncoder()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

# one-hot encodings & standardization : pipeline treatment
numeric_features = ['ages']
categorical_features = ['education', 'sex']

preprocessor = ColumnTransformer(transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(drop='first'), categorical_features)
])

# transform numerical values by standardize them and categorical one by dummy them
X_processed = preprocessor.fit_transform(X=X)
```

:::{.callout-important}
## ‚ö†Ô∏è Important : `fit_transform()` vs `transform()`

Toujours utiliser `fit_transform()` **uniquement sur les donn√©es d'entra√Ænement**, et `transform()` sur les donn√©es de validation ou de test.

Cela s‚Äôapplique √† **tous les types de preprocessing**, notamment :

- üü¶ **StandardScaler** : la moyenne et l‚Äô√©cart-type doivent √™tre appris sur le *train* uniquement.
- üüß **OneHotEncoder** : les cat√©gories doivent √™tre identifi√©es √† partir du *train* et appliqu√©es de mani√®re coh√©rente au *test*.

‚ùå Ne jamais faire `fit_transform()` sur le test, car cela introduit du **data leakage** (les donn√©es de test influencent le mod√®le).
:::

---

## Application de l'algorithme de la descente de gradient

|       Comme mentionn√© pr√©c√©demment, le choix du `learning rate` (ou taux d‚Äôapprentissage) est un param√®tre crucial dans l‚Äôalgorithme de descente de gradient. Un taux trop √©lev√© peut emp√™cher la convergence du mod√®le, tandis qu‚Äôun taux trop faible peut rendre l‚Äôapprentissage extr√™mement lent. Afin d‚Äôidentifier un taux optimal, plusieurs valeurs sont test√©es, et celle qui permet de minimiser au mieux la fonction de co√ªt est retenue.


```{python, echo=TRUE}
#| label: run-gradient


alphas = [1e-1, 1e-2] # , 1e-3, 1e-4, 1e-5 √† ajouter si vous voulez, je veux juste reduire l'affichage
n_features = X_processed.shape[1]
w = np.zeros(n_features)
b = np.random.randint(100, size=1)[0]
b = int(b)
tolerance = 1e-3
max_iters = 50000
cost_fn = compute_cost_fn
gradient_fn = compute_gradient
all_histories = {}
for alpha in alphas:
    print(f'Runing the algorithm for alpha : {alpha}\n')
    w = np.zeros(n_features)
    b = int(np.random.randint(100, size=1)[0])
    history = gradient_descent(
        X_processed,
        y_scaled,
        alpha=alpha,
        w_in=w,
        b_in=b,
        max_iter=max_iters,
        tolerance=tolerance,
        cost_fn=cost_fn,
        gradient_compute_fn=gradient_fn)
    print('\n')
    all_histories[str(alpha)] = history
```

|       Ici, on constate directement que notre algorithme a converg√© (crit√®re de *tol√©rance*) pour chacun des taux d‚Äôapprentissage (`learning rate`) test√©s.

Apr√®s cet entra√Ænement, il est int√©ressant de d√©terminer quel `learning rate` a permis d‚Äôobtenir le meilleur r√©sultat. Le code suivant est particuli√®rement utile lorsqu‚Äôon teste plusieurs valeurs de `learning rate` (plus de deux).


```{python, echo=TRUE}
#| label: see-good-alpha

best_alpha = None
min_cost = float('inf')

for alpha, hist in all_histories.items():
    costs = [v['cost_fn'] for v in hist.values()]
    min_cost_alpha = min(costs)
    print(f"Alpha {alpha} ‚û§ Min cost: {min_cost_alpha:.5f}")
    if min_cost_alpha < min_cost:
        min_cost = min_cost_alpha
        best_alpha = alpha

print(f"\n‚úÖ Best alpha: {best_alpha} with min cost: {min_cost:.2f}")
```

Avec ces deux `learning rates`, la fonction de co√ªt atteint 0, ce qui signifie que l‚Äôalgorithme a bien converg√© puisque la fonction de co√ªt est toujours positive.


```{python, echo=TRUE}
#| label: get-good-alpha

def get_best_w_b(cost_fn_and_params_hist: dict):
    hist_len = len(cost_fn_and_params_hist)
    best_of_hist = cost_fn_and_params_hist[hist_len-1]
    return best_of_hist
  
def predict(w, b, X):
    return X @ w + b
  
hist = all_histories['0.01']
best = get_best_w_b(hist)
```

|       On peut maintenant calculer nos pr√©dictions et v√©rifier leur qualit√© par rapport aux donn√©es r√©elles. Rappelons que les donn√©es ont √©t√© standardis√©es, il faudra donc ramener les pr√©dictions √† leur √©chelle d‚Äôorigine pour une interpr√©tation correcte.


```{python, echo=TRUE}
#| label: getting-pred

# standardised predictions
ypred_scaled = predict(best['w'], best['b'], X_processed).reshape((-1, 1))
print(f'Standardised data (first 3 values): {ypred_scaled[:4]}\n\n')

# normal scale
y_pred = scaler_y.inverse_transform(ypred_scaled)
print(f'Data on normal scale (first 3 values): {y_pred[:4]}\n\n')

# display of optimal parameters
print(f'Optimum parameters : {best}')
```

L'erreur quadratique moyenne sur les donn√©es d'apprentissage est donc **0.00610**.


```{python, fig.align='center', echo= TRUE, out.width='90%', out.height='80%'}
#| label: fig-plot-prediction
#| fig-cap: Predcitions vs Real data

fig, axes = plt.subplots(1, 3, figsize = (12, 8))
for i in range(len(axes)):
    axes[i].scatter(train[X_features[i]], train['salary'], label='target')
    axes[i].scatter(train[X_features[i]], y_pred, c='orange', label='predicted')
    axes[i].set_ylabel('Salary', size=10)
    axes[i].set_xlabel(X_features[i].capitalize(), size=10)
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].set_title(f'Relation between salary and {X_features[i].capitalize()}', size=10)
plt.legend()
plt.tight_layout()
plt.show()
```



|       On peut voir que l'ajustement est tr√®s satisfaisant, mais il reste  √† d√©terminer s'il ne s'agit pas d'un `surapprentissage`. Pour cela nous allons utiliser les donn√©es de test, pour √©valuer le mod√®le entrain√©. 

## Evaluation du mod√®le

|       Pour √©valuer le mod√®le nous utilisons la **validation crois√©e** car elle est robuste en terme d'√©valuation de performance d'un mod√®le.

- **But principal** : √âvaluer la performance d‚Äôun mod√®le de mani√®re fiable et robuste, en r√©duisant le biais d√ª √† une simple s√©paration train/test.

- **√âtape 1 : Partition des donn√©es**  
  Diviser l‚Äôensemble des donn√©es en *k* sous-ensembles (ou ¬´ folds ¬ª) de taille √† peu pr√®s √©gale.

- **√âtape 2 : Boucle sur les folds**  
  Pour chaque fold (de 1 √† k) :  
  - Utiliser ce fold comme ensemble de test (validation).  
  - Utiliser les *k-1* autres folds comme ensemble d‚Äôentra√Ænement.

- **√âtape 3 : Entra√Ænement**  
  Entra√Æner le mod√®le uniquement sur les donn√©es d‚Äôentra√Ænement (les *k-1* folds).

- **√âtape 4 : √âvaluation**  
  Tester le mod√®le entra√Æn√© sur le fold de test (le fold laiss√© de c√¥t√©), calculer une m√©trique de performance (ex : erreur quadratique moyenne).

- **√âtape 5 : Agr√©gation**  
  R√©p√©ter les √©tapes 2 √† 4 pour chaque fold, puis calculer la moyenne (et √©ventuellement l‚Äô√©cart-type) des performances obtenues sur chaque fold.

- **Avantages** :  
  - Meilleure estimation de la g√©n√©ralisation du mod√®le sur des donn√©es nouvelles.  
  - R√©duit le sur-apprentissage li√© √† un seul d√©coupage train/test.  
  - Utilise efficacement toutes les donn√©es pour entra√Ænement et validation.

- **Inconv√©nients** :  
  - Co√ªt computationnel plus √©lev√©, car le mod√®le est entra√Æn√© *k* fois.  
  - Peut √™tre sensible au choix de *k* (souvent 5 ou 10).



```{r, fig.align='center', echo= FALSE, out.width='90%', out.height='80%'}
#| label: fig-kfold-illustration
#| fig-cap: Illustration of a cross-validation process

knitr::include_graphics('k_fold.png')
```




```{python, echo = FALSE}
#| label: gradient-descent-fun-without-print

from typing import Callable
from copy import deepcopy
def gradient_descent(
    x: np.ndarray,
    y: np.ndarray,
    alpha: float,
    w_in: np.array,
    b_in: float,
    max_iter: int,
    tolerance: float,
    cost_fn: Callable,
    gradient_compute_fn: Callable) -> dict:
    """
    Effectue la descente de gradient pour ajuster
    les param√®tres w et b.

    Args:
        x (np.ndarray): Donn√©es d'entr√©e (m, n)
        y (np.ndarray): Cible (m,)
        w_in (np.ndarray): Poids initial (n,)
        b_in (float): Biais initial
        alpha (float): Taux d'apprentissage
        max_iter (int): Nombre maximal d'it√©rations
        tolerance (float): Seuil de convergence
        cost_fn (Callable): Fonction de co√ªt
        gradient_compute_fn (Callable): Fonction de calcul du gradient

    Returns:
        dict: Historique des param√®tres et co√ªt √† chaque it√©ration
    """

    w = deepcopy(w_in)
    b = b_in
    cost_fn_and_params_hist = {}
    for i in range(max_iter):
        fw_i, fb_i = gradient_compute_fn(x, y, w, b)
        w = w - alpha*fw_i
        b = b - alpha*fb_i
        cost_fn_i = cost_fn(x, y, w, b)
        cost_fn_and_params_hist[i] = {
            'w': w.copy(),
            'b': b,
            'cost_fn': cost_fn_i
        }
        if np.linalg.norm(fw_i) < tolerance and abs(fb_i) < tolerance:
            print(
                f'The algorithm converges at the {i}-th iteration'
                f'\nThen we have (w, b) = ({w}, {b:4f}) & cost_fn = {cost_fn_i:4f}'
                )
            break
    return cost_fn_and_params_hist
```

```{python}
def compute_accuracy(X, y_true, w, b):
    y_pred = X @ w + b
    y_pred_class = (y_pred >= 0.5).astype(int)  # ou seuil ajust√©
    return np.mean(y_pred_class == y_true)
```


```{python, echo=TRUE}
#| label: k-fold

from sklearn.metrics import r2_score

def k_fold_cross_validation(df, k, alpha, max_iters, tolerance, cost_fn, gradient_fn):
    """
    Perform k-fold cross-validation using gradient descent.

    Args:
        df (pd.DataFrame): full dataset (training set)
        k (int): number of folds
        alpha (float): learning rate
        max_iters (int): maximum number of iterations for gradient descent
        tolerance (float): convergence threshold
        cost_fn (Callable): cost function
        gradient_fn (Callable): gradient computation function

    Returns:
        dict: dictionnaire avec listes des mse_train, mse_test, r2_train, r2_test
    """
    # Shuffle the DataFrame index
    df = df.sample(frac=1).reset_index(drop=True)
    fold_size = len(df) // k
    
    mse_train_list = []
    mse_test_list = []
    r2_train_list = []
    r2_test_list = []
    
    for fold in range(k):
        # Define start and end indices for the test fold
        start = fold * fold_size
        end = (fold + 1) * fold_size if fold != k - 1 else len(df)
        
        # Split the data into test and train folds
        test_df = df.iloc[start:end]
        train_df = pd.concat([df.iloc[:start], df.iloc[end:]], axis=0)
        
        # Prepare features and target for train and test
        X_train = train_df.drop('salary', axis=1)
        y_train = train_df['salary']
        X_test = test_df.drop('salary', axis=1)
        y_test = test_df['salary']
        
        print('-'*10)
        print(f"\nFold {fold+1}:")
        print("Train unique education:", train_df['education'].unique())
        print("Test unique education:", test_df['education'].unique())
        print("Train unique sex:", train_df['sex'].unique())
        print("Test unique sex:", test_df['sex'].unique())
        print()
        
        numeric_features = ['ages']
        categorical_features = ['education', 'sex']

        preprocessor = ColumnTransformer(transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(drop='first'), categorical_features)
        ])
        
        # Preprocess the data using our pipeline
        X_train_processed = preprocessor.fit_transform(X_train)
        y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()
        
        X_test_processed = preprocessor.transform(X_test)
        y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()
        
        # Initialize parameters
        n_features = X_train_processed.shape[1]
        w = np.zeros(n_features)
        b = 0
        
        # Run gradient descent on training data
        history = gradient_descent(
            X_train_processed,
            y_train_scaled,
            alpha=alpha,
            w_in=w,
            b_in=b,
            max_iter=max_iters,
            tolerance=tolerance,
            cost_fn=cost_fn,
            gradient_compute_fn=gradient_fn
        )
        
        # Retrieve best parameters from last iteration
        last_iter = max(history.keys())
        w_best = history[last_iter]['w']
        b_best = history[last_iter]['b']
        
        # Predictions
        y_pred_train = X_train_processed @ w_best + b_best
        y_pred_test = X_test_processed @ w_best + b_best
        
        # Compute cost function (cost_fn returns half MSE)
        train_cost_half = cost_fn(X_train_processed, y_train_scaled, w_best, b_best)
        test_cost_half = cost_fn(X_test_processed, y_test_scaled, w_best, b_best)
        
        # Compute full MSE (because Gradient descent divided it by 2)
        train_mse = 2 * train_cost_half
        test_mse = 2 * test_cost_half
        
        # Compute R2 scores
        train_r2 = r2_score(y_train_scaled, y_pred_train)
        test_r2 = r2_score(y_test_scaled, y_pred_test)
        
        print(f'Fold {fold + 1}/{k} - Train MSE: {train_mse:.5f} | Test MSE: {test_mse:.5f}')
        print(f'Fold {fold + 1}/{k} - Train RMSE: {np.sqrt(train_mse):.5f} | Test RMSE: {np.sqrt(test_mse):.5f}')
        print(f'Fold {fold + 1}/{k} - Train R¬≤: {train_r2:.5f} | Test R¬≤: {test_r2:.5f}')
        
        mse_train_list.append(train_mse)
        mse_test_list.append(test_mse)
        r2_train_list.append(train_r2)
        r2_test_list.append(test_r2)
        
        print('-'*10)
        print('\n'*3)
    
    print(f'\nMedian Train MSE over {k} folds: {np.median(mse_train_list):.5f}')
    print(f'Median Test MSE over {k} folds: {np.median(mse_test_list):.5f}')
    print(f'Median Train R¬≤ over {k} folds: {np.median(r2_train_list):.5f}')
    print(f'Median Test R¬≤ over {k} folds: {np.median(r2_test_list):.5f}')
    results =  {
        "mse_train": mse_train_list,
        "mse_test": mse_test_list,
        "r2_train": r2_train_list,
        "r2_test": r2_test_list
    }
    return results
```


```{python, echo = TRUE}
#| label: testing-kfold model

np.random.seed(42)
k = 5
alpha = 0.01
max_iters = 10000
tolerance = 1e-4

results = k_fold_cross_validation(train, k, alpha, max_iters, tolerance, compute_cost_fn, compute_gradient)
```

|       On peut remaquer qu'√† chaque fold nos modalit√©s sont pr√©sentes et dans les donn√©es de test et dans celles de l'apprentissage. 


La validation crois√©e en 5 plis (K-Fold Cross-Validation) a √©t√© utilis√©e pour √©valuer la performance du mod√®le. Les valeurs ci-dessous correspondent √† la racine carr√©e de la fonction de co√ªt (Root Mean Square Error, RMSE), **calcul√©e sur des donn√©es de sortie standardis√©es**.

### R√©sultats par pli (RMSE Train vs Test)

- ‚úÖ Fold 1 :` 0.03362 vs 0.03346`
- ‚úÖ Fold 2 :` 0.03263 vs 0.03142`
- ‚úÖ Fold 3 :` 0.03005 vs 0.02598`
- ‚úÖ Fold 4 :` 0.03941 vs 0.04801`
- ‚úÖ Fold 5 :` 0.03251 vs 0.03157 `

|       Dans 4 des 5 folds, le MSE_test ‚â§ MSE_train, ce qui est surprenant mais pas impossible si la validation est bien faite et que les donn√©es sont tr√®s r√©guli√®res.

L‚Äô√©cart est faible dans tous les cas sauf pour le fold 4, o√π :

`MSE_test` = `0.00231` **>** `MSE_train` = `0.00155`

Cela sugg√®re un `l√©g√®re suradaptation` (overfitting) pour ce fold, mais l‚Äô`√©cart reste raisonnable`.


### Visualisation de la somme erreurs quadratiques par pli


```{python, fig.align='center', echo= TRUE, out.width='90%', out.height='80%'}
#| label: fig-rmse-value-plot
#| fig-cap: Model Performance Metrics per Fold

mse_train_list = results['mse_train']
mse_test_list = results['mse_test']
r2_train_list = results['r2_train']
r2_test_list = results['r2_test']

mse_median_train = np.median(mse_train_list)
mse_median_test = np.median(mse_test_list)
r2_median_train = np.median(r2_train_list)
r2_median_test = np.median(r2_test_list)


folds = np.arange(1, 6)

fig, axes = plt.subplots(1, 2, figsize=(12,6.5))

# --MSE--

axes[0].plot(folds, mse_train_list, label='Train MSE', linestyle='--', marker='o', linewidth=1.5)
axes[0].plot(folds, mse_test_list, label='Test MSE', linestyle='--', marker='o', linewidth=1.5)
axes[0].axhline(y=mse_median_train, color='r', linestyle='--', linewidth=1.5, label=f'Train Median MSE = {mse_median_train:.5f}')
axes[0].axhline(y=mse_median_test, color='g', linestyle='--', linewidth=1.5, label=f'Test Median MSE = {mse_median_test:.5f}')
axes[0].set_title('MSE per fold', fontsize=10)
axes[0].set_xlabel('Fold', fontsize=10)
axes[0].set_ylabel('MSE (standardized scale)', fontsize=10)
axes[0].legend(fontsize=8)
axes[0].tick_params(axis='both', which='major', labelsize=8)

# --R2--

axes[1].plot(folds, r2_train_list, label='Train R¬≤', linestyle='--', marker='o', linewidth=1.5)
axes[1].plot(folds, r2_test_list, label='Test R¬≤', linestyle='--', marker='o', linewidth=1.5)
axes[1].axhline(y=r2_median_train, color='r', linestyle='--', linewidth=1.5, label=f'Train Median R¬≤ = {r2_median_train:.5f}')
axes[1].axhline(y=r2_median_test, color='g', linestyle='--', linewidth=1.5, label=f'Test Median R¬≤ = {r2_median_test:.5f}')
axes[1].set_title('R¬≤ per fold', fontsize=10)
axes[1].set_xlabel('Fold', fontsize=10)
axes[1].set_ylabel('R¬≤', fontsize=10)
axes[1].legend(fontsize=8)
axes[1].tick_params(axis='both', which='major', labelsize=8)

plt.tight_layout()
plt.subplots_adjust(left=0.1, wspace=0.25)
plt.show()

```

|       Ce graphique pr√©sente les performances d‚Äôun mod√®le √† travers une validation crois√©e √† 5 plis (**5-fold cross-validation**), en utilisant deux m√©triques : **MSE (Mean Squared Error)** et **R¬≤ (coefficient de d√©termination)**.

### üìâ MSE par pli

- **Lignes bleues (Train MSE)** et **lignes orange (Test MSE)** montrent la performance sur les donn√©es d'entra√Ænement et de test respectivement.
- Les performances sont tr√®s similaires entre les diff√©rents plis, avec de **faibles valeurs de MSE**, ce qui indique une bonne qualit√© de pr√©diction.
- Une exception est observ√©e pour le pli 4, o√π le **Test MSE** est sensiblement plus √©lev√© (~0.0023), sugg√©rant une possible instabilit√© ou une complexit√© locale non bien captur√©e par le mod√®le.
- **Lignes horizontales** :
  - üî¥ Rouge pointill√©e : m√©diane du MSE d'entra√Ænement (‚âà 0.00106)
  - üü¢ Verte pointill√©e : m√©diane du MSE de test (‚âà 0.00100)

> **Interpr√©tation** : Le mod√®le g√©n√©ralise bien dans l‚Äôensemble, avec un **l√©ger sur-apprentissage** possible sur le pli 4.

### üìà R¬≤ par pli

- Le **R¬≤ d'entra√Ænement** et **de test** est tr√®s √©lev√© pour tous les plis, indiquant que le mod√®le explique **plus de 99.88 %** de la variance des donn√©es.

- Une baisse est observ√©e pour le pli 4 avec un **Test R¬≤ ‚âà 0.9978**, ce qui reste n√©anmoins tr√®s performant.
- **Lignes horizontales** :
  - üî¥ Rouge pointill√©e : m√©diane du R¬≤ d'entra√Ænement (‚âà 0.99894)
  - üü¢ Verte pointill√©e : m√©diane du R¬≤ de test (‚âà 0.99894)

---

|       En somme nous pouvons dirre que :

- Le mod√®le est **tr√®s performant**, avec des **erreurs faibles** et un **pouvoir explicatif √©lev√©** sur l'ensemble des plis.

- Le pli 4 montre une **l√©g√®re instabilit√©**, ce qui pourrait justifier une exploration compl√©mentaire sur les donn√©es de ce pli.

- Dans l'ensemble, les r√©sultats montrent un **excellent compromis biais-variance**, avec une **bonne g√©n√©ralisation**.


## Evaluation du mod√®le final sur les donn√©es de test


```{python, echo=TRUE}
#| label: testing-final model

# selecting test features
X_test = test.drop('salary', axis=1)

# transforming test features (one hot encodings and standardization)
X_test_processed = preprocessor.transform(X=X_test)

# predictions on standardized test features
y_pred_test_scaled = predict(best['w'], best['b'], X_test_processed).reshape((-1, 1))

# reverse scaling
y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled).reshape(200,)


# getting real y and changing the shape
y_true_test = test['salary'].values.reshape(200,)
```

|       Maintenant calculons le `RMSE` des erreurs commises en pr√©disant. D'abord essayons de visualiser cela de mani√®re s√©par√©e car il me vient √† l'id√©e d'ordonner les diff√©rents arrays obtenus, mais avant faut que je sois s√ªre que les salaires r√©els (pas dans le sens √©conomique du terme mais pour dire salaire observ√©) et ceux pr√©dits ont la m√™me allure.

```{python, fig.align='center', echo= TRUE, out.width='90%', out.height='80%'}
#| label: fig-testing-final-plot
#| fig-cap: 'Observed Slaries and Predicted Salaries visualisation'

fig, axes = plt.subplots(1, 2, figsize=(12, 8))
abs_ = np.arange(1, 201, 1)

axes[0].plot(abs_, y_true_test, c='g', label='Observed salaries', linewidth=1)
axes[0].set_title('Observed Salaries', fontsize=10)
axes[0].set_xlabel('Data points', fontsize=10)
axes[0].set_ylabel('Salary', fontsize=10)

axes[1].plot(abs_, y_pred_test, c='b', label='Predicted salaries', linewidth=1)
axes[1].set_title('Predicted Salaries', fontsize=10)
axes[1].set_xlabel('Data points', fontsize=10)
axes[1].set_ylabel('Salary', fontsize=10)

plt.tight_layout()
plt.show()
```

|       En effet les deux courbe ont pratiquement la m√™me allure, essayons de voir ce que √ßa donne quand on les arrange de mani√®re croissante.


```{python, fig.align='center', echo= TRUE, out.width='90%', out.height='80%'}
#| label: fig-testing-final-sorted-plot
#| fig-cap: 'Observed Slaries VS Predicted Salaries'

y_pred_test_sorted = np.sort(y_pred_test)
y_true_test_sorted = np.sort(y_true_test)
plt.figure(figsize=(12, 8))
abs_ = np.arange(1, 201, 1)
plt.plot(abs_,y_true_test_sorted , c='g', label='Observed values of the salary', linewidth=1)
plt.plot(abs_, y_pred_test_sorted, c='b',label='Predicted values of the salary', linewidth=1)
plt.ylabel('Salary')
plt.xlabel('Data points')
plt.legend(fontsize=10)
plt.tight_layout()
plt.show()
```

::: {.callout-note}
## Analyse graphique des salaires pr√©dits vs observ√©s

Les courbes des salaires pr√©dits et observ√©s montrent une forte similarit√©.  
Cela sugg√®re que le mod√®le est capable de bien capturer la relation entre les variables explicatives et le salaire, indiquant ainsi une **bonne capacit√© de g√©n√©ralisation** sur les donn√©es de test.
:::


- **Calcul de la racine de l'erreur quadratique moyenne**

```{python}
y_true_test_ = test['salary'].values.reshape(200,1)
```


```{python, echo = TRUE}
#| label: testing-final-rmse
# RMSE on standardised salaries
y_true_test_scaled = scaler_y.transform(y_true_test_).reshape(200, )

rmse_scaled = np.sqrt(np.mean((y_pred_test_scaled - y_true_test_scaled)**2))
print(f"standardized RMSE : {rmse_scaled:.2f}\n")


# RMSE on original wages
rmse_original = np.sqrt(
  (1/200)*(y_pred_test - y_true_test).T @ (y_pred_test - y_true_test)
)
print(f"original RMSE : {rmse_original:.2f}\n")
```

::: {.callout-warning title="‚ö†Ô∏è Attention √† l‚Äôinterpr√©tation : overfitting ou d√©s√©quilibre ?"}

Le fait que la `RMSE` sur le test soit nettement plus √©lev√©e que celle obtenue en validation crois√©e peut provenir de :

üîÅ Overfitting : le mod√®le a trop appris les sp√©cificit√©s du jeu d'entra√Ænement (m√™me via cross-validation), et g√©n√©ralise mal (ce qui est moins probable).

‚öñÔ∏è D√©s√©quilibre dans les variables cat√©gorielles : il se peut que la r√©partition des niveaux d‚Äô√©ducation ou des sexes soit diff√©rente entre le train et le test, ce qui fausse l‚Äô√©valuation (Ce qui est plus probable car au debut j'avais mentionn√© ce soucis).

Il est donc essentiel d‚Äôexaminer les distributions des variables dans chaque ensemble pour comprendre la cause exacte.

üßë‚Äçüè´ Mais rappelons que le but ici n'√©tait pas de construire le meilleur mod√®le, mais de montrer concr√®tement la mise en ≈ìuvre de l‚Äôalgorithme de descente de gradient en contexte r√©el.

Ce genre d‚Äô√©cart illustre parfaitement pourquoi la g√©n√©ralisation est un d√©fi fondamental en machine learning.
:::



- **Interpr√©tation de la RMSE en valeur r√©elle**

|       Les salaires dans notre dataset varient entre environ **24 933 ‚Ç¨** et **75 318 ‚Ç¨**, ce qui explique qu‚Äôune `RMSE` d‚Äôenviron `284 ‚Ç¨` soit coh√©rente.

Pour mieux comprendre ce que signifie cette erreur moyenne, consid√©rons l‚Äô√©chelle des salaires :  
la diff√©rence entre le minimum et le maximum est d‚Äôenviron **50 385 ‚Ç¨**.

Ainsi, une `RMSE` de `284 ‚Ç¨` correspond √† une erreur moyenne relative d‚Äôenviron :

$$
\frac{284}{50 385} \approx 0.0056 \quad \text{soit} \quad 0{,}56\%
$$


---

## Conclusion g√©n√©rale

|         En r√©sum√©, la **descente de gradient** s‚Äôest r√©v√©l√©e √™tre un algorithme simple, intuitif et pourtant puissant pour la **minimisation de la fonction de co√ªt** dans le cadre de la r√©gression lin√©aire. En parcourant de mani√®re it√©rative la direction oppos√©e au gradient, l'algorithme permet une r√©duction monotone du co√ªt √† chaque it√©ration.

Nous avons vu que ce proc√©d√© consiste √† ajuster les coefficients du mod√®le √† partir d‚Äôun point initial, en effectuant des pas proportionnels au n√©gatif du gradient et calibr√©s par le **taux d‚Äôapprentissage**. Tant que celui-ci est bien choisi (pas trop grand, pas trop petit), la proc√©dure converge vers un minimum local de la fonction de co√ªt. Toutefois, nous avons aussi montr√© que certains cas (comme des ravins √©troits ou des matrices Hessiennes mal conditionn√©es) peuvent ralentir la convergence ou provoquer des oscillations.

|       **La faible erreur en euros (300 sur des milliers) sur les donn√©es de test, combin√©e √† une RMSE relativement faible en validation crois√©e, sugg√®re que le mod√®le est bien calibr√© et capable de g√©n√©raliser efficacement. Cependant, l'√©cart entre l'entra√Ænement et la validation pourrait indiquer un l√©ger surapprentissage, bien que cet √©cart soit mod√©r√©**.



### Points cl√©s √† retenir

1. **Principe de fonctionnement**  
   Le param√®tre $\theta$ est mis √† jour selon $\theta \leftarrow \theta - \gamma \nabla_\theta J(\theta)$, avec $J$ la fonction de co√ªt et $\alpha$ le pas d‚Äôapprentissage.

2. **Monotonie et convergence** 

   √Ä chaque √©tape, la valeur du co√ªt d√©cro√Æt (tant que $\alpha$ est convenablement choisie), garantissant la progression vers un minimum local.

3. **Limites de la m√©thode** 

   La descente de gradient peut rencontrer des difficult√©s de convergence lorsqu‚Äôon affine le minimum dans des zones √©troites ou avec des gradients tr√®s pontuels. D‚Äôautres m√©thodes, comme le `gradient conjugu√©` ou `Newton`, peuvent alors offrir un gain en performance.
   
:::{.callout-warning title="‚ö†Ô∏è Attention : Influence de l'√©chelle des variables explicatives"}
Lorsque les variables explicatives ont des √©chelles tr√®s diff√©rentes, cela peut provoquer un comportement non optimal de la descente de gradient.

Par exemple :
- Une variable $x_1$ variant entre 0 et 1
- Une autre variable $x_2$ variant entre 10 et 50

Un petit poids $w_2$ associ√© √† $x_2$ peut entra√Æner une variation importante de la pr√©diction, tandis qu‚Äôun poids $w_1$ beaucoup plus grand associ√© √† $x_1$ pourrait n‚Äôavoir qu‚Äôun effet marginal.

**Cons√©quences :**
- La surface de la fonction de co√ªt est tr√®s √©tir√©e dans certaines directions.
- La descente de gradient progresse tr√®s lentement, zigzague ou peut ne jamais converger.
- Le nombre d‚Äôit√©rations n√©cessaires augmente fortement.

üëâ C‚Äôest pourquoi **la normalisation des variables** (standardisation ou min-max scaling) est une √©tape cruciale avant d‚Äôentra√Æner un mod√®le lin√©aire avec descente de gradient.
:::
   
:::{.callout-important}
## ‚ö†Ô∏è Important : `fit_transform()` vs `transform()`

Toujours utiliser `fit_transform()` **uniquement sur les donn√©es d'entra√Ænement**, et `transform()` sur les donn√©es de validation ou de test.

Cela s‚Äôapplique √† **tous les types de preprocessing**, notamment :

- üü¶ **StandardScaler** : la moyenne et l‚Äô√©cart-type doivent √™tre appris sur le *train* uniquement.
- üüß **OneHotEncoder** : les cat√©gories doivent √™tre identifi√©es √† partir du *train* et appliqu√©es de mani√®re coh√©rente au *test*.

‚ùå Ne jamais faire `fit_transform()` sur le test, car cela introduit du **data leakage** (les donn√©es de test influencent le mod√®le).
:::


### Perspectives

- **Am√©lioration du pas ($\gamma$)** : l‚Äôusage d‚Äôun pas adaptatif ou de sch√©mas comme l‚Äôapprentissage d√©croissant peut am√©liorer la rapidit√© et la robustesse de la convergence.

- **Extensions avanc√©es** : l‚Äôajout de r√©gularisation (comme Ridge ou Lasso) ou le passage √† des variantes stochastiques (SGD) ou mini-batch permet de g√©n√©raliser la m√©thode √† des ensembles plus volumineux et √† la mod√©lisation en profondeur.

- **Combinaison avec d'autres algorithmes** : pour pallier les inefficacit√©s, on peut int√©grer des techniques comme le momentum, AdaGrad, RMSprop ou Adam, qui corr√©lent le gradient pour acc√©l√©rer la convergence et stabiliser l‚Äôapprentissage.

### Finalement

|       La descente de gradient traduit √©l√©gamment les principes fondamentaux de l‚Äô**optimisation it√©rative** : √† chaque √©tape, un petit ajustement calcul√© √©loigne le mod√®le de l‚Äôerreur, jusqu‚Äô√† atteindre la valeur optimale des param√®tres \(\theta\). M√™me si elle n‚Äôest pas sans d√©fauts, cette m√©thode demeure une **brique essentielle** en apprentissage automatique, particuli√®rement en r√©gression lin√©aire, et sert de base √† des m√©thodes plus sophistiqu√©es.

---

## Remerciements et retour d‚Äôexp√©rience

J‚Äôai √©t√© ravi d‚Äôavoir consacr√© pr√®s de **20 heures** √† la pr√©paration et √† la r√©daction de cette publication. 

Cet effort m‚Äôa permis de :

- consolider mes acquis en **Python**,  
- approfondir mes connaissances en **Machine Learning**, tant sur le plan **th√©orique que pratique**,  
- renforcer ma **rigueur m√©thodologique** dans le traitement des donn√©es, l‚Äôexp√©rimentation et l‚Äôanalyse des r√©sultats.

Ce projet m‚Äôa √©galement offert une excellente opportunit√© de structurer une d√©marche compl√®te de mod√©lisation, depuis la g√©n√©ration des donn√©es jusqu‚Äô√† l‚Äôinterpr√©tation finale des performances du mod√®le.

Je suis enthousiaste √† l‚Äôid√©e de poursuivre cette exploration dans de futures publications de ce genre (travailler √† la mano), notamment sur des cas r√©els avec des mod√®les plus avanc√©s comme la **r√©gression logistique** ou des approches **r√©gularis√©es**.

Mon cours d'optimisation et de m√©thode de calcul num√©riques m'a √©t√© d'une grande utilit√© en 1A √† l'ENSAI.


---

## Annexes

### 1. Standardisation des variables

|       La **standardisation** permet de centrer et r√©duire les variables num√©riques pour qu‚Äôelles aient une moyenne nulle et un √©cart-type unitaire.

Pour une variable continue $x$, la standardisation est donn√©e par :

$$
x_{\text{std}} = \frac{x - \mu_x}{\sigma_x}
$$

o√π :

- $\mu_x = \dfrac{1}{n} \sum_{i=1}^n x_i$ est la moyenne,
- $\sigma_x = \sqrt{\dfrac{1}{n} \sum_{i=1}^n (x_i - \mu_x)^2}$ est l‚Äô√©cart-type.

Apr√®s avoir entra√Æn√© le mod√®le sur les donn√©es standardis√©es, on peut revenir √† l‚Äô√©chelle r√©elle avec :

$$
\hat{y} = \hat{y}_{\text{std}} \cdot \sigma_y + \mu_y
$$

o√π $\mu_y$ et $\sigma_y$ sont la moyenne et l‚Äô√©cart-type de la variable cible $y$.

---

### 2. Codage One-Hot

Le **One-Hot Encoding** transforme une variable cat√©gorielle √† $k$ modalit√©s en $k$ colonnes binaires.

Soit une variable cat√©gorielle :

$$
\text{cat} \in \{c_1, c_2, \ldots, c_k\}
$$

On cr√©e un vecteur :

$$
\mathbf{v} = (v_1, v_2, \ldots, v_k) \quad \text{o√π} \quad
v_j =
\begin{cases}
1 & \text{si } \text{cat} = c_j \\
0 & \text{sinon}
\end{cases}
$$

Afin d‚Äô√©viter la redondance, on supprime une modalit√© (ex. via `drop='first'`) pour √©viter le pi√®ge des variables muettes (dummy variable trap).

---

### 3. Calcul de la RMSE

La **Root Mean Square Error** (RMSE) mesure l'√©cart quadratique moyen entre les valeurs pr√©dites $\hat{y}_i$ et observ√©es $y_i$ :

$$
\text{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 }
$$

En notation vectorielle, si $\mathbf{y}$ est le vecteur des valeurs observ√©es et $\hat{\mathbf{y}}$ celui des valeurs pr√©dites :

$$
\text{RMSE} = \sqrt{ \frac{1}{n} (\mathbf{y} - \hat{\mathbf{y}})^T (\mathbf{y} - \hat{\mathbf{y}}) }
$$

Cette m√©trique est exprim√©e dans l‚Äôunit√© de la variable cible (ici, les euros), ce qui la rend facile √† interpr√©ter dans un contexte r√©el.

---

Ces op√©rations sont fondamentales dans toute pipeline de traitement pour la r√©gression ou tout autre algorithme supervis√©.

