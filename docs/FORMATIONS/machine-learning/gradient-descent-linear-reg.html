<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Djamal TOE">
<meta name="dcterms.date" content="2025-06-08">

<title>DJAMAL WEBSITE - Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="index.qmd" class="navbar-brand navbar-brand-logo">
    <img src="../../images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="index.qmd">
    <span class="navbar-title">DJAMAL WEBSITE</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-statistics--machine-learning" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Statistics &amp; Machine Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-statistics--machine-learning">    
        <li>
    <a class="dropdown-item" href="../../INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html" rel="" target="">
 <span class="dropdown-text">Diagnostic assisté par ACP et régression logistique</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html" rel="" target="">
 <span class="dropdown-text">Diagnostic tumeurs cérébrales - Reseau de neurones</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../ANALYSES_FACTORIELLES/acp-kmeans.html" rel="" target="">
 <span class="dropdown-text">Reduction de dimensionnalité et clustering non supervisé</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../FORMATIONS/logistic_regression_diabetes.html" rel="" target="">
 <span class="dropdown-text">Modélisation des données à variables dépendantes binaires</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../projet-traitement-donnees/report_writing/synthese-des-travaux.html" rel="" target="">
 <span class="dropdown-text">Prédire la durée de carrière des joueurs NBA</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../FORMATIONS/poisson_paludisme.html" rel="" target="">
 <span class="dropdown-text">Modélisation des données de comptage</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../INFO_MINI_PROJETS/shifumi-cnn-yolov8.html" rel="" target="">
 <span class="dropdown-text">Classification des gestes de la main avec Yolo et CNN</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-programming" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Programming</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-programming">    
        <li>
    <a class="dropdown-item" href="../../INFO_MINI_PROJETS/assistant_virtuel.html" rel="" target="">
 <span class="dropdown-text">Crée ton assistant virtuel avec pyhton</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html" rel="" target="">
 <span class="dropdown-text">Application desktop avec Java et Mysql</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-various" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Various</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-various">    
        <li>
    <a class="dropdown-item" href="../../FORMATIONS/machine-learning/gradient-descent-linear-reg.html" rel="" target="">
 <span class="dropdown-text">Régression linéaire par descente de gradient - théorie et application</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About me</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://www.linkedin.com/in/djamal-toe-7a18432b0" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text">LinkedIn</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#résumé" id="toc-résumé" class="nav-link active" data-scroll-target="#résumé">Résumé</a></li>
  <li><a href="#abstract" id="toc-abstract" class="nav-link" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#un-peu-de-formalisme" id="toc-un-peu-de-formalisme" class="nav-link" data-scroll-target="#un-peu-de-formalisme">Un peu de formalisme</a></li>
  <li><a href="#algorithme-de-la-descente-de-gradient-cas-de-la-regression-linéaire" id="toc-algorithme-de-la-descente-de-gradient-cas-de-la-regression-linéaire" class="nav-link" data-scroll-target="#algorithme-de-la-descente-de-gradient-cas-de-la-regression-linéaire">Algorithme de la descente de gradient : cas de la regression linéaire</a></li>
  <li><a href="#vérification-de-la-convergence" id="toc-vérification-de-la-convergence" class="nav-link" data-scroll-target="#vérification-de-la-convergence">🔎 Vérification de la convergence</a></li>
  <li><a href="#mise-à-jour-des-paramètres" id="toc-mise-à-jour-des-paramètres" class="nav-link" data-scroll-target="#mise-à-jour-des-paramètres">⚙️ Mise à jour des paramètres</a></li>
  <li><a href="#remarques-pratiques" id="toc-remarques-pratiques" class="nav-link" data-scroll-target="#remarques-pratiques">🧪 Remarques pratiques</a></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications">Applications</a></li>
  <li><a href="#analyse-exploratoire-relation-entre-salaire-et-variables-explicatives" id="toc-analyse-exploratoire-relation-entre-salaire-et-variables-explicatives" class="nav-link" data-scroll-target="#analyse-exploratoire-relation-entre-salaire-et-variables-explicatives">📊 Analyse exploratoire : Relation entre salaire et variables explicatives</a>
  <ul class="collapse">
  <li><a href="#relation-entre-le-salaire-et-lâge" id="toc-relation-entre-le-salaire-et-lâge" class="nav-link" data-scroll-target="#relation-entre-le-salaire-et-lâge">👴 Relation entre le salaire et l’âge</a></li>
  <li><a href="#relation-entre-le-salaire-et-le-sexe" id="toc-relation-entre-le-salaire-et-le-sexe" class="nav-link" data-scroll-target="#relation-entre-le-salaire-et-le-sexe">🚻 Relation entre le salaire et le sexe</a></li>
  <li><a href="#relation-entre-le-salaire-et-le-niveau-déducation" id="toc-relation-entre-le-salaire-et-le-niveau-déducation" class="nav-link" data-scroll-target="#relation-entre-le-salaire-et-le-niveau-déducation">🎓 Relation entre le salaire et le niveau d’éducation</a></li>
  <li><a href="#conclusion-de-lanalyse-exploratoire-rapide" id="toc-conclusion-de-lanalyse-exploratoire-rapide" class="nav-link" data-scroll-target="#conclusion-de-lanalyse-exploratoire-rapide">✅ Conclusion de l’analyse exploratoire rapide</a></li>
  </ul></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data preprocessing</a></li>
  <li><a href="#application-de-lalgorithme-de-la-descente-de-gradient" id="toc-application-de-lalgorithme-de-la-descente-de-gradient" class="nav-link" data-scroll-target="#application-de-lalgorithme-de-la-descente-de-gradient">Application de l’algorithme de la descente de gradient</a></li>
  <li><a href="#evaluation-du-modèle" id="toc-evaluation-du-modèle" class="nav-link" data-scroll-target="#evaluation-du-modèle">Evaluation du modèle</a>
  <ul class="collapse">
  <li><a href="#résultats-par-pli-rmse-train-vs-test" id="toc-résultats-par-pli-rmse-train-vs-test" class="nav-link" data-scroll-target="#résultats-par-pli-rmse-train-vs-test">Résultats par pli (RMSE Train vs Test)</a></li>
  <li><a href="#visualisation-de-la-somme-erreurs-quadratiques-par-pli" id="toc-visualisation-de-la-somme-erreurs-quadratiques-par-pli" class="nav-link" data-scroll-target="#visualisation-de-la-somme-erreurs-quadratiques-par-pli">Visualisation de la somme erreurs quadratiques par pli</a></li>
  <li><a href="#mse-par-pli" id="toc-mse-par-pli" class="nav-link" data-scroll-target="#mse-par-pli">📉 MSE par pli</a></li>
  <li><a href="#r²-par-pli" id="toc-r²-par-pli" class="nav-link" data-scroll-target="#r²-par-pli">📈 R² par pli</a></li>
  </ul></li>
  <li><a href="#evaluation-du-modèle-final-sur-les-données-de-test" id="toc-evaluation-du-modèle-final-sur-les-données-de-test" class="nav-link" data-scroll-target="#evaluation-du-modèle-final-sur-les-données-de-test">Evaluation du modèle final sur les données de test</a></li>
  <li><a href="#conclusion-générale" id="toc-conclusion-générale" class="nav-link" data-scroll-target="#conclusion-générale">Conclusion générale</a>
  <ul class="collapse">
  <li><a href="#points-clés-à-retenir" id="toc-points-clés-à-retenir" class="nav-link" data-scroll-target="#points-clés-à-retenir">Points clés à retenir</a></li>
  <li><a href="#perspectives" id="toc-perspectives" class="nav-link" data-scroll-target="#perspectives">Perspectives</a></li>
  <li><a href="#finalement" id="toc-finalement" class="nav-link" data-scroll-target="#finalement">Finalement</a></li>
  </ul></li>
  <li><a href="#remerciements-et-retour-dexpérience" id="toc-remerciements-et-retour-dexpérience" class="nav-link" data-scroll-target="#remerciements-et-retour-dexpérience">Remerciements et retour d’expérience</a></li>
  <li><a href="#annexes" id="toc-annexes" class="nav-link" data-scroll-target="#annexes">Annexes</a>
  <ul class="collapse">
  <li><a href="#standardisation-des-variables" id="toc-standardisation-des-variables" class="nav-link" data-scroll-target="#standardisation-des-variables">1. Standardisation des variables</a></li>
  <li><a href="#codage-one-hot" id="toc-codage-one-hot" class="nav-link" data-scroll-target="#codage-one-hot">2. Codage One-Hot</a></li>
  <li><a href="#calcul-de-la-rmse" id="toc-calcul-de-la-rmse" class="nav-link" data-scroll-target="#calcul-de-la-rmse">3. Calcul de la RMSE</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Djamal TOE </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 8, 2025</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, OneHotEncoder</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'deeplearning.mplstyle'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="résumé" class="level2">
<h2 class="anchored" data-anchor-id="résumé">Résumé</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;À la suite de mes cours d’optimisation et de calcul numérique, j’ai souhaité implémenter moi-même l’algorithme de la descente de gradient. J’ai choisi comme premier cas d’application la <strong><code>régression linéaire</code></strong>, dans le but d’estimer les paramètres optimaux (<code>w</code>, <code>b</code>) qui minimisent la fonction de coût (généralement l’erreur quadratique moyenne).</p>
<p>Même si, dans le cas de la régression linéaire, il existe une solution analytique explicite par la méthode des moindres carrés, cette situation est idéale pour comprendre et tester l’efficacité de la descente de gradient. En revanche, pour des modèles plus complexes comme la régression logistique, une solution analytique n’est plus disponible. Dans ces cas, on utilise systématiquement des <strong><code>méthodes numériques</code></strong> comme la descente de gradient. En pratique, il faut s’assurer que les données soient réparties de manière équilibrée selon des critères comme le sexe ou l’éducation entre les jeux d’apprentissage, de validation (avec la validation croisée) et de test, pour éviter que l’évaluation du modèle soit faussée.</p>
<hr>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Following my courses in optimisation and numerical computation, I wanted to implement the gradient descent algorithm myself. My first application was to linear regression, with the aim of estimating the optimal parameters (<code>w</code>, <code>b</code>) that minimise the cost function (generally the mean square error).</p>
<p>Even though, in the case of linear regression, there is an explicit analytical solution using the method of least squares, this situation is ideal for understanding and testing the effectiveness of gradient descent. However, for more complex models such as logistic regression, an analytical solution is no longer available. In these cases, numerical methods such as gradient descent are systematically used. In practice, care must be taken to ensure that the data is distributed evenly according to criteria such as gender or education between the training, validation (with cross-validation) and test sets, to avoid distorting the evaluation of the model.</p>
<hr>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;L’objectif de ce document est de présenter l’implémentation de l’algorithme de descente de gradient dans le cas de la régression linéaire multiple, afin de mieux comprendre le principe d’optimisation itérative. On commence par un rappel de la régression linéaire classique, avant de dériver la fonction de coût et ses gradients par rapport aux paramètres. Ensuite, l’algorithme est appliqué à un jeu de données simulé pour illustrer sa convergence et les effets du taux d’apprentissage.</p>
<p>Dans une deuxième partie, nous discuterons des limites de l’approche analytique, notamment dans des contextes où la descente de gradient devient incontournable (régression logistique, réseaux de neurones, etc.).</p>
<hr>
</section>
<section id="un-peu-de-formalisme" class="level2">
<h2 class="anchored" data-anchor-id="un-peu-de-formalisme">Un peu de formalisme</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nous souhaitons minimiser une fonction objective, appelée également <strong><code>fonction de coût</code></strong>. Avant de chercher une méthode numérique pour effectuer cette minimisation, il est essentiel de s’assurer que la fonction admet bien un minimum, et que celui-ci est <strong><code>unique</code></strong>.</p>
<p>En effet, une fonction peut présenter <strong><code>plusieurs minima locaux</code></strong>, et l’objectif est généralement d’atteindre le <strong><code>minimum global</code></strong>. Pour garantir l’existence d’un minimum global, on fait appel au <strong><code>théorème de Weierstrass</code></strong>, qui stipule qu’une fonction continue sur un ensemble compact atteint un minimum (et un maximum).</p>
<p>Mais pour garantir l’unicité du minimum, on utilise la notion de <strong><code>convexité</code></strong>. Une fonction strictement convexe sur un domaine convexe possède un unique minimum qui est donc le minimum global.</p>
<blockquote class="blockquote">
<p>💡 Si votre fonction de coût est <strong><code>concave</code></strong> (au lieu d’être convexe), il suffit d’en prendre l’opposé. La maximisation d’une fonction concave revient à minimiser son opposée, qui sera convexe.</p>
</blockquote>
<p>Ainsi, <strong>la stricte convexité</strong> de la fonction objective est une propriété cruciale : elle permet de garantir l’unicité du minimum et donc la convergence de l’algorithme vers une solution bien définie.</p>
<p>Je n’entrerai pas dans trop de details mathématiques. Si vous voulez en savoir plus (condition d’application du théorème de weierstrass -&gt; sémi-continuité + espace de contraintes borné ou semi-continuité + coercivité + espace de contraintes fermé) consultez cette page : &lt;<a href="https://en.wikipedia.org/wiki/Extreme_value_theorem" class="uri">https://en.wikipedia.org/wiki/Extreme_value_theorem</a>&gt; et faites des recherches supplémentaires.</p>
<hr>
</section>
<section id="algorithme-de-la-descente-de-gradient-cas-de-la-regression-linéaire" class="level2">
<h2 class="anchored" data-anchor-id="algorithme-de-la-descente-de-gradient-cas-de-la-regression-linéaire">Algorithme de la descente de gradient : cas de la regression linéaire</h2>
<ul>
<li><strong>Fonction de coût de la regression linéaire</strong></li>
</ul>
<p>La fonction de coût utilisée pour la régression linéaire est la (ou <code>Mean Squared Error</code>, MSE). Elle s’écrit comme suit :</p>
<p><span class="math display">\[
\begin{equation}
    J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^2
\end{equation}
\]</span></p>
<p>où :</p>
<p>En <strong><em>notation vectorielle</em></strong>, on peut réécrire la fonction de coût sous la forme :</p>
<p><span class="math display">\[
\begin{equation}
    J(w, b) = \frac{1}{2m} (\hat{Y} - Y)^T (\hat{Y} - Y)
\end{equation}
\]</span></p>
<p>où :</p>
<p><span class="math display">\[
\begin{equation}
    J(w, b) = \frac{1}{2m} \left\| \hat{Y} - Y \right\|^2
\end{equation}
\]</span></p>
<ul>
<li>🔻 <strong>Descente de Gradient</strong></li>
</ul>
<p>L’objectif de la descente de gradient est de déterminer les paramètres optimaux <span class="math inline">\(w\)</span> et <span class="math inline">\(b\)</span> qui minimisent la fonction de coût. Pour cela, on utilise un algorithme <strong>itératif</strong> qui met progressivement à jour ces paramètres <strong>dans le sens opposé au gradient</strong>.</p>
<p>À chaque itération, les nouvelles valeurs de <span class="math inline">\(w\)</span> et <span class="math inline">\(b\)</span> doivent idéalement conduire à une <strong>diminution de la fonction de coût</strong>. Si la fonction <strong>augmente</strong>, cela peut être dû à :</p>
<ul>
<li>un <strong>taux d’apprentissage</strong> (<span class="math inline">\(\alpha\)</span>) trop <strong>élevé</strong>, provoquant une <strong>divergence</strong> ;</li>
<li>une <strong>erreur de code</strong>, par exemple un mauvais calcul du gradient.</li>
</ul>
<p>En revanche, si la fonction de coût diminue <strong>trop lentement</strong>, cela peut signifier :</p>
<ul>
<li>un taux d’apprentissage trop <strong>faible</strong>, causant une <strong>convergence lente</strong> ou incomplète.</li>
</ul>
<blockquote class="blockquote">
<p>⚠️ Dans certains cas, la fonction de coût peut <strong>onduler</strong> à chaque itération (oscillations), souvent à cause d’une mauvaise normalisation ou d’un <span class="math inline">\(\alpha\)</span> mal ajusté.</p>
</blockquote>
<hr>
</section>
<section id="vérification-de-la-convergence" class="level2">
<h2 class="anchored" data-anchor-id="vérification-de-la-convergence">🔎 Vérification de la convergence</h2>
<p>Pour s’assurer que l’algorithme converge correctement, on peut :</p>
<ul>
<li><strong>Tracer graphiquement</strong> la valeur de la fonction de coût à chaque itération.</li>
<li>Fixer un <strong>seuil de tolérance</strong> :</li>
</ul>
<p><span class="math display">\[
\text{Si } \|\nabla J(w, b)\| &lt; \varepsilon, \text{ alors on arrête l'algorithme.}
\]</span></p>
<hr>
</section>
<section id="mise-à-jour-des-paramètres" class="level2">
<h2 class="anchored" data-anchor-id="mise-à-jour-des-paramètres">⚙️ Mise à jour des paramètres</h2>
<p>La descente de gradient met à jour <strong>simultanément</strong> les poids et le biais selon la règle :</p>
<p><span class="math display">\[
w := w - \alpha \cdot \frac{\partial J(w, b)}{\partial w}
\]</span></p>
<p><span class="math display">\[
b := b - \alpha \cdot \frac{\partial J(w, b)}{\partial b}
\]</span></p>
<p>où :</p>
<ul>
<li><span class="math inline">\(J(w, b)\)</span> est la fonction de coût,</li>
<li><span class="math inline">\(\alpha\)</span> est le <strong>learning rate</strong>.</li>
</ul>
<hr>
</section>
<section id="remarques-pratiques" class="level2">
<h2 class="anchored" data-anchor-id="remarques-pratiques">🧪 Remarques pratiques</h2>
<ul>
<li>Un <strong>taux d’apprentissage dynamique</strong> (adaptatif) peut améliorer la convergence.</li>
<li>Des techniques comme <strong>momentum</strong>, <strong>RMSprop</strong> ou <strong>Adam</strong> sont des variantes plus stables.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fiting" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gradient-descent-linear-reg_files/figure-html/fig-fiting-1.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Learning example</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Descente de Gradient et Régularisation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Descente de Gradient et Régularisation
</div>
</div>
<div class="callout-body-container callout-body">
<p>L’<strong>algorithme de descente de gradient</strong> est une méthode d’optimisation utilisée pour ajuster les paramètres d’un modèle en minimisant une fonction de coût. Il repose sur le calcul du gradient (ou pente) de cette fonction par rapport aux paramètres, et sur une mise à jour itérative jusqu’à convergence.</p>
<p>Cependant, en pratique, on rencontre souvent le <strong>problème de surapprentissage (overfitting)</strong>, notamment lorsque :</p>
<ul>
<li>Le nombre de variables est élevé par rapport au nombre d’observations.</li>
<li>Certaines variables explicatives n’apportent que peu ou pas d’information utile.</li>
<li>Le modèle devient trop complexe, capturant le bruit au lieu du signal.</li>
</ul>
<p>Pour pallier cela, plusieurs stratégies existent :</p>
<ul>
<li>🔄 <strong>Augmenter la taille du jeu de données</strong> : plus de données permet de mieux généraliser.</li>
<li>🧠 <strong>Sélectionner judicieusement les variables</strong> : par des techniques comme le <em>feature selection</em>, on garde seulement les plus pertinentes.</li>
<li>🛡️ <strong>Appliquer une régularisation</strong> (comme Lasso ou Ridge) : on pénalise la complexité du modèle pour éviter l’ajustement excessif.</li>
</ul>
<p>👉 Ces aspects seront peut-être explorés plus en détail dans une prochaine publication à travers un <strong>modèle de régression logistique appliqué à des données réelles</strong>, où la sélection de variables et la régularisation joueront un rôle central.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="La Régression Polynomiale">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
La Régression Polynomiale
</div>
</div>
<div class="callout-body-container callout-body">
<p>La <strong>régression polynomiale</strong> est une extension de la régression linéaire où l’on introduit des puissances supplémentaires des variables explicatives pour capturer des relations <strong>non linéaires</strong> entre les variables.</p>
<p>Exemple :</p>
<p><span class="math display">\[
y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \dots + w_d x^d + \varepsilon
\]</span></p>
<p>Cela permet au modèle de s’adapter à des courbes complexes, mais augmente également le <strong>risque de surapprentissage</strong>. Plus le degré ( d ) est élevé, plus le modèle est flexible, mais moins il généralise bien si les données ne sont pas suffisantes.</p>
<p>➡️ Il est donc <strong>essentiel de combiner cette approche avec des techniques de validation croisée et de régularisation</strong>, pour trouver le bon compromis entre biais et variance.</p>
</div>
</div>
</section>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">Applications</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dans cette section, nous coommençons d’abord par simuler un jeu de données synthétiques représentant des individus caractérisés par leur âge, leur niveau d’éducation, leur sexe, ainsi que leur salaire annuel. Le salaire est généré à partir d’un modèle probabiliste basé sur des salaires de base associés à chaque niveau d’éducation, auxquels s’ajoute un effet linéaire de l’âge, et une part d’aléa simulée à l’aide d’une distribution normale.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># generating data</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># fixing generator seed</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># number of observation</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># creating age variable</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>ages <span class="op">=</span> np.random.randint(low<span class="op">=</span><span class="dv">21</span>, high<span class="op">=</span><span class="dv">66</span>, size<span class="op">=</span>size)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># creating education level variable</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>levels <span class="op">=</span> [<span class="st">'BAC'</span>, <span class="st">'Bachelor degree'</span>, <span class="st">'Msc'</span>, <span class="st">'PhD'</span>]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>educ_level <span class="op">=</span> np.random.choice(levels, size<span class="op">=</span>size, p<span class="op">=</span>[<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>])</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># creating sexe variable</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>sexes <span class="op">=</span> [<span class="st">'M'</span>, <span class="st">'F'</span>]</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>sexe <span class="op">=</span> np.random.choice(sexes, size<span class="op">=</span>size)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># creating annual salary (Euro) based on the different informations above</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>base_salary <span class="op">=</span> {</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'BAC'</span>: <span class="dv">25000</span>,</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Bachelor degree'</span>: <span class="dv">30000</span>,</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Msc'</span>: <span class="dv">45000</span>,</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'PhD'</span>: <span class="dv">75000</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>salary <span class="op">=</span> np.asarray([</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>  <span class="bu">round</span>(np.random.normal(base_salary[edu_lvl] <span class="op">+</span> (age<span class="op">*</span><span class="dv">2</span>), <span class="dv">100</span>)) </span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> edu_lvl, age <span class="kw">in</span> <span class="bu">zip</span>(educ_level, ages)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        <span class="st">'salary'</span>: salary,</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        <span class="st">'ages'</span>: ages,</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">'sex'</span>: sexe,</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">'education'</span>: educ_level</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Algorithme du calcul de la fonction de coût</strong></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_cost_fn(x:np.ndarray, y:np.ndarray, w:np.array, b:<span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">        Calculates the cost function (MSE divided by 2)</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">        for multivariate linear regression.</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">            x (np.ndarray): Matrix of explanatory variables (shape: [m, n])</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">            y (np.ndarray): Vector of target values (shape: [m,])</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">            w (np.ndarray): Weight vector (shape: [n,])</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">            b (float): Bias (scalar)</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">            float: Value of the cost function</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> x<span class="op">@</span>w <span class="op">+</span> b</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    squared_error <span class="op">=</span> np.<span class="bu">sum</span>((y <span class="op">-</span> y_hat)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    cost_fn <span class="op">=</span> squared_error<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_fn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Algorithme du calcul du gradient de la fonction de coût</strong></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_gradient(x:np.ndarray, y:np.ndarray, w:np.array, b:<span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculates the gradient of the cost function (MSE) with respect to weights w and bias b.</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x (np.ndarray): Matrix of input features of form (m, n),</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">        where m is the number of observations</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">                        and n is the number of explanatory variables.</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">        y (np.ndarray): Vector of target values (m, ).</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">        w (np.ndarray): Weight vector (n, ).</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">        b (float): Scalar bias.</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: A pair (fw_prime, fb_prime) containing :</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">            - fw_prime (np.ndarray): The gradient of the cost function with respect to w, </span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">            of the form (n, ).</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">            - fb_prime (float): The gradient of the cost function with respect to b.</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> x <span class="op">@</span> w <span class="op">+</span> b</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> y_hat <span class="op">-</span> y</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    fw_prime <span class="op">=</span> (x.T <span class="op">@</span> error)<span class="op">/</span>m</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    fb_prime <span class="op">=</span> np.<span class="bu">sum</span>(error)<span class="op">/</span>m</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fw_prime, fb_prime</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><strong>Algorithme de la descente de gradient</strong></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> copy <span class="im">import</span> deepcopy</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    x: np.ndarray,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    y: np.ndarray,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    alpha: <span class="bu">float</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    w_in: np.array,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    b_in: <span class="bu">float</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    max_iter: <span class="bu">int</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    tolerance: <span class="bu">float</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    cost_fn: Callable,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    gradient_compute_fn: Callable) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Performs gradient descent to adjust</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">    the parameters w and b.</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">        x (np.ndarray): Input data (m, n)</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">        y (np.ndarray): Target (m,)</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co">        w_in (np.ndarray): Initial weight (n,)</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co">        b_in (float): Initial bias</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co">        alpha (float): Learning rate</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="co">        max_iter (int): Maximum number of iterations</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co">        tolerance (float): Convergence threshold</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co">        cost_fn (Callable): Cost function</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co">        gradient_compute_fn (Callable): Gradient calculation function</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co">        dict: History of parameters and cost at each iteration</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> deepcopy(w_in)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> b_in</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    cost_fn_and_params_hist <span class="op">=</span> {}</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        fw_i, fb_i <span class="op">=</span> gradient_compute_fn(x, y, w, b)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> w <span class="op">-</span> alpha<span class="op">*</span>fw_i</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> b <span class="op">-</span> alpha<span class="op">*</span>fb_i</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        cost_fn_i <span class="op">=</span> cost_fn(x, y, w, b)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        cost_fn_and_params_hist[i] <span class="op">=</span> {</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>            <span class="st">'w'</span>: w.copy(),</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>            <span class="st">'b'</span>: b,</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>            <span class="st">'cost_fn'</span>: cost_fn_i</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">15000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># you can adjust: I don't want to display all the iterations</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f'Iteraion </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> : </span><span class="ch">\n</span><span class="ss">|(w, b) = (</span><span class="sc">{</span>w<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>b<span class="sc">:4f}</span><span class="ss">) | cost_fn = </span><span class="sc">{</span>cost_fn_i<span class="sc">:4f}</span><span class="ss">|'</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(fw_i) <span class="op">&lt;</span> tolerance <span class="kw">and</span> <span class="bu">abs</span>(fb_i) <span class="op">&lt;</span> tolerance:</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f'The algorithm converges at the </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">-th iteration'</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f'</span><span class="ch">\n</span><span class="ss">Then we have (w, b) = (</span><span class="sc">{</span>w<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>b<span class="sc">:4f}</span><span class="ss">) &amp; cost_fn = </span><span class="sc">{</span>cost_fn_i<span class="sc">:4f}</span><span class="ss">'</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_fn_and_params_hist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
</section>
<section id="analyse-exploratoire-relation-entre-salaire-et-variables-explicatives" class="level2">
<h2 class="anchored" data-anchor-id="analyse-exploratoire-relation-entre-salaire-et-variables-explicatives">📊 Analyse exploratoire : Relation entre salaire et variables explicatives</h2>
<p>Ici on veut predire le salaire d’un individu en fonction de son niveau d’éducation, de son âge et de son sexe.</p>
<ul>
<li><strong>Premières lignes de la table</strong></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>df_5 <span class="op">=</span> df.loc[:<span class="dv">5</span>, :]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell tbl-cap-location-top">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>df_5_r <span class="ot">=</span> py<span class="sc">$</span>df_5</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">display_table</span>(df_5_r, <span class="st">""</span>, <span class="at">nrow_ =</span> <span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="tbl-first" class="anchored">
<table class="table table-sm table-striped small">
<caption>Table&nbsp;1: The first lines of our database</caption>
<thead>
<tr class="header">
<th style="text-align: left;">salary</th>
<th style="text-align: right;">ages</th>
<th style="text-align: right;">sex</th>
<th style="text-align: right;">education</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">75213</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">F</td>
<td style="text-align: right;">PhD</td>
</tr>
<tr class="even">
<td style="text-align: left;">45269</td>
<td style="text-align: right;">49</td>
<td style="text-align: right;">F</td>
<td style="text-align: right;">Msc</td>
</tr>
<tr class="odd">
<td style="text-align: left;">45060</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">M</td>
<td style="text-align: right;">Msc</td>
</tr>
<tr class="even">
<td style="text-align: left;">75109</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">F</td>
<td style="text-align: right;">PhD</td>
</tr>
<tr class="odd">
<td style="text-align: left;">45063</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">M</td>
<td style="text-align: right;">Msc</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<ul>
<li><strong>Résumé statistique de la table</strong></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>df.describe() <span class="co"># displays the statistical summary of numerical variables</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             salary         ages
count   1000.000000  1000.000000
mean   52498.163000    43.000000
std    20121.852889    12.945562
min    24882.000000    21.000000
25%    30139.000000    32.000000
50%    45133.000000    44.000000
75%    75065.000000    54.000000
max    75441.000000    65.000000</code></pre>
</div>
</div>
<ul>
<li><strong>Distribution du salaire en fonction des features</strong></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>X_features <span class="op">=</span> df.drop(<span class="st">'salary'</span>, axis<span class="op">=</span><span class="dv">1</span>).columns.to_list()</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>X_features</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['ages', 'sex', 'education']</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(axes)):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    axes[i].scatter(y<span class="op">=</span>df[<span class="st">'salary'</span>], x<span class="op">=</span>df[df.columns.to_list()[i <span class="op">+</span> <span class="dv">1</span>]])</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    axes[i].set_xlabel(X_features[i].capitalize(), fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    axes[i].set_ylabel(<span class="st">'Salary'</span>, size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    axes[i].set_xlabel(X_features[i].capitalize(), size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    axes[i].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    axes[i].set_title(<span class="ss">f'Relation between salary and </span><span class="sc">{</span>X_features[i]<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss">'</span>, size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-dist-plot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gradient-descent-linear-reg_files/figure-html/fig-dist-plot-1.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Distribution of variables as a function of salary</figcaption>
</figure>
</div>
</div>
</div>
<section id="relation-entre-le-salaire-et-lâge" class="level3">
<h3 class="anchored" data-anchor-id="relation-entre-le-salaire-et-lâge">👴 Relation entre le salaire et l’âge</h3>
<ul>
<li><strong>🔍 Observation :</strong>
<ul>
<li>Les salaires sont regroupés autour de quelques valeurs fixes (25k, 30k, 45k, 75k), indépendamment de l’âge.</li>
<li>La distribution semble en <strong>paliers</strong>, pas de tendance linéaire visible.</li>
</ul></li>
<li><strong>💡 Interprétation :</strong>
<ul>
<li>Il <strong>n’existe pas de relation linéaire claire</strong> entre l’âge et le salaire.</li>
<li>L’âge <strong>n’est pas un facteur explicatif majeur</strong> du salaire dans cet échantillon.</li>
<li>👉 Il faudrait explorer d’autres variables comme l’<code>éducation</code>, ect..</li>
</ul></li>
</ul>
<hr>
</section>
<section id="relation-entre-le-salaire-et-le-sexe" class="level3">
<h3 class="anchored" data-anchor-id="relation-entre-le-salaire-et-le-sexe">🚻 Relation entre le salaire et le sexe</h3>
<ul>
<li><strong>🔍 Observation :</strong>
<ul>
<li>Les salaires des femmes (<code>F</code>) et des hommes (<code>M</code>) sont répartis de manière similaire.</li>
<li>Aucune <strong>différence salariale flagrante</strong> n’apparaît sur ce graphique.</li>
</ul></li>
<li><strong>💡 Interprétation :</strong>
<ul>
<li>Le <strong>sexe ne semble pas avoir d’influence directe</strong> sur le salaire ici.</li>
<li>📌 Une analyse plus fine (ex. tests statistiques) serait nécessaire pour confirmer l’absence d’écart significatif (mais ce n’est pas le but ici, <a href="https://djamal2905.github.io/djamal_website/INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html">Cliquez ici pour voir une publication dans laquelle je réalise des tests statistiques afin de selectionner les variables essentielles pour la spécification de mon modèle</a>).</li>
</ul></li>
</ul>
<hr>
</section>
<section id="relation-entre-le-salaire-et-le-niveau-déducation" class="level3">
<h3 class="anchored" data-anchor-id="relation-entre-le-salaire-et-le-niveau-déducation">🎓 Relation entre le salaire et le niveau d’éducation</h3>
<ul>
<li><strong>🔍 Observation :</strong>
<ul>
<li>Le salaire <strong>augmente avec le niveau de diplôme</strong> :
<ul>
<li>PhD 🧑🔬 &gt; MSc 👨🎓 &gt; Bachelor 👨🏫 &gt; BAC 🎒</li>
</ul></li>
<li>Cette progression est claire et <strong>ordonnée</strong>.</li>
</ul></li>
<li><strong>💡 Interprétation :</strong>
<ul>
<li>Le <strong>niveau d’éducation est un facteur prédictif fort</strong> du salaire.</li>
<li>Il existe une <strong>relation croissante et logique</strong> entre diplôme obtenu et rémunération.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="conclusion-de-lanalyse-exploratoire-rapide" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-de-lanalyse-exploratoire-rapide">✅ Conclusion de l’analyse exploratoire rapide</h3>
<blockquote class="blockquote">
<p>🧠 <strong>De prime abord</strong>, on pourrait penser qu’il existe une relation non linéaire entre les variables explicatives et le salaire.<br>
En réalité, seule la variable <strong>🎓 niveau d’éducation</strong> montre une <strong>relation cohérente et croissante</strong>.<br>
Ni <strong>👴 l’âge</strong>, ni <strong>🚻 le sexe</strong> ne présentent d’influence claire sur le salaire.<br>
Cette analyse souligne l’importance d’une <strong>exploration visuelle et statistique</strong> avant toute modélisation.</p>
</blockquote>
<hr>
</section>
</section>
<section id="data-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="data-preprocessing">Data preprocessing</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code pour partitionner les données
</div>
</div>
<div class="callout-body-container callout-body">
<p>La fonction suivante permet de diviser vos données en ensembles d’entraînement et de test pour la validation du modèle. Il est important de s’assurer que les deux ensembles contiennent toutes les catégories ou modalités des variables catégorielles (<strong><em>indice : stratification</em></strong>). La fonction ci-dessous ne prend pas cela en charge automatiquement. Cependant, grâce à la taille de notre jeu de données et à l’utilisation de la graine aléatoire fixée à 42, nous pouvons garantir que ces caractéristiques sont bien présentes dans les deux ensembles.</p>
</div>
</div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> train[<span class="st">'education'</span>].unique()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>array([<span class="st">'Bachelor degree'</span>, <span class="st">'PhD'</span>, <span class="st">'Msc'</span>, <span class="st">'BAC'</span>], dtype<span class="op">=</span><span class="bu">object</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> test[<span class="st">'education'</span>].unique()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>array([<span class="st">'Msc'</span>, <span class="st">'PhD'</span>, <span class="st">'BAC'</span>, <span class="st">'Bachelor degree'</span>], dtype<span class="op">=</span><span class="bu">object</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_data_partition(df, train_ratio):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">        Creates a random partition of the DataFrame into training and test sets.</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">            df (pd.DataFrame): the complete DataFrame</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">            train_ratio (float): the proportion of rows to be used for training (ex: 0.8)</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">            tuple: (train_df, test_df)</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    nrow_df <span class="op">=</span> df.shape[<span class="dv">0</span>]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    nrow_train <span class="op">=</span> <span class="bu">round</span>(nrow_df<span class="op">*</span>train_ratio)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    train_idx <span class="op">=</span> np.random.choice(df.index, size<span class="op">=</span>nrow_train, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    test_idx <span class="op">=</span> df.index.difference(train_idx)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    train_df <span class="op">=</span> df.iloc[train_idx]</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    test_df <span class="op">=</span> df.iloc[test_idx]</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (train_df, test_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Une fois les données simulées, nous procédons à leur séparation en deux sous-ensembles distincts : un ensemble d’entraînement, utilisé pour ajuster le modèle, et un ensemble de test, destiné à l’évaluer. La fonction <code>create_data_partition</code> réalise une partition aléatoire selon un ratio défini (ici 80% pour l’entraînement, 20% pour le test). Ensuite, les variables explicatives (<code>ages</code>, <code>education</code>, <code>sex</code>) sont traitées via une pipeline de prétraitement : les variables numériques sont standardisées (centrées-réduites) tandis que les variables catégorielles sont transformées en indicatrices (encodage one-hot, sans la première modalité). Enfin, la variable cible (<code>salary</code>) est également standardisée pour assurer une convergence efficace de l’algorithme de descente de gradient.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data preprocessing</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># geting train and test datasets</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>train, test <span class="op">=</span> create_data_partition(df, <span class="fl">0.8</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># selecting features</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> train.drop(<span class="st">'salary'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># selecting target variable</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> train[<span class="st">'salary'</span>]</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># standardization of the target variable</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>scaler_y <span class="op">=</span> StandardScaler()</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>onehot_encoder_educ <span class="op">=</span> OneHotEncoder()</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>y_scaled <span class="op">=</span> scaler_y.fit_transform(y.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).flatten()</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># one-hot encodings &amp; standardization : pipeline treatment</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>numeric_features <span class="op">=</span> [<span class="st">'ages'</span>]</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>categorical_features <span class="op">=</span> [<span class="st">'education'</span>, <span class="st">'sex'</span>]</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>preprocessor <span class="op">=</span> ColumnTransformer(transformers<span class="op">=</span>[</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        (<span class="st">'num'</span>, StandardScaler(), numeric_features),</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        (<span class="st">'cat'</span>, OneHotEncoder(drop<span class="op">=</span><span class="st">'first'</span>), categorical_features)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="co"># transform numerical values by standardize them and categorical one by dummy them</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>X_processed <span class="op">=</span> preprocessor.fit_transform(X<span class="op">=</span>X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
⚠️ Important : <code>fit_transform()</code> vs <code>transform()</code>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Toujours utiliser <code>fit_transform()</code> <strong>uniquement sur les données d’entraînement</strong>, et <code>transform()</code> sur les données de validation ou de test.</p>
<p>Cela s’applique à <strong>tous les types de preprocessing</strong>, notamment :</p>
<ul>
<li>🟦 <strong>StandardScaler</strong> : la moyenne et l’écart-type doivent être appris sur le <em>train</em> uniquement.</li>
<li>🟧 <strong>OneHotEncoder</strong> : les catégories doivent être identifiées à partir du <em>train</em> et appliquées de manière cohérente au <em>test</em>.</li>
</ul>
<p>❌ Ne jamais faire <code>fit_transform()</code> sur le test, car cela introduit du <strong>data leakage</strong> (les données de test influencent le modèle).</p>
</div>
</div>
<hr>
</section>
<section id="application-de-lalgorithme-de-la-descente-de-gradient" class="level2">
<h2 class="anchored" data-anchor-id="application-de-lalgorithme-de-la-descente-de-gradient">Application de l’algorithme de la descente de gradient</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Comme mentionné précédemment, le choix du <code>learning rate</code> (ou taux d’apprentissage) est un paramètre crucial dans l’algorithme de descente de gradient. Un taux trop élevé peut empêcher la convergence du modèle, tandis qu’un taux trop faible peut rendre l’apprentissage extrêmement lent. Afin d’identifier un taux optimal, plusieurs valeurs sont testées, et celle qui permet de minimiser au mieux la fonction de coût est retenue.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="fl">1e-1</span>, <span class="fl">1e-2</span>] <span class="co"># , 1e-3, 1e-4, 1e-5 à ajouter si vous voulez, je veux juste reduire l'affichage</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> X_processed.shape[<span class="dv">1</span>]</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.zeros(n_features)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.randint(<span class="dv">100</span>, size<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="bu">int</span>(b)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>tolerance <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>max_iters <span class="op">=</span> <span class="dv">50000</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>cost_fn <span class="op">=</span> compute_cost_fn</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>gradient_fn <span class="op">=</span> compute_gradient</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>all_histories <span class="op">=</span> {}</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas:</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Runing the algorithm for alpha : </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.zeros(n_features)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="bu">int</span>(np.random.randint(<span class="dv">100</span>, size<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>])</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> gradient_descent(</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        X_processed,</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        y_scaled,</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span>alpha,</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        w_in<span class="op">=</span>w,</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        b_in<span class="op">=</span>b,</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        max_iter<span class="op">=</span>max_iters,</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        tolerance<span class="op">=</span>tolerance,</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        cost_fn<span class="op">=</span>cost_fn,</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        gradient_compute_fn<span class="op">=</span>gradient_fn)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    all_histories[<span class="bu">str</span>(alpha)] <span class="op">=</span> history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Runing the algorithm for alpha : 0.1

Iteraion 0 : 
|(w, b) = ([-0.00367445 -0.04214402 -0.03787291  0.00503207 -0.04740491], 0.900000) | cost_fn = 0.855335|
The algorithm converges at the 1687-th iteration
Then we have (w, b) = ([ 1.30308246e-03  2.22807855e-01  9.66238813e-01  2.45274791e+00
 -5.24625714e-04], -1.332871) &amp; cost_fn = 0.000037


Runing the algorithm for alpha : 0.01

Iteraion 0 : 
|(w, b) = ([-0.00036745 -0.0262144  -0.03431229 -0.04528429 -0.05781549], 11.880000) | cost_fn = 70.351204|
Iteraion 15000 : 
|(w, b) = ([ 2.13935724e-03  1.41295729e-01  8.87862367e-01  2.37691272e+00
 -1.16515913e-03], -1.261175) | cost_fn = 0.000502|
The algorithm converges at the 20966-th iteration
Then we have (w, b) = ([ 1.30383689e-03  2.22734320e-01  9.66168107e-01  2.45267950e+00
 -5.25203562e-04], -1.332807) &amp; cost_fn = 0.000037</code></pre>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ici, on constate directement que notre algorithme a convergé (critère de <em>tolérance</em>) pour chacun des taux d’apprentissage (<code>learning rate</code>) testés.</p>
<p>Après cet entraînement, il est intéressant de déterminer quel <code>learning rate</code> a permis d’obtenir le meilleur résultat. Le code suivant est particulièrement utile lorsqu’on teste plusieurs valeurs de <code>learning rate</code> (plus de deux).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>min_cost <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha, hist <span class="kw">in</span> all_histories.items():</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    costs <span class="op">=</span> [v[<span class="st">'cost_fn'</span>] <span class="cf">for</span> v <span class="kw">in</span> hist.values()]</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    min_cost_alpha <span class="op">=</span> <span class="bu">min</span>(costs)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Alpha </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss"> ➤ Min cost: </span><span class="sc">{</span>min_cost_alpha<span class="sc">:.5f}</span><span class="ss">"</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> min_cost_alpha <span class="op">&lt;</span> min_cost:</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        min_cost <span class="op">=</span> min_cost_alpha</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        best_alpha <span class="op">=</span> alpha</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Alpha 0.1 ➤ Min cost: 0.00004
Alpha 0.01 ➤ Min cost: 0.00004</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">✅ Best alpha: </span><span class="sc">{</span>best_alpha<span class="sc">}</span><span class="ss"> with min cost: </span><span class="sc">{</span>min_cost<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
✅ Best alpha: 0.1 with min cost: 0.00</code></pre>
</div>
</div>
<p>Avec ces deux <code>learning rates</code>, la fonction de coût atteint 0, ce qui signifie que l’algorithme a bien convergé puisque la fonction de coût est toujours positive.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_best_w_b(cost_fn_and_params_hist: <span class="bu">dict</span>):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    hist_len <span class="op">=</span> <span class="bu">len</span>(cost_fn_and_params_hist)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    best_of_hist <span class="op">=</span> cost_fn_and_params_hist[hist_len<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> best_of_hist</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(w, b, X):</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X <span class="op">@</span> w <span class="op">+</span> b</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> all_histories[<span class="st">'0.01'</span>]</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>best <span class="op">=</span> get_best_w_b(hist)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On peut maintenant calculer nos prédictions et vérifier leur qualité par rapport aux données réelles. Rappelons que les données ont été standardisées, il faudra donc ramener les prédictions à leur échelle d’origine pour une interprétation correcte.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># standardised predictions</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>ypred_scaled <span class="op">=</span> predict(best[<span class="st">'w'</span>], best[<span class="st">'b'</span>], X_processed).reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Standardised data (first 4 values): </span><span class="sc">{</span>ypred_scaled[:<span class="dv">4</span>]<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Standardised data (first 4 values): [[-1.11148687]
 [ 1.12097355]
 [ 1.11835771]
 [-1.1107826 ]]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># normal scale</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> scaler_y.inverse_transform(ypred_scaled)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Data on normal scale (first 4 values): </span><span class="sc">{</span>y_pred[:<span class="dv">4</span>]<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data on normal scale (first 4 values): [[30000.2280983 ]
 [75092.97779228]
 [75040.14127095]
 [30014.45331559]]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># display of optimal parameters</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Optimum parameters : </span><span class="sc">{</span>best<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimum parameters : {'w': array([ 1.30383689e-03,  2.22734320e-01,  9.66168107e-01,  2.45267950e+00,
       -5.25203562e-04]), 'b': -1.332806620067705, 'cost_fn': 3.722342588474543e-05}</code></pre>
</div>
</div>
<p>L’erreur quadratique moyenne sur les données d’apprentissage est donc <strong>0.00610</strong>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(axes)):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    axes[i].scatter(train[X_features[i]], train[<span class="st">'salary'</span>], label<span class="op">=</span><span class="st">'target'</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    axes[i].scatter(train[X_features[i]], y_pred, c<span class="op">=</span><span class="st">'orange'</span>, label<span class="op">=</span><span class="st">'predicted'</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    axes[i].set_ylabel(<span class="st">'Salary'</span>, size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    axes[i].set_xlabel(X_features[i].capitalize(), size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    axes[i].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    axes[i].set_title(<span class="ss">f'Relation between salary and </span><span class="sc">{</span>X_features[i]<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss">'</span>, size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-plot-prediction" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gradient-descent-linear-reg_files/figure-html/fig-plot-prediction-3.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Predcitions vs Real data</figcaption>
</figure>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On peut voir que l’ajustement est très satisfaisant, mais il reste à déterminer s’il ne s’agit pas d’un <code>surapprentissage</code>. Pour cela nous allons utiliser les données de test, pour évaluer le modèle entrainé.</p>
</section>
<section id="evaluation-du-modèle" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-du-modèle">Evaluation du modèle</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pour évaluer le modèle nous utilisons la <strong>validation croisée</strong> car elle est robuste en terme d’évaluation de performance d’un modèle.</p>
<ul>
<li><p><strong>But principal</strong> : Évaluer la performance d’un modèle de manière fiable et robuste, en réduisant le biais dû à une simple séparation train/test.</p></li>
<li><p><strong>Étape 1 : Partition des données</strong><br>
Diviser l’ensemble des données en <em>k</em> sous-ensembles (ou « folds ») de taille à peu près égale.</p></li>
<li><p><strong>Étape 2 : Boucle sur les folds</strong><br>
Pour chaque fold (de 1 à k) :</p>
<ul>
<li>Utiliser ce fold comme ensemble de test (validation).<br>
</li>
<li>Utiliser les <em>k-1</em> autres folds comme ensemble d’entraînement.</li>
</ul></li>
<li><p><strong>Étape 3 : Entraînement</strong><br>
Entraîner le modèle uniquement sur les données d’entraînement (les <em>k-1</em> folds).</p></li>
<li><p><strong>Étape 4 : Évaluation</strong><br>
Tester le modèle entraîné sur le fold de test (le fold laissé de côté), calculer une métrique de performance (ex : erreur quadratique moyenne).</p></li>
<li><p><strong>Étape 5 : Agrégation</strong><br>
Répéter les étapes 2 à 4 pour chaque fold, puis calculer la moyenne (et éventuellement l’écart-type) des performances obtenues sur chaque fold.</p></li>
<li><p><strong>Avantages</strong> :</p>
<ul>
<li>Meilleure estimation de la généralisation du modèle sur des données nouvelles.<br>
</li>
<li>Réduit le sur-apprentissage lié à un seul découpage train/test.<br>
</li>
<li>Utilise efficacement toutes les données pour entraînement et validation.</li>
</ul></li>
<li><p><strong>Inconvénients</strong> :</p>
<ul>
<li>Coût computationnel plus élevé, car le modèle est entraîné <em>k</em> fois.<br>
</li>
<li>Peut être sensible au choix de <em>k</em> (souvent 5 ou 10).</li>
</ul></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-kfold-illustration" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="k_fold.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Illustration of a cross-validation process</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> r2_score</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> k_fold_cross_validation(df, k, alpha, max_iters, tolerance, cost_fn, gradient_fn):</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform k-fold cross-validation using gradient descent.</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): full dataset (training set)</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co">        k (int): number of folds</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co">        alpha (float): learning rate</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co">        max_iters (int): maximum number of iterations for gradient descent</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co">        tolerance (float): convergence threshold</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co">        cost_fn (Callable): cost function</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="co">        gradient_fn (Callable): gradient computation function</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co">        dict: dictionnaire avec listes des mse_train, mse_test, r2_train, r2_test</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Shuffle the DataFrame index</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.sample(frac<span class="op">=</span><span class="dv">1</span>).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    fold_size <span class="op">=</span> <span class="bu">len</span>(df) <span class="op">//</span> k</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    mse_train_list <span class="op">=</span> []</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>    mse_test_list <span class="op">=</span> []</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>    r2_train_list <span class="op">=</span> []</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>    r2_test_list <span class="op">=</span> []</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fold <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define start and end indices for the test fold</span></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> fold <span class="op">*</span> fold_size</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>        end <span class="op">=</span> (fold <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> fold_size <span class="cf">if</span> fold <span class="op">!=</span> k <span class="op">-</span> <span class="dv">1</span> <span class="cf">else</span> <span class="bu">len</span>(df)</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split the data into test and train folds</span></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>        test_df <span class="op">=</span> df.iloc[start:end]</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>        train_df <span class="op">=</span> pd.concat([df.iloc[:start], df.iloc[end:]], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prepare features and target for train and test</span></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>        X_train <span class="op">=</span> train_df.drop(<span class="st">'salary'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>        y_train <span class="op">=</span> train_df[<span class="st">'salary'</span>]</span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>        X_test <span class="op">=</span> test_df.drop(<span class="st">'salary'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a>        y_test <span class="op">=</span> test_df[<span class="st">'salary'</span>]</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'-'</span><span class="op">*</span><span class="dv">10</span>)</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Fold </span><span class="sc">{</span>fold<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Train unique education:"</span>, train_df[<span class="st">'education'</span>].unique())</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Test unique education:"</span>, test_df[<span class="st">'education'</span>].unique())</span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Train unique sex:"</span>, train_df[<span class="st">'sex'</span>].unique())</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Test unique sex:"</span>, test_df[<span class="st">'sex'</span>].unique())</span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb30-50"><a href="#cb30-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-51"><a href="#cb30-51" aria-hidden="true" tabindex="-1"></a>        numeric_features <span class="op">=</span> [<span class="st">'ages'</span>]</span>
<span id="cb30-52"><a href="#cb30-52" aria-hidden="true" tabindex="-1"></a>        categorical_features <span class="op">=</span> [<span class="st">'education'</span>, <span class="st">'sex'</span>]</span>
<span id="cb30-53"><a href="#cb30-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-54"><a href="#cb30-54" aria-hidden="true" tabindex="-1"></a>        preprocessor <span class="op">=</span> ColumnTransformer(transformers<span class="op">=</span>[</span>
<span id="cb30-55"><a href="#cb30-55" aria-hidden="true" tabindex="-1"></a>            (<span class="st">'num'</span>, StandardScaler(), numeric_features),</span>
<span id="cb30-56"><a href="#cb30-56" aria-hidden="true" tabindex="-1"></a>            (<span class="st">'cat'</span>, OneHotEncoder(drop<span class="op">=</span><span class="st">'first'</span>), categorical_features)</span>
<span id="cb30-57"><a href="#cb30-57" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb30-58"><a href="#cb30-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-59"><a href="#cb30-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Preprocess the data using our pipeline</span></span>
<span id="cb30-60"><a href="#cb30-60" aria-hidden="true" tabindex="-1"></a>        X_train_processed <span class="op">=</span> preprocessor.fit_transform(X_train)</span>
<span id="cb30-61"><a href="#cb30-61" aria-hidden="true" tabindex="-1"></a>        y_train_scaled <span class="op">=</span> scaler_y.fit_transform(y_train.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).flatten()</span>
<span id="cb30-62"><a href="#cb30-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-63"><a href="#cb30-63" aria-hidden="true" tabindex="-1"></a>        X_test_processed <span class="op">=</span> preprocessor.transform(X_test)</span>
<span id="cb30-64"><a href="#cb30-64" aria-hidden="true" tabindex="-1"></a>        y_test_scaled <span class="op">=</span> scaler_y.transform(y_test.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).flatten()</span>
<span id="cb30-65"><a href="#cb30-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-66"><a href="#cb30-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize parameters</span></span>
<span id="cb30-67"><a href="#cb30-67" aria-hidden="true" tabindex="-1"></a>        n_features <span class="op">=</span> X_train_processed.shape[<span class="dv">1</span>]</span>
<span id="cb30-68"><a href="#cb30-68" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> np.zeros(n_features)</span>
<span id="cb30-69"><a href="#cb30-69" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-70"><a href="#cb30-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-71"><a href="#cb30-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run gradient descent on training data</span></span>
<span id="cb30-72"><a href="#cb30-72" aria-hidden="true" tabindex="-1"></a>        history <span class="op">=</span> gradient_descent(</span>
<span id="cb30-73"><a href="#cb30-73" aria-hidden="true" tabindex="-1"></a>            X_train_processed,</span>
<span id="cb30-74"><a href="#cb30-74" aria-hidden="true" tabindex="-1"></a>            y_train_scaled,</span>
<span id="cb30-75"><a href="#cb30-75" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">=</span>alpha,</span>
<span id="cb30-76"><a href="#cb30-76" aria-hidden="true" tabindex="-1"></a>            w_in<span class="op">=</span>w,</span>
<span id="cb30-77"><a href="#cb30-77" aria-hidden="true" tabindex="-1"></a>            b_in<span class="op">=</span>b,</span>
<span id="cb30-78"><a href="#cb30-78" aria-hidden="true" tabindex="-1"></a>            max_iter<span class="op">=</span>max_iters,</span>
<span id="cb30-79"><a href="#cb30-79" aria-hidden="true" tabindex="-1"></a>            tolerance<span class="op">=</span>tolerance,</span>
<span id="cb30-80"><a href="#cb30-80" aria-hidden="true" tabindex="-1"></a>            cost_fn<span class="op">=</span>cost_fn,</span>
<span id="cb30-81"><a href="#cb30-81" aria-hidden="true" tabindex="-1"></a>            gradient_compute_fn<span class="op">=</span>gradient_fn</span>
<span id="cb30-82"><a href="#cb30-82" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-83"><a href="#cb30-83" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-84"><a href="#cb30-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Retrieve best parameters from last iteration</span></span>
<span id="cb30-85"><a href="#cb30-85" aria-hidden="true" tabindex="-1"></a>        last_iter <span class="op">=</span> <span class="bu">max</span>(history.keys())</span>
<span id="cb30-86"><a href="#cb30-86" aria-hidden="true" tabindex="-1"></a>        w_best <span class="op">=</span> history[last_iter][<span class="st">'w'</span>]</span>
<span id="cb30-87"><a href="#cb30-87" aria-hidden="true" tabindex="-1"></a>        b_best <span class="op">=</span> history[last_iter][<span class="st">'b'</span>]</span>
<span id="cb30-88"><a href="#cb30-88" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-89"><a href="#cb30-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predictions</span></span>
<span id="cb30-90"><a href="#cb30-90" aria-hidden="true" tabindex="-1"></a>        y_pred_train <span class="op">=</span> X_train_processed <span class="op">@</span> w_best <span class="op">+</span> b_best</span>
<span id="cb30-91"><a href="#cb30-91" aria-hidden="true" tabindex="-1"></a>        y_pred_test <span class="op">=</span> X_test_processed <span class="op">@</span> w_best <span class="op">+</span> b_best</span>
<span id="cb30-92"><a href="#cb30-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-93"><a href="#cb30-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute cost function (cost_fn returns half MSE)</span></span>
<span id="cb30-94"><a href="#cb30-94" aria-hidden="true" tabindex="-1"></a>        train_cost_half <span class="op">=</span> cost_fn(X_train_processed, y_train_scaled, w_best, b_best)</span>
<span id="cb30-95"><a href="#cb30-95" aria-hidden="true" tabindex="-1"></a>        test_cost_half <span class="op">=</span> cost_fn(X_test_processed, y_test_scaled, w_best, b_best)</span>
<span id="cb30-96"><a href="#cb30-96" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-97"><a href="#cb30-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute full MSE (because Gradient descent divided it by 2)</span></span>
<span id="cb30-98"><a href="#cb30-98" aria-hidden="true" tabindex="-1"></a>        train_mse <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> train_cost_half</span>
<span id="cb30-99"><a href="#cb30-99" aria-hidden="true" tabindex="-1"></a>        test_mse <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> test_cost_half</span>
<span id="cb30-100"><a href="#cb30-100" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-101"><a href="#cb30-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute R2 scores</span></span>
<span id="cb30-102"><a href="#cb30-102" aria-hidden="true" tabindex="-1"></a>        train_r2 <span class="op">=</span> r2_score(y_train_scaled, y_pred_train)</span>
<span id="cb30-103"><a href="#cb30-103" aria-hidden="true" tabindex="-1"></a>        test_r2 <span class="op">=</span> r2_score(y_test_scaled, y_pred_test)</span>
<span id="cb30-104"><a href="#cb30-104" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-105"><a href="#cb30-105" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Fold </span><span class="sc">{</span>fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> - Train MSE: </span><span class="sc">{</span>train_mse<span class="sc">:.5f}</span><span class="ss"> | Test MSE: </span><span class="sc">{</span>test_mse<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb30-106"><a href="#cb30-106" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Fold </span><span class="sc">{</span>fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> - Train RMSE: </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(train_mse)<span class="sc">:.5f}</span><span class="ss"> | Test RMSE: </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(test_mse)<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb30-107"><a href="#cb30-107" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Fold </span><span class="sc">{</span>fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> - Train R²: </span><span class="sc">{</span>train_r2<span class="sc">:.5f}</span><span class="ss"> | Test R²: </span><span class="sc">{</span>test_r2<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb30-108"><a href="#cb30-108" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-109"><a href="#cb30-109" aria-hidden="true" tabindex="-1"></a>        mse_train_list.append(train_mse)</span>
<span id="cb30-110"><a href="#cb30-110" aria-hidden="true" tabindex="-1"></a>        mse_test_list.append(test_mse)</span>
<span id="cb30-111"><a href="#cb30-111" aria-hidden="true" tabindex="-1"></a>        r2_train_list.append(train_r2)</span>
<span id="cb30-112"><a href="#cb30-112" aria-hidden="true" tabindex="-1"></a>        r2_test_list.append(test_r2)</span>
<span id="cb30-113"><a href="#cb30-113" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-114"><a href="#cb30-114" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'-'</span><span class="op">*</span><span class="dv">10</span>)</span>
<span id="cb30-115"><a href="#cb30-115" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span><span class="op">*</span><span class="dv">3</span>)</span>
<span id="cb30-116"><a href="#cb30-116" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-117"><a href="#cb30-117" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\n</span><span class="ss">Median Train MSE over </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> folds: </span><span class="sc">{</span>np<span class="sc">.</span>median(mse_train_list)<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb30-118"><a href="#cb30-118" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Median Test MSE over </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> folds: </span><span class="sc">{</span>np<span class="sc">.</span>median(mse_test_list)<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb30-119"><a href="#cb30-119" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Median Train R² over </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> folds: </span><span class="sc">{</span>np<span class="sc">.</span>median(r2_train_list)<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb30-120"><a href="#cb30-120" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Median Test R² over </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> folds: </span><span class="sc">{</span>np<span class="sc">.</span>median(r2_test_list)<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb30-121"><a href="#cb30-121" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span>  {</span>
<span id="cb30-122"><a href="#cb30-122" aria-hidden="true" tabindex="-1"></a>        <span class="st">"mse_train"</span>: mse_train_list,</span>
<span id="cb30-123"><a href="#cb30-123" aria-hidden="true" tabindex="-1"></a>        <span class="st">"mse_test"</span>: mse_test_list,</span>
<span id="cb30-124"><a href="#cb30-124" aria-hidden="true" tabindex="-1"></a>        <span class="st">"r2_train"</span>: r2_train_list,</span>
<span id="cb30-125"><a href="#cb30-125" aria-hidden="true" tabindex="-1"></a>        <span class="st">"r2_test"</span>: r2_test_list</span>
<span id="cb30-126"><a href="#cb30-126" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb30-127"><a href="#cb30-127" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>max_iters <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>tolerance <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> k_fold_cross_validation(train, k, alpha, max_iters, tolerance, compute_cost_fn, compute_gradient)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>----------

Fold 1:
Train unique education: ['Bachelor degree' 'Msc' 'PhD' 'BAC']
Test unique education: ['PhD' 'Msc' 'Bachelor degree' 'BAC']
Train unique sex: ['F' 'M']
Test unique sex: ['F' 'M']

Fold 1/5 - Train MSE: 0.00113 | Test MSE: 0.00112
Fold 1/5 - Train RMSE: 0.03362 | Test RMSE: 0.03346
Fold 1/5 - Train R²: 0.99887 | Test R²: 0.99889
----------




----------

Fold 2:
Train unique education: ['PhD' 'Msc' 'Bachelor degree' 'BAC']
Test unique education: ['Bachelor degree' 'Msc' 'PhD' 'BAC']
Train unique sex: ['F' 'M']
Test unique sex: ['F' 'M']

Fold 2/5 - Train MSE: 0.00106 | Test MSE: 0.00099
Fold 2/5 - Train RMSE: 0.03263 | Test RMSE: 0.03142
Fold 2/5 - Train R²: 0.99894 | Test R²: 0.99894
----------




----------

Fold 3:
Train unique education: ['PhD' 'Msc' 'Bachelor degree' 'BAC']
Test unique education: ['Bachelor degree' 'PhD' 'Msc' 'BAC']
Train unique sex: ['F' 'M']
Test unique sex: ['F' 'M']

Fold 3/5 - Train MSE: 0.00090 | Test MSE: 0.00067
Fold 3/5 - Train RMSE: 0.03005 | Test RMSE: 0.02598
Fold 3/5 - Train R²: 0.99910 | Test R²: 0.99932
----------




----------

Fold 4:
Train unique education: ['PhD' 'Msc' 'Bachelor degree' 'BAC']
Test unique education: ['PhD' 'Bachelor degree' 'BAC' 'Msc']
Train unique sex: ['F' 'M']
Test unique sex: ['F' 'M']

Fold 4/5 - Train MSE: 0.00155 | Test MSE: 0.00231
Fold 4/5 - Train RMSE: 0.03941 | Test RMSE: 0.04801
Fold 4/5 - Train R²: 0.99845 | Test R²: 0.99786
----------




----------

Fold 5:
Train unique education: ['PhD' 'Msc' 'Bachelor degree' 'BAC']
Test unique education: ['Msc' 'PhD' 'BAC' 'Bachelor degree']
Train unique sex: ['F' 'M']
Test unique sex: ['F' 'M']

Fold 5/5 - Train MSE: 0.00106 | Test MSE: 0.00100
Fold 5/5 - Train RMSE: 0.03251 | Test RMSE: 0.03157
Fold 5/5 - Train R²: 0.99894 | Test R²: 0.99898
----------





Median Train MSE over 5 folds: 0.00106
Median Test MSE over 5 folds: 0.00100
Median Train R² over 5 folds: 0.99894
Median Test R² over 5 folds: 0.99894</code></pre>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On peut remaquer qu’à chaque fold nos modalités sont présentes et dans les données de test et dans celles de l’apprentissage.</p>
<p>La validation croisée en 5 plis (K-Fold Cross-Validation) a été utilisée pour évaluer la performance du modèle. Les valeurs ci-dessous correspondent à la racine carrée de la fonction de coût (Root Mean Square Error, RMSE), <strong>calculée sur des données de sortie standardisées</strong>.</p>
<section id="résultats-par-pli-rmse-train-vs-test" class="level3">
<h3 class="anchored" data-anchor-id="résultats-par-pli-rmse-train-vs-test">Résultats par pli (RMSE Train vs Test)</h3>
<ul>
<li>✅ Fold 1 :<code>0.03362 vs 0.03346</code></li>
<li>✅ Fold 2 :<code>0.03263 vs 0.03142</code></li>
<li>✅ Fold 3 :<code>0.03005 vs 0.02598</code></li>
<li>✅ Fold 4 :<code>0.03941 vs 0.04801</code></li>
<li>✅ Fold 5 :<code>0.03251 vs 0.03157</code></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dans 4 des 5 folds, le MSE_test ≤ MSE_train, ce qui est surprenant mais pas impossible si la validation est bien faite et que les données sont très régulières.</p>
<p>L’écart est faible dans tous les cas sauf pour le fold 4, où :</p>
<p><code>MSE_test</code> = <code>0.00231</code> <strong>&gt;</strong> <code>MSE_train</code> = <code>0.00155</code></p>
<p>Cela suggère un <code>légère suradaptation</code> (overfitting) pour ce fold, mais l’<code>écart reste raisonnable</code>.</p>
</section>
<section id="visualisation-de-la-somme-erreurs-quadratiques-par-pli" class="level3">
<h3 class="anchored" data-anchor-id="visualisation-de-la-somme-erreurs-quadratiques-par-pli">Visualisation de la somme erreurs quadratiques par pli</h3>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>mse_train_list <span class="op">=</span> results[<span class="st">'mse_train'</span>]</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>mse_test_list <span class="op">=</span> results[<span class="st">'mse_test'</span>]</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>r2_train_list <span class="op">=</span> results[<span class="st">'r2_train'</span>]</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>r2_test_list <span class="op">=</span> results[<span class="st">'r2_test'</span>]</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>mse_median_train <span class="op">=</span> np.median(mse_train_list)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>mse_median_test <span class="op">=</span> np.median(mse_test_list)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>r2_median_train <span class="op">=</span> np.median(r2_train_list)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>r2_median_test <span class="op">=</span> np.median(r2_test_list)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>folds <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="fl">6.5</span>))</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="co"># --MSE--</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(folds, mse_train_list, label<span class="op">=</span><span class="st">'Train MSE'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(folds, mse_test_list, label<span class="op">=</span><span class="st">'Test MSE'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axhline(y<span class="op">=</span>mse_median_train, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, label<span class="op">=</span><span class="ss">f'Train Median MSE = </span><span class="sc">{</span>mse_median_train<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axhline(y<span class="op">=</span>mse_median_test, color<span class="op">=</span><span class="st">'g'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, label<span class="op">=</span><span class="ss">f'Test Median MSE = </span><span class="sc">{</span>mse_median_test<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'MSE per fold'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Fold'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'MSE (standardized scale)'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].tick_params(axis<span class="op">=</span><span class="st">'both'</span>, which<span class="op">=</span><span class="st">'major'</span>, labelsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a><span class="co"># --R2--</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(folds, r2_train_list, label<span class="op">=</span><span class="st">'Train R²'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(folds, r2_test_list, label<span class="op">=</span><span class="st">'Test R²'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axhline(y<span class="op">=</span>r2_median_train, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, label<span class="op">=</span><span class="ss">f'Train Median R² = </span><span class="sc">{</span>r2_median_train<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axhline(y<span class="op">=</span>r2_median_test, color<span class="op">=</span><span class="st">'g'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, label<span class="op">=</span><span class="ss">f'Test Median R² = </span><span class="sc">{</span>r2_median_test<span class="sc">:.5f}</span><span class="ss">'</span>)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'R² per fold'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Fold'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'R²'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].tick_params(axis<span class="op">=</span><span class="st">'both'</span>, which<span class="op">=</span><span class="st">'major'</span>, labelsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(left<span class="op">=</span><span class="fl">0.1</span>, wspace<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-rmse-value-plot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gradient-descent-linear-reg_files/figure-html/fig-rmse-value-plot-1.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Model Performance Metrics per Fold</figcaption>
</figure>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ce graphique présente les performances d’un modèle à travers une validation croisée à 5 plis (<strong>5-fold cross-validation</strong>), en utilisant deux métriques : <strong>MSE (Mean Squared Error)</strong> et <strong>R² (coefficient de détermination)</strong>.</p>
</section>
<section id="mse-par-pli" class="level3">
<h3 class="anchored" data-anchor-id="mse-par-pli">📉 MSE par pli</h3>
<ul>
<li><strong>Lignes bleues (Train MSE)</strong> et <strong>lignes orange (Test MSE)</strong> montrent la performance sur les données d’entraînement et de test respectivement.</li>
<li>Les performances sont très similaires entre les différents plis, avec de <strong>faibles valeurs de MSE</strong>, ce qui indique une bonne qualité de prédiction.</li>
<li>Une exception est observée pour le pli 4, où le <strong>Test MSE</strong> est sensiblement plus élevé (~0.0023), suggérant une possible instabilité ou une complexité locale non bien capturée par le modèle.</li>
<li><strong>Lignes horizontales</strong> :
<ul>
<li>🔴 Rouge pointillée : médiane du MSE d’entraînement (≈ 0.00106)</li>
<li>🟢 Verte pointillée : médiane du MSE de test (≈ 0.00100)</li>
</ul></li>
</ul>
<blockquote class="blockquote">
<p><strong>Interprétation</strong> : Le modèle généralise bien dans l’ensemble, avec un <strong>léger sur-apprentissage</strong> possible sur le pli 4.</p>
</blockquote>
</section>
<section id="r²-par-pli" class="level3">
<h3 class="anchored" data-anchor-id="r²-par-pli">📈 R² par pli</h3>
<ul>
<li><p>Le <strong>R² d’entraînement</strong> et <strong>de test</strong> est très élevé pour tous les plis, indiquant que le modèle explique <strong>plus de 99.88 %</strong> de la variance des données.</p></li>
<li><p>Une baisse est observée pour le pli 4 avec un <strong>Test R² ≈ 0.9978</strong>, ce qui reste néanmoins très performant.</p></li>
<li><p><strong>Lignes horizontales</strong> :</p>
<ul>
<li>🔴 Rouge pointillée : médiane du R² d’entraînement (≈ 0.99894)</li>
<li>🟢 Verte pointillée : médiane du R² de test (≈ 0.99894)</li>
</ul></li>
</ul>
<hr>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;En somme nous pouvons dirre que :</p>
<ul>
<li><p>Le modèle est <strong>très performant</strong>, avec des <strong>erreurs faibles</strong> et un <strong>pouvoir explicatif élevé</strong> sur l’ensemble des plis.</p></li>
<li><p>Le pli 4 montre une <strong>légère instabilité</strong>, ce qui pourrait justifier une exploration complémentaire sur les données de ce pli.</p></li>
<li><p>Dans l’ensemble, les résultats montrent un <strong>excellent compromis biais-variance</strong>, avec une <strong>bonne généralisation</strong>.</p></li>
</ul>
</section>
</section>
<section id="evaluation-du-modèle-final-sur-les-données-de-test" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-du-modèle-final-sur-les-données-de-test">Evaluation du modèle final sur les données de test</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># selecting test features</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test.drop(<span class="st">'salary'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># transforming test features (one hot encodings and standardization)</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>X_test_processed <span class="op">=</span> preprocessor.transform(X<span class="op">=</span>X_test)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions on standardized test features</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>y_pred_test_scaled <span class="op">=</span> predict(best[<span class="st">'w'</span>], best[<span class="st">'b'</span>], X_test_processed).reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co"># reverse scaling</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> scaler_y.inverse_transform(y_pred_test_scaled).reshape(<span class="dv">200</span>,)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co"># getting real y and changing the shape</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>y_true_test <span class="op">=</span> test[<span class="st">'salary'</span>].values.reshape(<span class="dv">200</span>,)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Maintenant calculons le <code>RMSE</code> des erreurs commises en prédisant. D’abord essayons de visualiser cela de manière séparée car il me vient à l’idée d’ordonner les différents arrays obtenus, mais avant faut que je sois sûre que les salaires réels (pas dans le sens économique du terme mais pour dire salaire observé) et ceux prédits ont la même allure.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>abs_ <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">1</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(abs_, y_true_test, c<span class="op">=</span><span class="st">'g'</span>, label<span class="op">=</span><span class="st">'Observed salaries'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Observed Salaries'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Data points'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Salary'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(abs_, y_pred_test, c<span class="op">=</span><span class="st">'b'</span>, label<span class="op">=</span><span class="st">'Predicted salaries'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Predicted Salaries'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Data points'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Salary'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-testing-final-plot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gradient-descent-linear-reg_files/figure-html/fig-testing-final-plot-3.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Observed Slaries and Predicted Salaries visualisation</figcaption>
</figure>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;En effet les deux courbe ont pratiquement la même allure, essayons de voir ce que ça donne quand on les arrange de manière croissante.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>y_pred_test_sorted <span class="op">=</span> np.sort(y_pred_test)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>y_true_test_sorted <span class="op">=</span> np.sort(y_true_test)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>abs_ <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">1</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>plt.plot(abs_,y_true_test_sorted , c<span class="op">=</span><span class="st">'g'</span>, label<span class="op">=</span><span class="st">'Observed values of the salary'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>plt.plot(abs_, y_pred_test_sorted, c<span class="op">=</span><span class="st">'b'</span>,label<span class="op">=</span><span class="st">'Predicted values of the salary'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Salary'</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Data points'</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-testing-final-sorted-plot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gradient-descent-linear-reg_files/figure-html/fig-testing-final-sorted-plot-5.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Observed Slaries VS Predicted Salaries</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Analyse graphique des salaires prédits vs observés
</div>
</div>
<div class="callout-body-container callout-body">
<p>Les courbes des salaires prédits et observés montrent une forte similarité.<br>
Cela suggère que le modèle est capable de bien capturer la relation entre les variables explicatives et le salaire, indiquant ainsi une <strong>bonne capacité de généralisation</strong> sur les données de test.</p>
</div>
</div>
<ul>
<li><strong>Calcul de la racine de l’erreur quadratique moyenne</strong></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># RMSE on standardised salaries</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>y_true_test_scaled <span class="op">=</span> scaler_y.transform(y_true_test_).reshape(<span class="dv">200</span>, )</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>rmse_scaled <span class="op">=</span> np.sqrt(np.mean((y_pred_test_scaled <span class="op">-</span> y_true_test_scaled)<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"standardized RMSE : </span><span class="sc">{</span>rmse_scaled<span class="sc">:.2f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>standardized RMSE : 1.38</code></pre>
</div>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># RMSE on original wages</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>rmse_original <span class="op">=</span> np.sqrt(</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="op">/</span><span class="dv">200</span>)<span class="op">*</span>(y_pred_test <span class="op">-</span> y_true_test).T <span class="op">@</span> (y_pred_test <span class="op">-</span> y_true_test)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"original RMSE : </span><span class="sc">{</span>rmse_original<span class="sc">:.2f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>original RMSE : 283.96</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled" title="⚠️ Attention à l’interprétation : overfitting ou déséquilibre ?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
⚠️ Attention à l’interprétation : overfitting ou déséquilibre ?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Le fait que la <code>RMSE</code> sur le test soit nettement plus élevée que celle obtenue en validation croisée peut provenir de :</p>
<p>🔁 Overfitting : le modèle a trop appris les spécificités du jeu d’entraînement (même via cross-validation), et généralise mal (ce qui est moins probable).</p>
<p>⚖️ Déséquilibre dans les variables catégorielles : il se peut que la répartition des niveaux d’éducation ou des sexes soit différente entre le train et le test, ce qui fausse l’évaluation (Ce qui est plus probable car au debut j’avais mentionné ce soucis).</p>
<p>Il est donc essentiel d’examiner les distributions des variables dans chaque ensemble pour comprendre la cause exacte.</p>
<p>🧑‍🏫 Mais rappelons que le but ici n’était pas de construire le meilleur modèle, mais de montrer concrètement la mise en œuvre de l’algorithme de descente de gradient en contexte réel.</p>
<p>Ce genre d’écart illustre parfaitement pourquoi la généralisation est un défi fondamental en machine learning.</p>
</div>
</div>
<ul>
<li><strong>Interprétation de la RMSE en valeur réelle</strong></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Les salaires dans notre dataset varient entre environ <strong>24 933 €</strong> et <strong>75 318 €</strong>, ce qui explique qu’une <code>RMSE</code> d’environ <code>284 €</code> soit cohérente.</p>
<p>Pour mieux comprendre ce que signifie cette erreur moyenne, considérons l’échelle des salaires :<br>
la différence entre le minimum et le maximum est d’environ <strong>50 385 €</strong>.</p>
<p>Ainsi, une <code>RMSE</code> de <code>284 €</code> correspond à une erreur moyenne relative d’environ :</p>
<p><span class="math display">\[
\frac{284}{50 385} \approx 0.0056 \quad \text{soit} \quad 0{,}56\%
\]</span></p>
<hr>
</section>
<section id="conclusion-générale" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-générale">Conclusion générale</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;En résumé, la <strong>descente de gradient</strong> s’est révélée être un algorithme simple, intuitif et pourtant puissant pour la <strong>minimisation de la fonction de coût</strong> dans le cadre de la régression linéaire. En parcourant de manière itérative la direction opposée au gradient, l’algorithme permet une réduction monotone du coût à chaque itération.</p>
<p>Nous avons vu que ce procédé consiste à ajuster les coefficients du modèle à partir d’un point initial, en effectuant des pas proportionnels au négatif du gradient et calibrés par le <strong>taux d’apprentissage</strong>. Tant que celui-ci est bien choisi (pas trop grand, pas trop petit), la procédure converge vers un minimum local de la fonction de coût. Toutefois, nous avons aussi montré que certains cas (comme des ravins étroits ou des matrices Hessiennes mal conditionnées) peuvent ralentir la convergence ou provoquer des oscillations.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>La faible erreur en euros (300 sur des milliers) sur les données de test, combinée à une RMSE relativement faible en validation croisée, suggère que le modèle est bien calibré et capable de généraliser efficacement. Cependant, l’écart entre l’entraînement et la validation pourrait indiquer un léger surapprentissage, bien que cet écart soit modéré</strong>.</p>
<section id="points-clés-à-retenir" class="level3">
<h3 class="anchored" data-anchor-id="points-clés-à-retenir">Points clés à retenir</h3>
<ol type="1">
<li><p><strong>Principe de fonctionnement</strong><br>
Le paramètre <span class="math inline">\(\theta\)</span> est mis à jour selon <span class="math inline">\(\theta \leftarrow \theta - \gamma \nabla_\theta J(\theta)\)</span>, avec <span class="math inline">\(J\)</span> la fonction de coût et <span class="math inline">\(\alpha\)</span> le pas d’apprentissage.</p></li>
<li><p><strong>Monotonie et convergence</strong></p>
<p>À chaque étape, la valeur du coût décroît (tant que <span class="math inline">\(\alpha\)</span> est convenablement choisie), garantissant la progression vers un minimum local.</p></li>
<li><p><strong>Limites de la méthode</strong></p>
<p>La descente de gradient peut rencontrer des difficultés de convergence lorsqu’on affine le minimum dans des zones étroites ou avec des gradients très pontuels. D’autres méthodes, comme le <code>gradient conjugué</code> ou <code>Newton</code>, peuvent alors offrir un gain en performance.</p></li>
</ol>
<div class="callout callout-style-default callout-warning callout-titled" title="⚠️ Attention : Influence de l'échelle des variables explicatives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
⚠️ Attention : Influence de l’échelle des variables explicatives
</div>
</div>
<div class="callout-body-container callout-body">
<p>Lorsque les variables explicatives ont des échelles très différentes, cela peut provoquer un comportement non optimal de la descente de gradient.</p>
<p>Par exemple : - Une variable <span class="math inline">\(x_1\)</span> variant entre 0 et 1 - Une autre variable <span class="math inline">\(x_2\)</span> variant entre 10 et 50</p>
<p>Un petit poids <span class="math inline">\(w_2\)</span> associé à <span class="math inline">\(x_2\)</span> peut entraîner une variation importante de la prédiction, tandis qu’un poids <span class="math inline">\(w_1\)</span> beaucoup plus grand associé à <span class="math inline">\(x_1\)</span> pourrait n’avoir qu’un effet marginal.</p>
<p><strong>Conséquences :</strong> - La surface de la fonction de coût est très étirée dans certaines directions. - La descente de gradient progresse très lentement, zigzague ou peut ne jamais converger. - Le nombre d’itérations nécessaires augmente fortement.</p>
<p>👉 C’est pourquoi <strong>la normalisation des variables</strong> (standardisation ou min-max scaling) est une étape cruciale avant d’entraîner un modèle linéaire avec descente de gradient.</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
⚠️ Important : <code>fit_transform()</code> vs <code>transform()</code>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Toujours utiliser <code>fit_transform()</code> <strong>uniquement sur les données d’entraînement</strong>, et <code>transform()</code> sur les données de validation ou de test.</p>
<p>Cela s’applique à <strong>tous les types de preprocessing</strong>, notamment :</p>
<ul>
<li>🟦 <strong>StandardScaler</strong> : la moyenne et l’écart-type doivent être appris sur le <em>train</em> uniquement.</li>
<li>🟧 <strong>OneHotEncoder</strong> : les catégories doivent être identifiées à partir du <em>train</em> et appliquées de manière cohérente au <em>test</em>.</li>
</ul>
<p>❌ Ne jamais faire <code>fit_transform()</code> sur le test, car cela introduit du <strong>data leakage</strong> (les données de test influencent le modèle).</p>
</div>
</div>
</section>
<section id="perspectives" class="level3">
<h3 class="anchored" data-anchor-id="perspectives">Perspectives</h3>
<ul>
<li><p><strong>Amélioration du pas (<span class="math inline">\(\gamma\)</span>)</strong> : l’usage d’un pas adaptatif ou de schémas comme l’apprentissage décroissant peut améliorer la rapidité et la robustesse de la convergence.</p></li>
<li><p><strong>Extensions avancées</strong> : l’ajout de régularisation (comme Ridge ou Lasso) ou le passage à des variantes stochastiques (SGD) ou mini-batch permet de généraliser la méthode à des ensembles plus volumineux et à la modélisation en profondeur.</p></li>
<li><p><strong>Combinaison avec d’autres algorithmes</strong> : pour pallier les inefficacités, on peut intégrer des techniques comme le momentum, AdaGrad, RMSprop ou Adam, qui corrélent le gradient pour accélérer la convergence et stabiliser l’apprentissage.</p></li>
</ul>
</section>
<section id="finalement" class="level3">
<h3 class="anchored" data-anchor-id="finalement">Finalement</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La descente de gradient traduit élégamment les principes fondamentaux de l’<strong>optimisation itérative</strong> : à chaque étape, un petit ajustement calculé éloigne le modèle de l’erreur, jusqu’à atteindre la valeur optimale des paramètres (). Même si elle n’est pas sans défauts, cette méthode demeure une <strong>brique essentielle</strong> en apprentissage automatique, particulièrement en régression linéaire, et sert de base à des méthodes plus sophistiquées.</p>
<hr>
</section>
</section>
<section id="remerciements-et-retour-dexpérience" class="level2">
<h2 class="anchored" data-anchor-id="remerciements-et-retour-dexpérience">Remerciements et retour d’expérience</h2>
<p>J’ai été ravi d’avoir consacré près de <strong>20 heures</strong> à la préparation et à la rédaction de cette publication.</p>
<p>Cet effort m’a permis de :</p>
<ul>
<li>consolider mes acquis en <strong>Python</strong>,<br>
</li>
<li>approfondir mes connaissances en <strong>Machine Learning</strong>, tant sur le plan <strong>théorique que pratique</strong>,<br>
</li>
<li>renforcer ma <strong>rigueur méthodologique</strong> dans le traitement des données, l’expérimentation et l’analyse des résultats.</li>
</ul>
<p>Ce projet m’a également offert une excellente opportunité de structurer une démarche complète de modélisation, depuis la génération des données jusqu’à l’interprétation finale des performances du modèle.</p>
<p>Je suis enthousiaste à l’idée de poursuivre cette exploration dans de futures publications de ce genre (travailler à la mano), notamment sur des cas réels avec des modèles plus avancés comme la <strong>régression logistique</strong> ou des approches <strong>régularisées</strong>.</p>
<p>Mon cours d’optimisation et de méthode de calcul numériques m’a été d’une grande utilité en 1A à l’ENSAI.</p>
<hr>
</section>
<section id="annexes" class="level2">
<h2 class="anchored" data-anchor-id="annexes">Annexes</h2>
<section id="standardisation-des-variables" class="level3">
<h3 class="anchored" data-anchor-id="standardisation-des-variables">1. Standardisation des variables</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La <strong>standardisation</strong> permet de centrer et réduire les variables numériques pour qu’elles aient une moyenne nulle et un écart-type unitaire.</p>
<p>Pour une variable continue <span class="math inline">\(x\)</span>, la standardisation est donnée par :</p>
<p><span class="math display">\[
x_{\text{std}} = \frac{x - \mu_x}{\sigma_x}
\]</span></p>
<p>où :</p>
<ul>
<li><span class="math inline">\(\mu_x = \dfrac{1}{n} \sum_{i=1}^n x_i\)</span> est la moyenne,</li>
<li><span class="math inline">\(\sigma_x = \sqrt{\dfrac{1}{n} \sum_{i=1}^n (x_i - \mu_x)^2}\)</span> est l’écart-type.</li>
</ul>
<p>Après avoir entraîné le modèle sur les données standardisées, on peut revenir à l’échelle réelle avec :</p>
<p><span class="math display">\[
\hat{y} = \hat{y}_{\text{std}} \cdot \sigma_y + \mu_y
\]</span></p>
<p>où <span class="math inline">\(\mu_y\)</span> et <span class="math inline">\(\sigma_y\)</span> sont la moyenne et l’écart-type de la variable cible <span class="math inline">\(y\)</span>.</p>
<hr>
</section>
<section id="codage-one-hot" class="level3">
<h3 class="anchored" data-anchor-id="codage-one-hot">2. Codage One-Hot</h3>
<p>Le <strong>One-Hot Encoding</strong> transforme une variable catégorielle à <span class="math inline">\(k\)</span> modalités en <span class="math inline">\(k\)</span> colonnes binaires.</p>
<p>Soit une variable catégorielle :</p>
<p><span class="math display">\[
\text{cat} \in \{c_1, c_2, \ldots, c_k\}
\]</span></p>
<p>On crée un vecteur :</p>
<p><span class="math display">\[
\mathbf{v} = (v_1, v_2, \ldots, v_k) \quad \text{où} \quad
v_j =
\begin{cases}
1 &amp; \text{si } \text{cat} = c_j \\
0 &amp; \text{sinon}
\end{cases}
\]</span></p>
<p>Afin d’éviter la redondance, on supprime une modalité (ex. via <code>drop='first'</code>) pour éviter le piège des variables muettes (dummy variable trap).</p>
<hr>
</section>
<section id="calcul-de-la-rmse" class="level3">
<h3 class="anchored" data-anchor-id="calcul-de-la-rmse">3. Calcul de la RMSE</h3>
<p>La <strong>Root Mean Square Error</strong> (RMSE) mesure l’écart quadratique moyen entre les valeurs prédites <span class="math inline">\(\hat{y}_i\)</span> et observées <span class="math inline">\(y_i\)</span> :</p>
<p><span class="math display">\[
\text{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 }
\]</span></p>
<p>En notation vectorielle, si <span class="math inline">\(\mathbf{y}\)</span> est le vecteur des valeurs observées et <span class="math inline">\(\hat{\mathbf{y}}\)</span> celui des valeurs prédites :</p>
<p><span class="math display">\[
\text{RMSE} = \sqrt{ \frac{1}{n} (\mathbf{y} - \hat{\mathbf{y}})^T (\mathbf{y} - \hat{\mathbf{y}}) }
\]</span></p>
<p>Cette métrique est exprimée dans l’unité de la variable cible (ici, les euros), ce qui la rend facile à interpréter dans un contexte réel.</p>
<hr>
<p>Ces opérations sont fondamentales dans toute pipeline de traitement pour la régression ou tout autre algorithme supervisé.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>