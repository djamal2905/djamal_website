[
  {
    "objectID": "about/expe.html",
    "href": "about/expe.html",
    "title": "Expériences",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\nAnalyse statistique avancée\n\n\nModèles de régression linéaire, logistique, et mixte (GLMM) ; Analyses exploratoires (ACP, AFC, ACM); Tests d’hypothèses \n\n\n\n\nProgrammation et automatisation avec R\n\n\nNettoyage et manipulation de données avec dplyr ; Visualisation avec ggplot2, sf (données spatiales) ; Création de rapports automatisés et tableaux de bord interactifs avec Shiny; \n\n\n\n\nProgrammation et visualisation avec Python\n\n\nManipulation et analyse de données avec pandas, geopandas (données spatiales); Visualisation avancée avec matplotlib, seaborn; \n\n\n\n\nCollecte et gestion des données\n\n\nConception et déploiement de formulaires numériques (KoboCollect); Gestion de bases de données; Préparation des données pour analyse; \n\n\n\n\nCommunication scientifique\n\n\nRédaction de rapports et articles scientifiques; Présentation et restitution des résultats; Création de supports visuels clairs; \n\n\n\n\nModélisation et Machine Learning\n\n\nPréparation des données; Modélisation supervisée (régression, validation croisée); Utilisation de scikit-learn; Interprétation des résultats \n\n\n\n\nAnalyse économique et économétrie\n\n\nRevue critique et synthèse bibliographique; Modélisation économétrique appliquée aux données sociales et économiques \n\n\n\n\nGestion de projets et travail collaboratif\n\n\nCoordination sur terrain; Participation à la logistique d’enquêtes; Organisation d’événements scientifiques; Github \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Statistiques, Data Science & Projets Numériques - About Djamal TOE",
    "section": "",
    "text": "Bienvenue sur mon site personnel.\nJe suis Djamal Y. TOE, statisticien passionné par les analyses avancées, la visualisation de données, et la résolution de problèmes complexes à travers des approches quantitatives. Ce site présente mes projets, mes recherches, et mes contributions dans le domaine des statistiques et de la science des données. Je suis titulaire d’un bachelor en statistiques-informatique et actuellement élève ingénieur en Data Science à l’Ecole Nationale de la statistique et de l’Analyse de l’Information à Bruz Rennes, France."
  },
  {
    "objectID": "about.html#à-propos-de-moi",
    "href": "about.html#à-propos-de-moi",
    "title": "Statistiques, Data Science & Projets Numériques - About Djamal TOE",
    "section": "À propos de moi",
    "text": "À propos de moi\nJe combine mes compétences en statistiques, programmation et analyse de données pour transformer des ensembles de données en informations exploitables. Mon objectif est d’améliorer la prise de décision grâce à des modèles et des méthodes robustes. Ayant effectuer des stages en entreprises, j’ai appris beaucoup de choses notamment en bio-statistiques et sur les modélisations qui y sont utilisées. J’ai également des connaissance en cartographie avec R et python."
  },
  {
    "objectID": "about.html#expérience",
    "href": "about.html#expérience",
    "title": "Statistiques, Data Science & Projets Numériques - About Djamal TOE",
    "section": "Expérience",
    "text": "Expérience\n\nCentre MURAZ\nStagiaire en Biostatistique (sur site)\nfévr. 2024 – avr. 2024 (3 mois) — Bobo-Dioulasso, Burkina Faso\n\nÉvaluation statistique d’interventions de santé publique contre le paludisme via analyses et modèles de régression.\n\nAutomatisation des processus de nettoyage, analyse et reporting avec R.\n\nCréation de tableaux de bord interactifs avec RShiny pour l’exploration des données épidémiologiques.\n\nRenforcement de compétences en communication scientifique et application pratique des méthodes statistiques.\n\n\n\nCentre MURAZ\nStagiaire en Biostatistique (sur site)\naoût 2023 – janv. 2024 (6 mois) — Bobo-Dioulasso, Burkina Faso\n\nAnalyses statistiques classiques et exploratoires (tests d’hypothèses, ACP, ACM, AFC).\n\nConception de formulaires numériques pour la collecte de données (KoboCollect).\n\nParticipation à la cartographie et à la coordination opérationnelle pour enquêtes terrain.\n\nContribution à des événements scientifiques.\n\nPremière expérience pratique en statistiques appliquées à la santé publique."
  },
  {
    "objectID": "about.html#bénévolat",
    "href": "about.html#bénévolat",
    "title": "Statistiques, Data Science & Projets Numériques - About Djamal TOE",
    "section": "Bénévolat",
    "text": "Bénévolat\n\nMembre du Comité Logistique\nBurkina Faso Health Sciences Association (ASSB) — mai 2024\nOrganisation, gestion technique et coordination lors des Journées des Sciences de la Santé de Bobo-Dioulasso.\nDéveloppeur\nJournée Internationale de la Sécurité Sanitaire des Aliments (JISSA 2024) — mars-avr. 2024\nConception d’un formulaire en ligne et d’une application pour la collecte et le traitement de drafts scientifiques.\nAttestation de reconnaissance pour cette contribution."
  },
  {
    "objectID": "about.html#projets-académiques-clés",
    "href": "about.html#projets-académiques-clés",
    "title": "Statistiques, Data Science & Projets Numériques - About Djamal TOE",
    "section": "Projets académiques clés",
    "text": "Projets académiques clés\n\nPrédiction de la durée de carrière des joueurs NBA\nMars – Mai 2025 — ENSAI\nProjet ML complet : préparation des données, modélisation supervisée avec régression linéaire (scikit-learn), validation croisée, régularisation, tests unitaires, et interface Shiny interactive. Résultats : RMSE ~4 ans, analyse des variables clés.\n\n\nLien entre jeux (vidéo, pari) et santé des adolescents\nDéc. 2024 – Mai 2025 — ENSAI\nÉtude statistique sur données ESCAPAD 2022, tests d’association, ACM, avec mise en évidence de profils distincts selon le genre et des corrélations avec la santé mentale.\n\n\nImpact des aides au logement sur loyers et demande en France\nNov. 2024 – Mars 2025 — ENSAI\nRevue critique et rapport sur les effets des aides au logement via une analyse économétrique et synthèse bibliographique.\n\n\nProjet d’enquête pratique — Licence Statistiques & Informatique\nMai – Juin 2023 — Université Nazi Boni\nConception et déploiement d’une enquête complète, du protocole à la collecte mobile (KoboCollect) et préparation des données.\n\n\nPrévision de l’Indice Harmonisé des Prix à la Consommation (IHPC)\nNov. – Déc. 2021 — Université Nazi Boni\nModélisation de séries temporelles : lissages exponentiels, Holt-Winters, régression, ARIMA/SARIMA, avec comparaison des performances."
  },
  {
    "objectID": "about.html#compétences",
    "href": "about.html#compétences",
    "title": "Statistiques, Data Science & Projets Numériques - About Djamal TOE",
    "section": "Compétences",
    "text": "Compétences"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "",
    "text": "Dans un monde où les indicateurs de développement sont multiples (revenu, santé, éducation, accès aux services, environnement), il devient crucial de synthétiser l’information pour comprendre les grands profils qui distinguent les pays. Pour ce faire, nous mobilisons deux techniques statistiques puissantes et complémentaires :\n\nL’analyse en composantes principales (ACP), qui permet de réduire la dimension des données en identifiant les axes principaux de variation entre les pays,\nLe clustering par K-means, qui regroupe les pays selon leurs profils de développement similaires dans l’espace défini par l’ACP.\n\nCette approche nous permettra : - D’identifier visuellement les dimensions clés du développement, - De regrouper les pays en classes homogènes, facilitant ainsi l’analyse comparative.\nNous appliquerons cette démarche sur un ensemble de variables décrivant les niveaux de vie, l’éducation, la santé, l’environnement et l’accès aux services, dans le but de dresser une cartographie synthétique et interprétable des grandes catégories de pays à travers le monde.\n\n\n\n\n\nLe jeu de données utilisé dans cette analyse provient de Kaggle et regroupe plusieurs indicateurs socio-économiques et de santé pour 167 pays.\n\n\n\nL’ACP est une méthode statistique de réduction de dimensionnalité. Elle transforme un grand nombre de variables corrélées en un nombre plus petit de variables non corrélées appelées composantes principales. Ces composantes capturent l’essentiel de la variation présente dans les données originales.\nPourquoi utiliser l’ACP ?\n- Simplifier l’analyse en réduisant la complexité des données multidimensionnelles,\n- Visualiser facilement les relations entre observations et variables,\n- Mettre en évidence les structures sous-jacentes dans les données.\n\n\n\n      K-means est une méthode de classification non supervisée qui consiste à regrouper un ensemble d’observations en K clusters (groupes), où chaque observation appartient au cluster dont elle est la plus proche selon une mesure de distance (souvent euclidienne).\nPourquoi utiliser K-means ?\n\nIdentifier des groupes homogènes dans les données,\n\nFaciliter l’interprétation en catégorisant les observations,\n\nDétecter des profils ou comportements similaires.\n\n\n\n\nL’ACP et le K-means sont souvent utilisés conjointement car ils se complètent parfaitement :\n- ACP prépare les données en réduisant leur dimension, en supprimant le bruit et les redondances, ce qui facilite la visualisation et la compréhension,\n- K-means exploite l’espace réduit par l’ACP pour effectuer un regroupement plus robuste et plus interprétable, évitant les problèmes liés à la malédiction de la dimension.\nAinsi, l’association ACP + K-means permet d’analyser efficacement des données complexes, en identifiant à la fois les principales dimensions d’influence et les groupes d’observations partageant des caractéristiques communes.\n\n\n\n\n\n\n\nCode\nrm(list=ls())\n\n##--- package à installer\npackages &lt;- c(\n  \"dplyr\",\"cluster\",\n  \"ggplot2\",\"factoextra\",\n  \"FactoMineR\", \"pheatmap\",\n  \"ggrepel\", \"patchwork\"\n)\n\n##-- Boucle pour installer et charger les packages\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}\n\n\n\n\n\n\n\nCode\ndf &lt;- read.csv('data_country.csv')\n# vérification\nglimpse(df)\n\n\nRows: 167\nColumns: 10\n$ Country    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Antigua and…\n$ child_mort &lt;dbl&gt; 90.2, 16.6, 27.3, 119.0, 10.3, 14.5, 18.1, 4.8, 4.3, 39.2, …\n$ exports    &lt;dbl&gt; 10.0, 28.0, 38.4, 62.3, 45.5, 18.9, 20.8, 19.8, 51.3, 54.3,…\n$ health     &lt;dbl&gt; 7.58, 6.55, 4.17, 2.85, 6.03, 8.10, 4.40, 8.73, 11.00, 5.88…\n$ imports    &lt;dbl&gt; 44.9, 48.6, 31.4, 42.9, 58.9, 16.0, 45.3, 20.9, 47.8, 20.7,…\n$ income     &lt;int&gt; 1610, 9930, 12900, 5900, 19100, 18700, 6700, 41400, 43200, …\n$ inflation  &lt;dbl&gt; 9.440, 4.490, 16.100, 22.400, 1.440, 20.900, 7.770, 1.160, …\n$ life_expec &lt;dbl&gt; 56.2, 76.3, 76.5, 60.1, 76.8, 75.8, 73.3, 82.0, 80.5, 69.1,…\n$ total_fer  &lt;dbl&gt; 5.82, 1.65, 2.89, 6.16, 2.13, 2.37, 1.69, 1.93, 1.44, 1.92,…\n$ gdpp       &lt;int&gt; 553, 4090, 4460, 3530, 12200, 10300, 3220, 51900, 46900, 58…\n\n\n\n\n\nDescription des données\n\n\n\nLe jeu de données contient les indicateurs suivants avec leurs éventuelles significations pour 167 pays :\n\nchild_mort : taux de mortalité infantile (pour 1000 naissances vivantes)\nexports et imports : en % du PIB\nhealth : dépenses de santé en % du PIB\nincome : revenu moyen par personne\ninflation : taux d’inflation\nlife_expec : espérance de vie\ntotal_fer : taux de fécondité\ngdpp : PIB par habitant\n\nLes formats des variables semblent être adéquats.\nPour aller plus loin dans le descriptis des données, on peut taper la commande suivante :\n\n\nCode\nsummary(df)\n\n\n   Country            child_mort        exports            health      \n Length:167         Min.   :  2.60   Min.   :  0.109   Min.   : 1.810  \n Class :character   1st Qu.:  8.25   1st Qu.: 23.800   1st Qu.: 4.920  \n Mode  :character   Median : 19.30   Median : 35.000   Median : 6.320  \n                    Mean   : 38.27   Mean   : 41.109   Mean   : 6.816  \n                    3rd Qu.: 62.10   3rd Qu.: 51.350   3rd Qu.: 8.600  \n                    Max.   :208.00   Max.   :200.000   Max.   :17.900  \n    imports             income         inflation         life_expec   \n Min.   :  0.0659   Min.   :   609   Min.   : -4.210   Min.   :32.10  \n 1st Qu.: 30.2000   1st Qu.:  3355   1st Qu.:  1.810   1st Qu.:65.30  \n Median : 43.3000   Median :  9960   Median :  5.390   Median :73.10  \n Mean   : 46.8902   Mean   : 17145   Mean   :  7.782   Mean   :70.56  \n 3rd Qu.: 58.7500   3rd Qu.: 22800   3rd Qu.: 10.750   3rd Qu.:76.80  \n Max.   :174.0000   Max.   :125000   Max.   :104.000   Max.   :82.80  \n   total_fer          gdpp       \n Min.   :1.150   Min.   :   231  \n 1st Qu.:1.795   1st Qu.:  1330  \n Median :2.410   Median :  4660  \n Mean   :2.948   Mean   : 12964  \n 3rd Qu.:3.880   3rd Qu.: 14050  \n Max.   :7.490   Max.   :105000  \n\n\nOn voit qu’on des données qui sont propres ,ce qui est rarement le cas dans des situations réélles.\n\n\n\nVisualisation des informations sur la distribution des variables\n\n\n\n\n\nCode\nplot_box_plot &lt;- function(variable_name_str, title = \"\") {\n  # Vérification que la variable existe dans df\n  if(!variable_name_str %in% colnames(df)) {\n    stop(\"La variable spécifiée n'existe pas dans la table\")\n  }\n\n  \n  var_titles &lt;- list(\n    \"child_mort\" = \"Taux de mortalité infantile\",\n    \"exports\"    = \"Exportations (% du PIB)\",\n    \"health\"     = \"Dépenses de santé (% du PIB)\",\n    \"imports\"    = \"Importations (% du PIB)\",\n    \"income\"     = \"Revenu par habitant (en USD)\",\n    \"inflation\"  = \"Taux d'inflation (%)\",\n    \"life_expec\" = \"Espérance de vie (en années)\",\n    \"total_fer\"  = \"Taux de fécondité (enfants par femme)\",\n    \"gdpp\"       = \"PIB par habitant (en USD)\"\n  )\n  \n  title &lt;- var_titles[[variable_name_str]]\n  # titre par défaut\n  if (title == \"\") {\n    title &lt;- paste(\"Distribution de la variable\", variable_name_str)\n  }\n\n  # Création du graphique\n  plt &lt;- ggplot(df, aes(y = .data[[variable_name_str]])) +\n    geom_boxplot(fill = \"skyblue\", color = \"darkblue\") +\n    theme_light() +\n    labs(title = title, y = variable_name_str)+\n  theme(plot.title = element_text(size = 9)) \n\n  return(plt)\n}\n\nplots &lt;- lapply(colnames(df[,-1]), plot_box_plot)\nwrap_plots(plots, ncol = 3)\n\n\n\n\n\nDistribution des variables du jeu de données\n\n\n\n\n\n\n\n\n      On calcule les corrélation de Pearson quand on suspecte des relations linéaires entre les variables, quand celles -ci sont sous forme d’échelle ou de ratio. Ici en plus de ploter le heatmap des variables, nous afficherons celles qui sont significativement corrélées (Test de corrélation de Pearson cf. Annexe 1)\n\n\nCode\n# calcul de la matrice de corrélations de pearson\ncor_matrix &lt;- cor(df[, -1], method = \"pearson\")\n\n# fonction pour extraire les p-values\nget_pval &lt;- function(x, y) {\n  res &lt;- suppressWarnings(cor.test(x, y, method = \"pearson\"))\n  return(res$p.value)\n}\n\n# matrice des p-values\nn &lt;- ncol(df[, -1])\npval_matrix &lt;- matrix(NA, nrow = n, ncol = n)\nrownames(pval_matrix) &lt;- colnames(df[, -1])\ncolnames(pval_matrix) &lt;- colnames(df[, -1])\nfor (i in 1:n) {\n  for (j in 1:n) {\n    pval_matrix[i, j] &lt;- get_pval(df[, -1][[i]], df[, -1][[j]])\n  }\n}\n\nget_signif_stars &lt;- function(p) {\n  if (p &lt; 0.01) {\n    return(\"***\")\n  } else if (p &lt; 0.05) {\n    return(\"**\")\n  } else if (p &lt; 0.1) {\n    return(\"*\")\n  } else {\n    return(\"\")\n  }\n}\n\n# Création de la matrice des annotations\nnumber_labels &lt;- matrix(\"\", nrow = n, ncol = n)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    rho &lt;- cor_matrix[i, j]\n    p &lt;- pval_matrix[i, j]\n    stars &lt;- get_signif_stars(p)\n    number_labels[i, j] &lt;- paste0(sprintf(\"%.2f\", rho), stars)\n  }\n}\nrownames(number_labels) &lt;- rownames(cor_matrix)\ncolnames(number_labels) &lt;- colnames(cor_matrix)\n\n\n# heatmat sans clustering (car par defaut la fonction essaie de faire une CAH)\nplt &lt;- pheatmap(\n  cor_matrix,\n  cluster_rows = FALSE,\n  cluster_cols = FALSE,\n  color = colorRampPalette(c(\"blue\", \"white\", \"red\"))(50),\n  display_numbers = number_labels,\n  number_format = \"\",\n  fontsize_number = 8,\n  main = \"Heatmap de la corrélation de Person\",\n  angle_col = 90 \n)\nplt\n\n\n\n\n\nHeatmap des varibales quantitative de la base de données\n\n\n\n\nInterpretation :\nComme on peut le constater, plusieurs variables présentent des corrélations significatives entre elles. Cela est mis en évidence par les étoiles indiquant le niveau de signification statistique : *** pour une signification au seuil de 1 %, ** pour 5 %, et * pour 10 %. La coloration des cellules — plus elles sont proches du rouge ou du bleu foncé, plus la corrélation est forte (positive ou négative) — renforce cette lecture. Par exemple le taux de mortalité infantile est négativement corrélé à l’expérance de vie (-0,89 ***). Un taux de mortalité infantile élevé signifie que de nombreux décès surviennent très tôt dans la vie. Comme l’espérance de vie est une moyenne pondérée des âges au décès, la présence de nombreux décès précoces fait chuter la moyenne, d’où la corrélation négative. Par contre le taux de mortalité infantile est positivement corrélé à le taux de fécondité (0,85 ***). Cela peut s’expliquer par le fait que, dans les pays où le taux de fécondité est élevé, le nombre total de naissances est plus important. Dès lors, si les conditions sanitaires restent précaires, cela augmente mécaniquement le nombre d’enfants susceptibles de décéder en bas âge. Ce phénomène s’apparente à un processus binomial, où chaque naissance représente une “épreuve” avec une certaine probabilité de décès. Plus il y a d’épreuves, plus la fréquence des décès peut être élevée, ce qui se traduit par un taux de mortalité infantile plus important.\nCes interdépendances marquées entre les variables justifient le recours à une analyse en composantes principales (ACP), qui permettra de résumer l’information contenue dans ces variables corrélées tout en réduisant la dimensionnalité du jeu de données.\n\n\n\n\n      Plusieurs packages sur R permettre de mettre en oeuvre l’analyse en composantes principales. Nous utiliserons les packages FactoMineR et factoextra. Très souvent, on standardise les données avant de réaliser une analyse en composantes principales. Cette étape permet de ramener toutes les variables à une échelle comparable, en neutralisant les différences d’unités ou d’amplitudes. Ainsi, chaque variable contribue équitablement à la construction des composantes principales, sans que celles ayant une grande variance ne dominent l’analyse.\n\ndf_acp &lt;- df[, -1]\nrownames(df_acp) &lt;- df[,1]\nacp_model &lt;- PCA(df_acp, scale.unit = TRUE, graph = FALSE)\n\n\nLes infos du modèle\n\n\nnames(acp_model)\n\n[1] \"eig\"  \"var\"  \"ind\"  \"svd\"  \"call\"\n\n\n\neig : Valeurs propres associées aux composantes principales. Elles indiquent la variance expliquée par chaque axe.\nvar : Informations sur les variables actives (coordonnées, contributions, qualités de représentation (cos2)).\nind : Informations sur les individus (lignes) : coordonnées dans l’espace factoriel, contributions, cos2.\nsvd : Résultats de la décomposition en valeurs singulières (utile si vous voulez aller dans le détail mathématique).\ncall : L’appel de la fonction (PCA(...)), utile pour garder la trace de tes paramètres d’appel.\n\nMais nous allons nous concentrés que sur eig, var et ind.\n\n\n\nValeurs proppres : Choix des dimensions d’analyse\n\n\n\n\n\nCode\nfviz_eig(acp_model, geom = 'line') +\n  labs(title = \"Pourcentages des variances expliquées par les composantes principales\",\n       y = \"Pourcentage d'inertie\", x = \"Composantes principales\")\n\n\n\n\n\nDiagramme des variances expliquées par les composantes principales\n\n\n\n\n      Le screeplot (ou graphique des éboulis) met en évidence une chute marquée des valeurs propres entre la première et la deuxième dimension. Un léger coude apparaît ensuite entre la deuxième et la troisième composante. Au-delà, la décroissance devient plus faible et progressive, avec un second coude observable autour de la sixième dimension.\nPour déterminer le nombre optimal d’axes à retenir, nous nous appuierons sur deux critères complémentaires :\n\nLe critère de Kaiser, qui recommande de ne retenir que les composantes dont la valeur propre est supérieure à 1.\nLe critère du taux d’inertie, qui suggère de conserver le nombre de dimensions nécessaires pour expliquer un seuil acceptable de la variance totale, souvent fixé à 70 % ou 80 % selon le contexte.\n\n\n\nCode\nfviz_eig(acp_model, choice = \"eigenvalue\" , geom = 'bar')+\n  geom_hline(yintercept = 1, linetype = 2, color = \"red\") +\n  labs(title = \"Valeurs propres des composantes principales\",\n       y = \"Valeur propre\", x = \"Composantes principales\")\n\n\n\n\n\nDiagramme des valeurs propres issues de l’ACP\n\n\n\n\nEn se basant donc sur ces deux critère nous pouvons sélectionner les trois premières dimensions.\n\n\n\nAnalyses des variables\n\n\n\n\n\nCode\nfviz_pca_var(acp_model, col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corrélation des variables\",\n       y = \"Dimension 2\", x = \"Dimension 1\") + \n  theme_light()\n\nfviz_pca_var(acp_model, axes = c(1, 3), col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corrélation des variables\",\n       y = \"Dimension 3\", x = \"Dimension 1\") + \n  theme_light()\n\n\nfviz_pca_var(acp_model, axes = c(2, 3), col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corrélation des variables\",\n       y = \"Dimension 3\", x = \"Dimension 2\") + \n  theme_light()\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 1 et 2\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 1 et 3\n\n\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 2 et 3\n\n\n\n\nCartes de la representation des variables sur les dimensions\n\n\n\n\n\nGraphique a)\n\non observe que les variables health et inflation sont peu bien représentées sur le plan factoriel (valeurs de cos² faibles). À l’inverse, la variable exports est celle qui est la mieux projetée sur ce plan.\n\nLes variables imports et exports sont davantage alignées avec la dimension 2, ce qui suggère que cet axe reflète essentiellement les activités économiques extérieures (importations et exportations). Nous pourrions ainsi interpréter l’Axe 2 comme celui de l’ouverture commerciale.\nConcernant l’Axe 1, il oppose le taux de fécondité et le taux de mortalité infantile (positivement corrélés entre eux) à l’espérance de vie, la croissance du PIB par tête, et aux revenus moyens par habitant. On peut donc interpréter cet axe comme celui du niveau de développement socio-économique.\nEn résumé :\n\nAxe 1 : Niveau de développement (fécondité + mortalité infantile ↔︎ espérance de vie, revenus, PIB)\nAxe 2 : Activités d’importation et d’exportation\n\nGraphique b)\n\nSur ce plan (Axe 1-3), les importations ne sont pas bien représentées, ce qui signifie qu’elles ont une faible contribution à la projection dans cet espace factoriel.\n\nLa dimension 1, comme précédemment, semble caractériser un niveau de développement socio-économique, opposant les pays à forte mortalité infantile et fécondité élevée à ceux présentant une espérance de vie plus longue, un revenu moyen plus élevé, les exportations et un PIB par tête plus important.\nQuant à l’Axe 3, il oppose principalement l’inflation aux dépenses de santé. Ces deux variables apparaissent en opposition sur cet axe, suggérant que dans les pays où l’inflation est forte, la part des dépenses de santé dans le PIB tend à être plus faible, et inversement.\nEn résumé :\n\nAxe 1 : Niveau de développement socio-économique\nAxe 3 : Opposition entre taux d’inflation et dépenses de santé\n\nGraphique b)\n\n      Toutes les variables en bleues sont mal représentées. On voit encore une oppostion entre le taux d’inflation et les dépenses de santé sur l’axe 3. Quant à l’axe 2, il décrit les activités d’importation et d’exportation.\n\n\n\n\n\n\nInterpretation des variables\n\n\n\nOn pouvait choisir d’afficher le cercle de corrélation des variables avec leur contribution respective en lieu et place de leur qualité de représentation. Le plus souvent les variables qui sont bien représentées sont celles qui contribuent le plus à la formation des axes factorielles. On peut également combiner ces deux critères.\nOn interprête que les variables qui ont une bonne contribution (critère : souvent supérieure à la contribution moyenne sur les axes choisis) ou une bonne qualité de représentations (cos2 &gt; 0,6, mais souvent subjectif).\n\n\n\n\n\nAnalyses des individus\n\n\n\n\n\nCode\nfviz_pca_ind(acp_model, \n             col.ind = \"cos2\",                # coloration des individus selon qualité dereprésentation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             geom = c(\"point\", \"text_repel\")) +   # points + labels repel\n  labs(title = \"Projection des individus (Dim 1 et 2)\",\n       x = \"Dimension 1\", y = \"Dimension 2\") +\n  theme_light()\n\nfviz_pca_ind(acp_model, axes = c(1, 3),\n             col.ind = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             geom = c(\"point\", \"text_repel\")) +\n  labs(title = \"Projection des individus (Dim 1 et 3)\",\n       x = \"Dimension 1\", y = \"Dimension 3\") +\n  theme_light()\n\nfviz_pca_ind(acp_model, axes = c(2, 3),\n             col.ind = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             geom = c(\"point\", \"text_repel\")) +\n  labs(title = \"Projection des individus (Dim 2 et 3)\",\n       x = \"Dimension 2\", y = \"Dimension 3\") +\n  theme_light()\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 1 et 2\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 1 et 3\n\n\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 2 et 3\n\n\n\n\nCartes de la representation des variables sur les dimension\n\n\n\n\nLe principe d’analyse ne change pas. Les individus en bleu ciel sont mal représentés. On voit des points atypiques qui se démarquent. Ceux-ci contribuent fortement à la formation des axes aux quels ils sont proches.\n\n\nCode\nfviz_pca_ind(acp_model, \n             col.ind = \"contrib\",                # coloration des individus selon qualité dereprésentation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +   # points + labels repel\n  labs(title = \"Projection des individus (Dim 1 et 2)\",\n       x = \"Dimension 1\", y = \"Dimension 2\") +\n  theme_light()\n\n\n\n\n\nAffichages des individus atypiques dimensions 1 et 2\n\n\n\n\nIls s’agit de Singapore, du luxembourg et éventuellemet de Malta. Qu’on les retire ou pas cela n’aurait pas changé quelle que chose, car les variables qu’on a ont probablement toutes des dénominateurs communs en fonction de leurs définitions (la taille de la population pour les variables par hahbitants etc.).\n\ncontrib_ind_dim_1_2 &lt;- as.data.frame(acp_model$ind$contrib[, 1:2]) %&gt;% \n  filter(rownames(as.data.frame(acp_model$ind$contrib[, 1:2])) %in% c(\"Singapore\", \"Luxembourg\", \"Malta\"))\nknitr::kable(contrib_ind_dim_1_2, format = \"html\")\n\n\n\n\n\nDim.1\nDim.2\n\n\n\n\nLuxembourg\n6.928982\n9.108194\n\n\nMalta\n1.960319\n8.794095\n\n\nSingapore\n4.842860\n17.290257\n\n\n\n\n\n\n\nPour les autres plans, le processus est pareil.\n\n\n\n\n      Ici on applique directement les kmeans aux coordonnées factorielles issues de l’ACP. Et on avait décider de travailler que sur les trois premières dimensions. Ceci est une illustration de la réduction de dimentionalités avant de passer à l’application d’une méthode qui servira à résoudre un problème de classifications.\nEtant donné qu’on veut choisir le nombre de cluster qui non seulement minimise l’inertie intra-classe (donc maximise l’inertie inter-classe), mais à partir du quel son incrémentation ne change presque plus ou de peu l’inertie intra-classe. En d’autres termes ou on n’a plus de chute brutale. Chute car plus le nombre clusters augmente, plus l’inertie-intra classe diminue.\n\n\nCode\nset.seed(123) # fixe la graine générative\n\ndataTocluster &lt;- scale(acp_model$ind$coord[,1:3]) # selection des coordonnées factorielles sur les 3 premiers axes\n\nresKmeans &lt;- list() # pour stocker les résultats de l'algorithme\nCPtheta &lt;- rep(0, 8) # pour stocker les inerties intra-classes (8 classes choisies de manière subjective)\nfor (K in 1:8){\n  resKmeans[[K]] &lt;- kmeans(dataTocluster, K, nstart = 50)\n  CPtheta[K] &lt;- resKmeans[[K]]$tot.withinss\n}\n\n\n\n\nCode\ndf_ev_inertie &lt;- data.frame(\n  InertieIntraclasse = CPtheta,\n  NombreClusters = 1:8\n)\n\nggplot(df_ev_inertie, aes(x = NombreClusters, y = InertieIntraclasse)) +\n  geom_line() +\n  geom_point(shape = 21, fill = \"white\", color = \"black\", size = 3) +\n  labs(\n    x = \"Nombre de clusters\",\n    y = \"Inertie intra-classe\"\n  ) + theme_light()\n\n\n\n\n\nEvolution de l’inertie intra classe\n\n\n\n\nD’après le critère du coude, \\(K=4\\) (éventuellement \\(k=5\\)) est un choix pertinent. Affichons ainsi un descriptif des résultats du k-means.\n\n\nCode\nclassif_4 &lt;- resKmeans[[4]]\nstr(classif_4)\n\n\nList of 9\n $ cluster     : Named int [1:167] 2 4 3 3 4 3 4 4 4 3 ...\n  ..- attr(*, \"names\")= chr [1:167] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n $ centers     : num [1:4, 1:3] 2.204 -0.907 -0.236 0.66 3.856 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:3] \"Dim.1\" \"Dim.2\" \"Dim.3\"\n $ totss       : num 498\n $ withinss    : num [1:4] 10.7 54.5 91.6 74.6\n $ tot.withinss: num 231\n $ betweenss   : num 267\n $ size        : int [1:4] 4 52 39 72\n $ iter        : int 3\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\nCes chiffres signifient que :\n\nLa variance totale des données est de 498.\nLa variance intra-classe totale est de 231, ce qui mesure la compacité des clusters.\nLa variance inter-classe est de 267, indiquant la séparation entre les groupes.\nEnviron 54 % de la variance totale est expliquée par la partition.\nLes tailles des clusters sont inégales, allant de 4 à 72 individus.\nL’algorithme a convergé rapidement en 3 itérations.\n\nLa partition en 4 clusters présenterait donc un bon équilibre entre homogénéité interne et séparation externe.\n\n\nCode\nclassif_5 &lt;- resKmeans[[5]]\nstr(classif_5)\n\n\nList of 9\n $ cluster     : Named int [1:167] 5 3 1 1 4 3 3 3 3 1 ...\n  ..- attr(*, \"names\")= chr [1:167] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n $ centers     : num [1:5, 1:3] -7.97e-05 2.68 7.00e-01 2.53e-01 -1.09 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:3] \"Dim.1\" \"Dim.2\" \"Dim.3\"\n $ totss       : num 498\n $ withinss    : num [1:5] 54.3 4.79 36.17 53.68 43.34\n $ tot.withinss: num 192\n $ betweenss   : num 306\n $ size        : int [1:5] 20 3 45 51 48\n $ iter        : int 4\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\nCes chiffres signifient que :\n\nLa variance totale des données est de 498.\nLa variance intra-classe totale est de 192, indiquant la compacité des clusters.\nLa variance inter-classe est de 306, mesurant la séparation entre les groupes.\nEnviron 61% de la variance totale est expliquée par la partition, ce qui est une amélioration par rapport à k=4.\nLes tailles des clusters varient de 3 à 51 individus, indiquant une répartition inégale.\nL’algorithme a convergé en 4 itérations.\n\n      La partition en 5 clusters améliore la séparation entre groupes et la compacité interne.\n\n\nCode\ntable(classif_5$cluster)\n\n\n\n 1  2  3  4  5 \n20  3 45 51 48 \n\n\nOn choisit \\(k=5\\) en combinant critère du coude et variance expliquée.\n\n\n\n\n\n\nChoix de k (nombre de clusters)\n\n\n\nLe choix du k est un peu subjctif mais on a plusieurs règles permettant de le choisir. Il s’agit entre autre du critère du taux d’inertie, critère du coude etc. On aurait donc bien pu prendre k=4.\n\n\n\n\n\nReprésentations graphiques\n\n\n\n\n\nCode\ndataTocluster &lt;- as.data.frame(dataTocluster)\ndataTocluster &lt;- dataTocluster %&gt;% \n  mutate (classe = as.factor(classif_5$cluster))\n\nggplot(dataTocluster, aes (x = Dim.1, y = Dim.2, color = classe)) +\n  geom_point() +\n  geom_text_repel(aes(label = rownames(dataTocluster)), size = 3) + \n  labs(\n    title = \"Classification des pays (Axe 1 - Axe 3)\",\n    x = \"Dimension 1\",\n    y = \"Dimension 3\",\n    color = \"Classe/Cluster\"\n  )+\n  theme_light()\n\n\n\n\n\nClassification des pays sur la base de leurs données socio-économiques\n\n\n\n\n      Certains points sont non labellisés pour éviter que les noms des pays ne se chevauchent, ce qui pourrait nuire à la lisibilité du graphique. Vous trouverez en annexe 4 les tables détaillant les différentes classes.\nSur le premier plan factoriel, on observe que Singapour, Malte et le Luxembourg appartiennent à la même classe. Cette proximité pourrait s’expliquer par plusieurs facteurs communs à ces pays (cf. annexe 3):\n\nEspérance de vie élevée : Ces pays affichent une espérance de vie parmi les plus hautes au monde, reflétant une qualité de vie et des systèmes de santé performants.\nFaible taux de mortalité infantile : Les taux de mortalité infantile y sont très bas, indiquant un accès généralisé aux soins prénatals et postnatals de qualité.\nRevenu par habitant élevé : Le PIB par habitant est significativement supérieur à la moyenne mondiale, traduisant une économie développée et stable.\nDépenses de santé élevées en pourcentage du PIB : Ces pays investissent une part importante de leur PIB dans la santé, ce qui se traduit par des infrastructures médicales avancées et un personnel soignant bien formé.\nFaible taux de fécondité : Le taux de fécondité y est inférieur au seuil de renouvellement des générations, ce qui est caractéristique des pays développés avec un niveau d’éducation élevé et une urbanisation importante.\n\nCes similitudes dans les indicateurs socio-économiques et sanitaires justifient leur regroupement sur le plan factoriel de l’ACP.\nVous pouvez analyser le graphique sur le plan 1-3.\n\n\nCode\ndataTocluster &lt;- as.data.frame(dataTocluster)\ndataTocluster &lt;- dataTocluster %&gt;% \n  mutate (classe = as.factor(classif_5$cluster))\n\nggplot(dataTocluster, aes(x = Dim.1, y = Dim.3, color = classe)) +\n  geom_point() +\n  geom_text_repel(aes(label = rownames(dataTocluster)), size = 3) +  # cex ≈ size = 3\n  labs(\n    title = \"Classification des pays (Axe 1 - Axe 3)\",\n    x = \"Dimension 1\",\n    y = \"Dimension 3\",\n    color = \"Classe/Cluster\"\n  ) +\n  theme_light()\n\n\n\n\n\nClassification des pays sur la base de leurs données socio-économiques"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#contexte",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#contexte",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "",
    "text": "Dans un monde où les indicateurs de développement sont multiples (revenu, santé, éducation, accès aux services, environnement), il devient crucial de synthétiser l’information pour comprendre les grands profils qui distinguent les pays. Pour ce faire, nous mobilisons deux techniques statistiques puissantes et complémentaires :\n\nL’analyse en composantes principales (ACP), qui permet de réduire la dimension des données en identifiant les axes principaux de variation entre les pays,\nLe clustering par K-means, qui regroupe les pays selon leurs profils de développement similaires dans l’espace défini par l’ACP.\n\nCette approche nous permettra : - D’identifier visuellement les dimensions clés du développement, - De regrouper les pays en classes homogènes, facilitant ainsi l’analyse comparative.\nNous appliquerons cette démarche sur un ensemble de variables décrivant les niveaux de vie, l’éducation, la santé, l’environnement et l’accès aux services, dans le but de dresser une cartographie synthétique et interprétable des grandes catégories de pays à travers le monde."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#présentation-de-lacp-et-du-k-means",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#présentation-de-lacp-et-du-k-means",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "",
    "text": "Le jeu de données utilisé dans cette analyse provient de Kaggle et regroupe plusieurs indicateurs socio-économiques et de santé pour 167 pays.\n\n\n\nL’ACP est une méthode statistique de réduction de dimensionnalité. Elle transforme un grand nombre de variables corrélées en un nombre plus petit de variables non corrélées appelées composantes principales. Ces composantes capturent l’essentiel de la variation présente dans les données originales.\nPourquoi utiliser l’ACP ?\n- Simplifier l’analyse en réduisant la complexité des données multidimensionnelles,\n- Visualiser facilement les relations entre observations et variables,\n- Mettre en évidence les structures sous-jacentes dans les données.\n\n\n\n      K-means est une méthode de classification non supervisée qui consiste à regrouper un ensemble d’observations en K clusters (groupes), où chaque observation appartient au cluster dont elle est la plus proche selon une mesure de distance (souvent euclidienne).\nPourquoi utiliser K-means ?\n\nIdentifier des groupes homogènes dans les données,\n\nFaciliter l’interprétation en catégorisant les observations,\n\nDétecter des profils ou comportements similaires.\n\n\n\n\nL’ACP et le K-means sont souvent utilisés conjointement car ils se complètent parfaitement :\n- ACP prépare les données en réduisant leur dimension, en supprimant le bruit et les redondances, ce qui facilite la visualisation et la compréhension,\n- K-means exploite l’espace réduit par l’ACP pour effectuer un regroupement plus robuste et plus interprétable, évitant les problèmes liés à la malédiction de la dimension.\nAinsi, l’association ACP + K-means permet d’analyser efficacement des données complexes, en identifiant à la fois les principales dimensions d’influence et les groupes d’observations partageant des caractéristiques communes."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#installation-des-pacakges",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#installation-des-pacakges",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "",
    "text": "Code\nrm(list=ls())\n\n##--- package à installer\npackages &lt;- c(\n  \"dplyr\",\"cluster\",\n  \"ggplot2\",\"factoextra\",\n  \"FactoMineR\", \"pheatmap\",\n  \"ggrepel\", \"patchwork\"\n)\n\n##-- Boucle pour installer et charger les packages\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#chargement-de-la-base-de-données-et-informations",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#chargement-de-la-base-de-données-et-informations",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "",
    "text": "Code\ndf &lt;- read.csv('data_country.csv')\n# vérification\nglimpse(df)\n\n\nRows: 167\nColumns: 10\n$ Country    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Antigua and…\n$ child_mort &lt;dbl&gt; 90.2, 16.6, 27.3, 119.0, 10.3, 14.5, 18.1, 4.8, 4.3, 39.2, …\n$ exports    &lt;dbl&gt; 10.0, 28.0, 38.4, 62.3, 45.5, 18.9, 20.8, 19.8, 51.3, 54.3,…\n$ health     &lt;dbl&gt; 7.58, 6.55, 4.17, 2.85, 6.03, 8.10, 4.40, 8.73, 11.00, 5.88…\n$ imports    &lt;dbl&gt; 44.9, 48.6, 31.4, 42.9, 58.9, 16.0, 45.3, 20.9, 47.8, 20.7,…\n$ income     &lt;int&gt; 1610, 9930, 12900, 5900, 19100, 18700, 6700, 41400, 43200, …\n$ inflation  &lt;dbl&gt; 9.440, 4.490, 16.100, 22.400, 1.440, 20.900, 7.770, 1.160, …\n$ life_expec &lt;dbl&gt; 56.2, 76.3, 76.5, 60.1, 76.8, 75.8, 73.3, 82.0, 80.5, 69.1,…\n$ total_fer  &lt;dbl&gt; 5.82, 1.65, 2.89, 6.16, 2.13, 2.37, 1.69, 1.93, 1.44, 1.92,…\n$ gdpp       &lt;int&gt; 553, 4090, 4460, 3530, 12200, 10300, 3220, 51900, 46900, 58…\n\n\n\n\n\nDescription des données\n\n\n\nLe jeu de données contient les indicateurs suivants avec leurs éventuelles significations pour 167 pays :\n\nchild_mort : taux de mortalité infantile (pour 1000 naissances vivantes)\nexports et imports : en % du PIB\nhealth : dépenses de santé en % du PIB\nincome : revenu moyen par personne\ninflation : taux d’inflation\nlife_expec : espérance de vie\ntotal_fer : taux de fécondité\ngdpp : PIB par habitant\n\nLes formats des variables semblent être adéquats.\nPour aller plus loin dans le descriptis des données, on peut taper la commande suivante :\n\n\nCode\nsummary(df)\n\n\n   Country            child_mort        exports            health      \n Length:167         Min.   :  2.60   Min.   :  0.109   Min.   : 1.810  \n Class :character   1st Qu.:  8.25   1st Qu.: 23.800   1st Qu.: 4.920  \n Mode  :character   Median : 19.30   Median : 35.000   Median : 6.320  \n                    Mean   : 38.27   Mean   : 41.109   Mean   : 6.816  \n                    3rd Qu.: 62.10   3rd Qu.: 51.350   3rd Qu.: 8.600  \n                    Max.   :208.00   Max.   :200.000   Max.   :17.900  \n    imports             income         inflation         life_expec   \n Min.   :  0.0659   Min.   :   609   Min.   : -4.210   Min.   :32.10  \n 1st Qu.: 30.2000   1st Qu.:  3355   1st Qu.:  1.810   1st Qu.:65.30  \n Median : 43.3000   Median :  9960   Median :  5.390   Median :73.10  \n Mean   : 46.8902   Mean   : 17145   Mean   :  7.782   Mean   :70.56  \n 3rd Qu.: 58.7500   3rd Qu.: 22800   3rd Qu.: 10.750   3rd Qu.:76.80  \n Max.   :174.0000   Max.   :125000   Max.   :104.000   Max.   :82.80  \n   total_fer          gdpp       \n Min.   :1.150   Min.   :   231  \n 1st Qu.:1.795   1st Qu.:  1330  \n Median :2.410   Median :  4660  \n Mean   :2.948   Mean   : 12964  \n 3rd Qu.:3.880   3rd Qu.: 14050  \n Max.   :7.490   Max.   :105000  \n\n\nOn voit qu’on des données qui sont propres ,ce qui est rarement le cas dans des situations réélles.\n\n\n\nVisualisation des informations sur la distribution des variables\n\n\n\n\n\nCode\nplot_box_plot &lt;- function(variable_name_str, title = \"\") {\n  # Vérification que la variable existe dans df\n  if(!variable_name_str %in% colnames(df)) {\n    stop(\"La variable spécifiée n'existe pas dans la table\")\n  }\n\n  \n  var_titles &lt;- list(\n    \"child_mort\" = \"Taux de mortalité infantile\",\n    \"exports\"    = \"Exportations (% du PIB)\",\n    \"health\"     = \"Dépenses de santé (% du PIB)\",\n    \"imports\"    = \"Importations (% du PIB)\",\n    \"income\"     = \"Revenu par habitant (en USD)\",\n    \"inflation\"  = \"Taux d'inflation (%)\",\n    \"life_expec\" = \"Espérance de vie (en années)\",\n    \"total_fer\"  = \"Taux de fécondité (enfants par femme)\",\n    \"gdpp\"       = \"PIB par habitant (en USD)\"\n  )\n  \n  title &lt;- var_titles[[variable_name_str]]\n  # titre par défaut\n  if (title == \"\") {\n    title &lt;- paste(\"Distribution de la variable\", variable_name_str)\n  }\n\n  # Création du graphique\n  plt &lt;- ggplot(df, aes(y = .data[[variable_name_str]])) +\n    geom_boxplot(fill = \"skyblue\", color = \"darkblue\") +\n    theme_light() +\n    labs(title = title, y = variable_name_str)+\n  theme(plot.title = element_text(size = 9)) \n\n  return(plt)\n}\n\nplots &lt;- lapply(colnames(df[,-1]), plot_box_plot)\nwrap_plots(plots, ncol = 3)\n\n\n\n\n\nDistribution des variables du jeu de données"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#corrélogramme-des-variables",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#corrélogramme-des-variables",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "",
    "text": "On calcule les corrélation de Pearson quand on suspecte des relations linéaires entre les variables, quand celles -ci sont sous forme d’échelle ou de ratio. Ici en plus de ploter le heatmap des variables, nous afficherons celles qui sont significativement corrélées (Test de corrélation de Pearson cf. Annexe 1)\n\n\nCode\n# calcul de la matrice de corrélations de pearson\ncor_matrix &lt;- cor(df[, -1], method = \"pearson\")\n\n# fonction pour extraire les p-values\nget_pval &lt;- function(x, y) {\n  res &lt;- suppressWarnings(cor.test(x, y, method = \"pearson\"))\n  return(res$p.value)\n}\n\n# matrice des p-values\nn &lt;- ncol(df[, -1])\npval_matrix &lt;- matrix(NA, nrow = n, ncol = n)\nrownames(pval_matrix) &lt;- colnames(df[, -1])\ncolnames(pval_matrix) &lt;- colnames(df[, -1])\nfor (i in 1:n) {\n  for (j in 1:n) {\n    pval_matrix[i, j] &lt;- get_pval(df[, -1][[i]], df[, -1][[j]])\n  }\n}\n\nget_signif_stars &lt;- function(p) {\n  if (p &lt; 0.01) {\n    return(\"***\")\n  } else if (p &lt; 0.05) {\n    return(\"**\")\n  } else if (p &lt; 0.1) {\n    return(\"*\")\n  } else {\n    return(\"\")\n  }\n}\n\n# Création de la matrice des annotations\nnumber_labels &lt;- matrix(\"\", nrow = n, ncol = n)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    rho &lt;- cor_matrix[i, j]\n    p &lt;- pval_matrix[i, j]\n    stars &lt;- get_signif_stars(p)\n    number_labels[i, j] &lt;- paste0(sprintf(\"%.2f\", rho), stars)\n  }\n}\nrownames(number_labels) &lt;- rownames(cor_matrix)\ncolnames(number_labels) &lt;- colnames(cor_matrix)\n\n\n# heatmat sans clustering (car par defaut la fonction essaie de faire une CAH)\nplt &lt;- pheatmap(\n  cor_matrix,\n  cluster_rows = FALSE,\n  cluster_cols = FALSE,\n  color = colorRampPalette(c(\"blue\", \"white\", \"red\"))(50),\n  display_numbers = number_labels,\n  number_format = \"\",\n  fontsize_number = 8,\n  main = \"Heatmap de la corrélation de Person\",\n  angle_col = 90 \n)\nplt\n\n\n\n\n\nHeatmap des varibales quantitative de la base de données\n\n\n\n\nInterpretation :\nComme on peut le constater, plusieurs variables présentent des corrélations significatives entre elles. Cela est mis en évidence par les étoiles indiquant le niveau de signification statistique : *** pour une signification au seuil de 1 %, ** pour 5 %, et * pour 10 %. La coloration des cellules — plus elles sont proches du rouge ou du bleu foncé, plus la corrélation est forte (positive ou négative) — renforce cette lecture. Par exemple le taux de mortalité infantile est négativement corrélé à l’expérance de vie (-0,89 ***). Un taux de mortalité infantile élevé signifie que de nombreux décès surviennent très tôt dans la vie. Comme l’espérance de vie est une moyenne pondérée des âges au décès, la présence de nombreux décès précoces fait chuter la moyenne, d’où la corrélation négative. Par contre le taux de mortalité infantile est positivement corrélé à le taux de fécondité (0,85 ***). Cela peut s’expliquer par le fait que, dans les pays où le taux de fécondité est élevé, le nombre total de naissances est plus important. Dès lors, si les conditions sanitaires restent précaires, cela augmente mécaniquement le nombre d’enfants susceptibles de décéder en bas âge. Ce phénomène s’apparente à un processus binomial, où chaque naissance représente une “épreuve” avec une certaine probabilité de décès. Plus il y a d’épreuves, plus la fréquence des décès peut être élevée, ce qui se traduit par un taux de mortalité infantile plus important.\nCes interdépendances marquées entre les variables justifient le recours à une analyse en composantes principales (ACP), qui permettra de résumer l’information contenue dans ces variables corrélées tout en réduisant la dimensionnalité du jeu de données."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#mise-en-oeuvre-de-lacp",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#mise-en-oeuvre-de-lacp",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "",
    "text": "Plusieurs packages sur R permettre de mettre en oeuvre l’analyse en composantes principales. Nous utiliserons les packages FactoMineR et factoextra. Très souvent, on standardise les données avant de réaliser une analyse en composantes principales. Cette étape permet de ramener toutes les variables à une échelle comparable, en neutralisant les différences d’unités ou d’amplitudes. Ainsi, chaque variable contribue équitablement à la construction des composantes principales, sans que celles ayant une grande variance ne dominent l’analyse.\n\ndf_acp &lt;- df[, -1]\nrownames(df_acp) &lt;- df[,1]\nacp_model &lt;- PCA(df_acp, scale.unit = TRUE, graph = FALSE)\n\n\nLes infos du modèle\n\n\nnames(acp_model)\n\n[1] \"eig\"  \"var\"  \"ind\"  \"svd\"  \"call\"\n\n\n\neig : Valeurs propres associées aux composantes principales. Elles indiquent la variance expliquée par chaque axe.\nvar : Informations sur les variables actives (coordonnées, contributions, qualités de représentation (cos2)).\nind : Informations sur les individus (lignes) : coordonnées dans l’espace factoriel, contributions, cos2.\nsvd : Résultats de la décomposition en valeurs singulières (utile si vous voulez aller dans le détail mathématique).\ncall : L’appel de la fonction (PCA(...)), utile pour garder la trace de tes paramètres d’appel.\n\nMais nous allons nous concentrés que sur eig, var et ind.\n\n\n\nValeurs proppres : Choix des dimensions d’analyse\n\n\n\n\n\nCode\nfviz_eig(acp_model, geom = 'line') +\n  labs(title = \"Pourcentages des variances expliquées par les composantes principales\",\n       y = \"Pourcentage d'inertie\", x = \"Composantes principales\")\n\n\n\n\n\nDiagramme des variances expliquées par les composantes principales\n\n\n\n\n      Le screeplot (ou graphique des éboulis) met en évidence une chute marquée des valeurs propres entre la première et la deuxième dimension. Un léger coude apparaît ensuite entre la deuxième et la troisième composante. Au-delà, la décroissance devient plus faible et progressive, avec un second coude observable autour de la sixième dimension.\nPour déterminer le nombre optimal d’axes à retenir, nous nous appuierons sur deux critères complémentaires :\n\nLe critère de Kaiser, qui recommande de ne retenir que les composantes dont la valeur propre est supérieure à 1.\nLe critère du taux d’inertie, qui suggère de conserver le nombre de dimensions nécessaires pour expliquer un seuil acceptable de la variance totale, souvent fixé à 70 % ou 80 % selon le contexte.\n\n\n\nCode\nfviz_eig(acp_model, choice = \"eigenvalue\" , geom = 'bar')+\n  geom_hline(yintercept = 1, linetype = 2, color = \"red\") +\n  labs(title = \"Valeurs propres des composantes principales\",\n       y = \"Valeur propre\", x = \"Composantes principales\")\n\n\n\n\n\nDiagramme des valeurs propres issues de l’ACP\n\n\n\n\nEn se basant donc sur ces deux critère nous pouvons sélectionner les trois premières dimensions.\n\n\n\nAnalyses des variables\n\n\n\n\n\nCode\nfviz_pca_var(acp_model, col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corrélation des variables\",\n       y = \"Dimension 2\", x = \"Dimension 1\") + \n  theme_light()\n\nfviz_pca_var(acp_model, axes = c(1, 3), col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corrélation des variables\",\n       y = \"Dimension 3\", x = \"Dimension 1\") + \n  theme_light()\n\n\nfviz_pca_var(acp_model, axes = c(2, 3), col.var = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))+\n  labs(title = \"Cercle de corrélation des variables\",\n       y = \"Dimension 3\", x = \"Dimension 2\") + \n  theme_light()\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 1 et 2\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 1 et 3\n\n\n\n\n\n\n\n\n\nrepresentation des variables sur les dimension sur les axes 2 et 3\n\n\n\n\nCartes de la representation des variables sur les dimensions\n\n\n\n\n\nGraphique a)\n\non observe que les variables health et inflation sont peu bien représentées sur le plan factoriel (valeurs de cos² faibles). À l’inverse, la variable exports est celle qui est la mieux projetée sur ce plan.\n\nLes variables imports et exports sont davantage alignées avec la dimension 2, ce qui suggère que cet axe reflète essentiellement les activités économiques extérieures (importations et exportations). Nous pourrions ainsi interpréter l’Axe 2 comme celui de l’ouverture commerciale.\nConcernant l’Axe 1, il oppose le taux de fécondité et le taux de mortalité infantile (positivement corrélés entre eux) à l’espérance de vie, la croissance du PIB par tête, et aux revenus moyens par habitant. On peut donc interpréter cet axe comme celui du niveau de développement socio-économique.\nEn résumé :\n\nAxe 1 : Niveau de développement (fécondité + mortalité infantile ↔︎ espérance de vie, revenus, PIB)\nAxe 2 : Activités d’importation et d’exportation\n\nGraphique b)\n\nSur ce plan (Axe 1-3), les importations ne sont pas bien représentées, ce qui signifie qu’elles ont une faible contribution à la projection dans cet espace factoriel.\n\nLa dimension 1, comme précédemment, semble caractériser un niveau de développement socio-économique, opposant les pays à forte mortalité infantile et fécondité élevée à ceux présentant une espérance de vie plus longue, un revenu moyen plus élevé, les exportations et un PIB par tête plus important.\nQuant à l’Axe 3, il oppose principalement l’inflation aux dépenses de santé. Ces deux variables apparaissent en opposition sur cet axe, suggérant que dans les pays où l’inflation est forte, la part des dépenses de santé dans le PIB tend à être plus faible, et inversement.\nEn résumé :\n\nAxe 1 : Niveau de développement socio-économique\nAxe 3 : Opposition entre taux d’inflation et dépenses de santé\n\nGraphique b)\n\n      Toutes les variables en bleues sont mal représentées. On voit encore une oppostion entre le taux d’inflation et les dépenses de santé sur l’axe 3. Quant à l’axe 2, il décrit les activités d’importation et d’exportation.\n\n\n\n\n\n\nInterpretation des variables\n\n\n\nOn pouvait choisir d’afficher le cercle de corrélation des variables avec leur contribution respective en lieu et place de leur qualité de représentation. Le plus souvent les variables qui sont bien représentées sont celles qui contribuent le plus à la formation des axes factorielles. On peut également combiner ces deux critères.\nOn interprête que les variables qui ont une bonne contribution (critère : souvent supérieure à la contribution moyenne sur les axes choisis) ou une bonne qualité de représentations (cos2 &gt; 0,6, mais souvent subjectif).\n\n\n\n\n\nAnalyses des individus\n\n\n\n\n\nCode\nfviz_pca_ind(acp_model, \n             col.ind = \"cos2\",                # coloration des individus selon qualité dereprésentation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             geom = c(\"point\", \"text_repel\")) +   # points + labels repel\n  labs(title = \"Projection des individus (Dim 1 et 2)\",\n       x = \"Dimension 1\", y = \"Dimension 2\") +\n  theme_light()\n\nfviz_pca_ind(acp_model, axes = c(1, 3),\n             col.ind = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             geom = c(\"point\", \"text_repel\")) +\n  labs(title = \"Projection des individus (Dim 1 et 3)\",\n       x = \"Dimension 1\", y = \"Dimension 3\") +\n  theme_light()\n\nfviz_pca_ind(acp_model, axes = c(2, 3),\n             col.ind = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             geom = c(\"point\", \"text_repel\")) +\n  labs(title = \"Projection des individus (Dim 2 et 3)\",\n       x = \"Dimension 2\", y = \"Dimension 3\") +\n  theme_light()\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 1 et 2\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 1 et 3\n\n\n\n\n\n\n\n\n\nRepresentation des individus sur les dimension sur les axes 2 et 3\n\n\n\n\nCartes de la representation des variables sur les dimension\n\n\n\n\nLe principe d’analyse ne change pas. Les individus en bleu ciel sont mal représentés. On voit des points atypiques qui se démarquent. Ceux-ci contribuent fortement à la formation des axes aux quels ils sont proches.\n\n\nCode\nfviz_pca_ind(acp_model, \n             col.ind = \"contrib\",                # coloration des individus selon qualité dereprésentation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +   # points + labels repel\n  labs(title = \"Projection des individus (Dim 1 et 2)\",\n       x = \"Dimension 1\", y = \"Dimension 2\") +\n  theme_light()\n\n\n\n\n\nAffichages des individus atypiques dimensions 1 et 2\n\n\n\n\nIls s’agit de Singapore, du luxembourg et éventuellemet de Malta. Qu’on les retire ou pas cela n’aurait pas changé quelle que chose, car les variables qu’on a ont probablement toutes des dénominateurs communs en fonction de leurs définitions (la taille de la population pour les variables par hahbitants etc.).\n\ncontrib_ind_dim_1_2 &lt;- as.data.frame(acp_model$ind$contrib[, 1:2]) %&gt;% \n  filter(rownames(as.data.frame(acp_model$ind$contrib[, 1:2])) %in% c(\"Singapore\", \"Luxembourg\", \"Malta\"))\nknitr::kable(contrib_ind_dim_1_2, format = \"html\")\n\n\n\n\n\nDim.1\nDim.2\n\n\n\n\nLuxembourg\n6.928982\n9.108194\n\n\nMalta\n1.960319\n8.794095\n\n\nSingapore\n4.842860\n17.290257\n\n\n\n\n\n\n\nPour les autres plans, le processus est pareil."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#clustering-des-pays-à-laide-des-k-means",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#clustering-des-pays-à-laide-des-k-means",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "",
    "text": "Ici on applique directement les kmeans aux coordonnées factorielles issues de l’ACP. Et on avait décider de travailler que sur les trois premières dimensions. Ceci est une illustration de la réduction de dimentionalités avant de passer à l’application d’une méthode qui servira à résoudre un problème de classifications.\nEtant donné qu’on veut choisir le nombre de cluster qui non seulement minimise l’inertie intra-classe (donc maximise l’inertie inter-classe), mais à partir du quel son incrémentation ne change presque plus ou de peu l’inertie intra-classe. En d’autres termes ou on n’a plus de chute brutale. Chute car plus le nombre clusters augmente, plus l’inertie-intra classe diminue.\n\n\nCode\nset.seed(123) # fixe la graine générative\n\ndataTocluster &lt;- scale(acp_model$ind$coord[,1:3]) # selection des coordonnées factorielles sur les 3 premiers axes\n\nresKmeans &lt;- list() # pour stocker les résultats de l'algorithme\nCPtheta &lt;- rep(0, 8) # pour stocker les inerties intra-classes (8 classes choisies de manière subjective)\nfor (K in 1:8){\n  resKmeans[[K]] &lt;- kmeans(dataTocluster, K, nstart = 50)\n  CPtheta[K] &lt;- resKmeans[[K]]$tot.withinss\n}\n\n\n\n\nCode\ndf_ev_inertie &lt;- data.frame(\n  InertieIntraclasse = CPtheta,\n  NombreClusters = 1:8\n)\n\nggplot(df_ev_inertie, aes(x = NombreClusters, y = InertieIntraclasse)) +\n  geom_line() +\n  geom_point(shape = 21, fill = \"white\", color = \"black\", size = 3) +\n  labs(\n    x = \"Nombre de clusters\",\n    y = \"Inertie intra-classe\"\n  ) + theme_light()\n\n\n\n\n\nEvolution de l’inertie intra classe\n\n\n\n\nD’après le critère du coude, \\(K=4\\) (éventuellement \\(k=5\\)) est un choix pertinent. Affichons ainsi un descriptif des résultats du k-means.\n\n\nCode\nclassif_4 &lt;- resKmeans[[4]]\nstr(classif_4)\n\n\nList of 9\n $ cluster     : Named int [1:167] 2 4 3 3 4 3 4 4 4 3 ...\n  ..- attr(*, \"names\")= chr [1:167] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n $ centers     : num [1:4, 1:3] 2.204 -0.907 -0.236 0.66 3.856 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:3] \"Dim.1\" \"Dim.2\" \"Dim.3\"\n $ totss       : num 498\n $ withinss    : num [1:4] 10.7 54.5 91.6 74.6\n $ tot.withinss: num 231\n $ betweenss   : num 267\n $ size        : int [1:4] 4 52 39 72\n $ iter        : int 3\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\nCes chiffres signifient que :\n\nLa variance totale des données est de 498.\nLa variance intra-classe totale est de 231, ce qui mesure la compacité des clusters.\nLa variance inter-classe est de 267, indiquant la séparation entre les groupes.\nEnviron 54 % de la variance totale est expliquée par la partition.\nLes tailles des clusters sont inégales, allant de 4 à 72 individus.\nL’algorithme a convergé rapidement en 3 itérations.\n\nLa partition en 4 clusters présenterait donc un bon équilibre entre homogénéité interne et séparation externe.\n\n\nCode\nclassif_5 &lt;- resKmeans[[5]]\nstr(classif_5)\n\n\nList of 9\n $ cluster     : Named int [1:167] 5 3 1 1 4 3 3 3 3 1 ...\n  ..- attr(*, \"names\")= chr [1:167] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n $ centers     : num [1:5, 1:3] -7.97e-05 2.68 7.00e-01 2.53e-01 -1.09 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:3] \"Dim.1\" \"Dim.2\" \"Dim.3\"\n $ totss       : num 498\n $ withinss    : num [1:5] 54.3 4.79 36.17 53.68 43.34\n $ tot.withinss: num 192\n $ betweenss   : num 306\n $ size        : int [1:5] 20 3 45 51 48\n $ iter        : int 4\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\nCes chiffres signifient que :\n\nLa variance totale des données est de 498.\nLa variance intra-classe totale est de 192, indiquant la compacité des clusters.\nLa variance inter-classe est de 306, mesurant la séparation entre les groupes.\nEnviron 61% de la variance totale est expliquée par la partition, ce qui est une amélioration par rapport à k=4.\nLes tailles des clusters varient de 3 à 51 individus, indiquant une répartition inégale.\nL’algorithme a convergé en 4 itérations.\n\n      La partition en 5 clusters améliore la séparation entre groupes et la compacité interne.\n\n\nCode\ntable(classif_5$cluster)\n\n\n\n 1  2  3  4  5 \n20  3 45 51 48 \n\n\nOn choisit \\(k=5\\) en combinant critère du coude et variance expliquée.\n\n\n\n\n\n\nChoix de k (nombre de clusters)\n\n\n\nLe choix du k est un peu subjctif mais on a plusieurs règles permettant de le choisir. Il s’agit entre autre du critère du taux d’inertie, critère du coude etc. On aurait donc bien pu prendre k=4.\n\n\n\n\n\nReprésentations graphiques\n\n\n\n\n\nCode\ndataTocluster &lt;- as.data.frame(dataTocluster)\ndataTocluster &lt;- dataTocluster %&gt;% \n  mutate (classe = as.factor(classif_5$cluster))\n\nggplot(dataTocluster, aes (x = Dim.1, y = Dim.2, color = classe)) +\n  geom_point() +\n  geom_text_repel(aes(label = rownames(dataTocluster)), size = 3) + \n  labs(\n    title = \"Classification des pays (Axe 1 - Axe 3)\",\n    x = \"Dimension 1\",\n    y = \"Dimension 3\",\n    color = \"Classe/Cluster\"\n  )+\n  theme_light()\n\n\n\n\n\nClassification des pays sur la base de leurs données socio-économiques\n\n\n\n\n      Certains points sont non labellisés pour éviter que les noms des pays ne se chevauchent, ce qui pourrait nuire à la lisibilité du graphique. Vous trouverez en annexe 4 les tables détaillant les différentes classes.\nSur le premier plan factoriel, on observe que Singapour, Malte et le Luxembourg appartiennent à la même classe. Cette proximité pourrait s’expliquer par plusieurs facteurs communs à ces pays (cf. annexe 3):\n\nEspérance de vie élevée : Ces pays affichent une espérance de vie parmi les plus hautes au monde, reflétant une qualité de vie et des systèmes de santé performants.\nFaible taux de mortalité infantile : Les taux de mortalité infantile y sont très bas, indiquant un accès généralisé aux soins prénatals et postnatals de qualité.\nRevenu par habitant élevé : Le PIB par habitant est significativement supérieur à la moyenne mondiale, traduisant une économie développée et stable.\nDépenses de santé élevées en pourcentage du PIB : Ces pays investissent une part importante de leur PIB dans la santé, ce qui se traduit par des infrastructures médicales avancées et un personnel soignant bien formé.\nFaible taux de fécondité : Le taux de fécondité y est inférieur au seuil de renouvellement des générations, ce qui est caractéristique des pays développés avec un niveau d’éducation élevé et une urbanisation importante.\n\nCes similitudes dans les indicateurs socio-économiques et sanitaires justifient leur regroupement sur le plan factoriel de l’ACP.\nVous pouvez analyser le graphique sur le plan 1-3.\n\n\nCode\ndataTocluster &lt;- as.data.frame(dataTocluster)\ndataTocluster &lt;- dataTocluster %&gt;% \n  mutate (classe = as.factor(classif_5$cluster))\n\nggplot(dataTocluster, aes(x = Dim.1, y = Dim.3, color = classe)) +\n  geom_point() +\n  geom_text_repel(aes(label = rownames(dataTocluster)), size = 3) +  # cex ≈ size = 3\n  labs(\n    title = \"Classification des pays (Axe 1 - Axe 3)\",\n    x = \"Dimension 1\",\n    y = \"Dimension 3\",\n    color = \"Classe/Cluster\"\n  ) +\n  theme_light()\n\n\n\n\n\nClassification des pays sur la base de leurs données socio-économiques"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-1-test-de-corrélation-de-pearson",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-1-test-de-corrélation-de-pearson",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "Annexe 1 : Test de corrélation de Pearson",
    "text": "Annexe 1 : Test de corrélation de Pearson\nLe test de corrélation de Pearson est utilisé pour évaluer la présence d’une relation linéaire significative entre deux variables quantitatives continues. Il est adapté lorsque les données suivent une distribution normale ou sont suffisamment symétriques.\n\nObjectif du test\nDéterminer s’il existe une corrélation linéaire significative entre deux variables \\(X\\) et \\(Y\\).\n\n\nParamètre testé\nLe coefficient de corrélation linéaire \\(\\rho\\), mesurant l’intensité et le sens de la relation linéaire entre \\(X\\) et \\(Y\\) dans la population.\n\\[\n\\rho = \\text{Corr}(X, Y)\n\\]\n\n\nHypothèses\n\n\n\nHypothèse nulle \\(H_0\\) :\n\\[\n\\rho = 0\n\\] (pas de corrélation linéaire entre \\(X\\) et \\(Y\\)\nHypothèse alternative \\(H_1\\) :\n\\[\n\\rho \\ne 0\n\\] (corrélation linéaire significative)\n\n\n\nStatistique de test\n\n\nÀ partir du coefficient de corrélation de l’échantillon \\(\\rho\\), la statistique de test est :\n\\[\nt = \\frac{\\rho \\sqrt{n - 2}}{\\sqrt{1 - \\rho^2}}\n\\]\n\n\nDistribution de la statistique sous \\(H_0\\)\n\n\nSous \\(H_0\\), la statistique suit une loi de Student à \\(n - 2\\) degrés de liberté :\n\\[\nt \\sim \\mathcal{T}(n - 2)\n\\]\n\n\nRègle de décision\n\n\n\nChoisir un seuil de signification \\(\\alpha\\) (généralement \\(\\alpha = 0{,}05\\)).\nCalculer la valeur critique ou la p-value.\nRejeter \\(H_0\\) si :\n\n\\[\n|t| &gt; t_{\\alpha/2, n - 2}\n\\]\nou\n\\[\n\\text{p-value} &lt; \\alpha\n\\]\n\n\nRégion de rejet\n\n\nPour un test bilatéral :\n\\[\n\\mathcal{R} = \\left\\{ t : |t| &gt; t_{\\alpha/2, n - 2} \\right\\}\n\\]\n\n\nConditions d’application\n\n\n\nLes deux variables doivent être quantitatives continues.\nRelation supposée linéaire (à vérifier avec un nuage de points).\nNormalité des variables ou grande taille de l’échantillon."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-2-algorithme-des-k-moyennes-et-détermination-du-nombre-optimal-de-groupes",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-2-algorithme-des-k-moyennes-et-détermination-du-nombre-optimal-de-groupes",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "Annexe 2 : Algorithme des k-moyennes et détermination du nombre optimal de groupes",
    "text": "Annexe 2 : Algorithme des k-moyennes et détermination du nombre optimal de groupes\nL’algorithme des k-moyennes (ou nuées dynamiques) est une méthode de classification non supervisée permettant de regrouper des individus en k groupes homogènes. Il repose sur la minimisation de l’inertie intra-classe, c’est-à-dire la somme des distances quadratiques entre chaque individu et le centre de son groupe.\nL’algorithme fonctionne de la manière suivante :\n\nChoisir le nombre de groupes \\(k\\) à former (déterminé à l’avance).\nInitialiser \\(k\\) centres aléatoires.\nAffecter chaque individu au centre le plus proche.\nRecalculer les centres (barycentres) des groupes.\nRépéter les étapes 3 et 4 jusqu’à stabilisation des centres.\n\n\n\n\nAlgorithme 1 : Algorithme de Lloyd\n\n\n\nDonnées : \\(x_1, \\dots, x_n\\)\nInitialisation : poser \\(m = 0\\) et tirer au hasard \\(K\\) points de \\(\\mathbb{R}^p\\) comme centres initiaux : \\[\n\\mu_1^{[m]}, \\dots, \\mu_K^{[m]}\n\\]\n\nTant que la partition n’est pas stable :\n\nIncrémentation du compteur : \\[\nm \\leftarrow m + 1\n\\]\nMise à jour de la partition à centres fixés :\n\nAffecter chaque individu à la classe dont le centre est le plus proche : \\[\nP_k^{[m]} = \\left\\{ i : d_M(x_i, \\mu_k^{[m-1]}) \\leq d_M(x_i, \\mu_{k'}^{[m-1]}) \\ \\forall k' = 1, \\dots, K \\right\\}\n\\]\n\nMise à jour des centres à partition fixée : \\[\n\\mu_k^{[m]} = \\frac{\\sum_{i=1}^n z_{ik}^{[m]} x_i}{\\sum_{i=1}^n z_{ik}^{[m]}}\n\\]\n\navec : \\[\nz_{ik}^{[m]} =\n\\begin{cases}\n1 & \\text{si } i \\in P_k^{[m]} \\\\\n0 & \\text{sinon}\n\\end{cases}\n\\]\n\nRésultat final :\nLa partition : \\[\n\\mathcal{P}^{[m]} = \\{P_1^{[m]}, \\dots, P_K^{[m]}\\}\n\\]\net les centres associés : \\[\n\\mu_1^{[m]}, \\dots, \\mu_K^{[m]}\n\\]\n\nSource : Cours Kmeans – ENSAI 1A, 2024-2025\nEnseignant : Javier GONZALEZ\n\n\n\n\n\n\nNote\n\n\n\nCe processus converge généralement rapidement et il est très efficace, même sur de grands jeux de données.\n\n\n\nUtilisation après ACP\nL’algorithme des k-moyennes s’applique sur des variables quantitatives. On peut donc l’utiliser directement sur les coordonnées factorielles obtenues via une Analyse en Composantes Principales (ACP).\nCela présente plusieurs avantages :\n\nRéduction de la dimension : seules les premières composantes (celles expliquant le plus de variance) sont conservées.\nÉlimination de la redondance : les variables sont orthogonales.\nMeilleure visualisation des structures.\n\nL’application des k-moyennes après ACP permet donc de classer les individus dans l’espace factoriel de manière plus claire et plus efficace.\n\n\nDétermination du nombre optimal de groupes \\(k\\)\nLe choix de \\(k\\) n’a pas besoin d’être exact à l’unité près car l’algorithme reste robuste. Toutefois, on peut s’aider d’un critère objectif : la courbe des inerties intra-classes en fonction de \\(k\\).\nLa courbe présente souvent une cassure (méthode du coude), indiquant un bon compromis entre complexité du modèle et qualité du regroupement."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-3-similitude-de-singapore-malta-et-luxembourg",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-3-similitude-de-singapore-malta-et-luxembourg",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "Annexe 3 : Similitude de Singapore, Malta et Luxembourg",
    "text": "Annexe 3 : Similitude de Singapore, Malta et Luxembourg\n\n\n\nCaractérisiques socio-économiques de Singapore, de Malta du Luxembourg \n\n\nCountry\nchild_mort\nexports\nhealth\nimports\nincome\ninflation\nlife_expec\ntotal_fer\ngdpp\n\n\n\n\nLuxembourg\n2.8\n175\n7.77\n142\n91700\n3.620\n81.3\n1.63\n105000\n\n\nMalta\n6.8\n153\n8.65\n154\n28300\n3.830\n80.3\n1.36\n21100\n\n\nSingapore\n2.8\n200\n3.96\n174\n72100\n-0.046\n82.7\n1.15\n46600\n\n\n\n\n\n\n\nOn peut voir que les caractéristiques sont très proches mises à part quelques unes."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-4-tables-des-clusters-des-pays-du-premier-plan-factoriel",
    "href": "ANALYSES_FACTORIELLES/acp-kmeans.html#annexe-4-tables-des-clusters-des-pays-du-premier-plan-factoriel",
    "title": "Vers une meilleure compréhension des facteurs de développement : réduction de dimension et classification des pays par K-means sur des indicateurs socio-économiques mondiaux",
    "section": "Annexe 4 : Tables des clusters des pays du premier plan factoriel",
    "text": "Annexe 4 : Tables des clusters des pays du premier plan factoriel\n\n\n\nLes pays du cluster 1 \n\n\n\nclasse\nPays du cluster\n\n\n\n\nAlgeria\n1\nAlgeria\n\n\nAngola\n1\nAngola\n\n\nAzerbaijan\n1\nAzerbaijan\n\n\nBahrain\n1\nBahrain\n\n\nBrunei\n1\nBrunei\n\n\nCongo, Rep.\n1\nCongo, Rep.\n\n\nEquatorial Guinea\n1\nEquatorial Guinea\n\n\nGabon\n1\nGabon\n\n\nIndonesia\n1\nIndonesia\n\n\nKazakhstan\n1\nKazakhstan\n\n\nKuwait\n1\nKuwait\n\n\nLibya\n1\nLibya\n\n\nMongolia\n1\nMongolia\n\n\nNigeria\n1\nNigeria\n\n\nOman\n1\nOman\n\n\nQatar\n1\nQatar\n\n\nSaudi Arabia\n1\nSaudi Arabia\n\n\nSri Lanka\n1\nSri Lanka\n\n\nUnited Arab Emirates\n1\nUnited Arab Emirates\n\n\nVenezuela\n1\nVenezuela\n\n\n\n\n\n\n\n\n\n\nLes pays du cluster 2 \n\n\n\nclasse\nPays du cluster\n\n\n\n\nLuxembourg\n2\nLuxembourg\n\n\nMalta\n2\nMalta\n\n\nSingapore\n2\nSingapore\n\n\n\n\n\n\n\n\n\n\nLes pays du cluster 3 \n\n\n\nclasse\nPays du cluster\n\n\n\n\nAlbania\n3\nAlbania\n\n\nArgentina\n3\nArgentina\n\n\nArmenia\n3\nArmenia\n\n\nAustralia\n3\nAustralia\n\n\nAustria\n3\nAustria\n\n\nBahamas\n3\nBahamas\n\n\nBarbados\n3\nBarbados\n\n\nBosnia and Herzegovina\n3\nBosnia and Herzegovina\n\n\nBrazil\n3\nBrazil\n\n\nCanada\n3\nCanada\n\n\nChile\n3\nChile\n\n\nChina\n3\nChina\n\n\nColombia\n3\nColombia\n\n\nCosta Rica\n3\nCosta Rica\n\n\nCroatia\n3\nCroatia\n\n\nCyprus\n3\nCyprus\n\n\nDenmark\n3\nDenmark\n\n\nDominican Republic\n3\nDominican Republic\n\n\nEcuador\n3\nEcuador\n\n\nEl Salvador\n3\nEl Salvador\n\n\nFinland\n3\nFinland\n\n\nFrance\n3\nFrance\n\n\nGermany\n3\nGermany\n\n\nGreece\n3\nGreece\n\n\nIceland\n3\nIceland\n\n\nIran\n3\nIran\n\n\nIsrael\n3\nIsrael\n\n\nItaly\n3\nItaly\n\n\nJapan\n3\nJapan\n\n\nNew Zealand\n3\nNew Zealand\n\n\nNorway\n3\nNorway\n\n\nPeru\n3\nPeru\n\n\nPoland\n3\nPoland\n\n\nPortugal\n3\nPortugal\n\n\nRomania\n3\nRomania\n\n\nRussia\n3\nRussia\n\n\nSerbia\n3\nSerbia\n\n\nSouth Korea\n3\nSouth Korea\n\n\nSpain\n3\nSpain\n\n\nSweden\n3\nSweden\n\n\nSwitzerland\n3\nSwitzerland\n\n\nTurkey\n3\nTurkey\n\n\nUnited Kingdom\n3\nUnited Kingdom\n\n\nUnited States\n3\nUnited States\n\n\nUruguay\n3\nUruguay\n\n\n\n\n\n\n\n\n\n\nLes pays du cluster 4 \n\n\n\nclasse\nPays du cluster\n\n\n\n\nAntigua and Barbuda\n4\nAntigua and Barbuda\n\n\nBelarus\n4\nBelarus\n\n\nBelgium\n4\nBelgium\n\n\nBelize\n4\nBelize\n\n\nBhutan\n4\nBhutan\n\n\nBotswana\n4\nBotswana\n\n\nBulgaria\n4\nBulgaria\n\n\nCambodia\n4\nCambodia\n\n\nCape Verde\n4\nCape Verde\n\n\nCzech Republic\n4\nCzech Republic\n\n\nEstonia\n4\nEstonia\n\n\nFiji\n4\nFiji\n\n\nGeorgia\n4\nGeorgia\n\n\nGrenada\n4\nGrenada\n\n\nGuyana\n4\nGuyana\n\n\nHungary\n4\nHungary\n\n\nIreland\n4\nIreland\n\n\nJamaica\n4\nJamaica\n\n\nJordan\n4\nJordan\n\n\nKiribati\n4\nKiribati\n\n\nKyrgyz Republic\n4\nKyrgyz Republic\n\n\nLatvia\n4\nLatvia\n\n\nLebanon\n4\nLebanon\n\n\nLesotho\n4\nLesotho\n\n\nLiberia\n4\nLiberia\n\n\nLithuania\n4\nLithuania\n\n\nMacedonia, FYR\n4\nMacedonia, FYR\n\n\nMalaysia\n4\nMalaysia\n\n\nMaldives\n4\nMaldives\n\n\nMauritius\n4\nMauritius\n\n\nMicronesia, Fed. Sts.\n4\nMicronesia, Fed. Sts.\n\n\nMoldova\n4\nMoldova\n\n\nMontenegro\n4\nMontenegro\n\n\nMorocco\n4\nMorocco\n\n\nNamibia\n4\nNamibia\n\n\nNetherlands\n4\nNetherlands\n\n\nPanama\n4\nPanama\n\n\nParaguay\n4\nParaguay\n\n\nSamoa\n4\nSamoa\n\n\nSeychelles\n4\nSeychelles\n\n\nSlovak Republic\n4\nSlovak Republic\n\n\nSlovenia\n4\nSlovenia\n\n\nSolomon Islands\n4\nSolomon Islands\n\n\nSt. Vincent and the Grenadines\n4\nSt. Vincent and the Grenadines\n\n\nSuriname\n4\nSuriname\n\n\nThailand\n4\nThailand\n\n\nTunisia\n4\nTunisia\n\n\nTurkmenistan\n4\nTurkmenistan\n\n\nUkraine\n4\nUkraine\n\n\nVanuatu\n4\nVanuatu\n\n\nVietnam\n4\nVietnam\n\n\n\n\n\n\n\n\n\n\nLes pays du cluster 5 \n\n\n\nclasse\nPays du cluster\n\n\n\n\nAfghanistan\n5\nAfghanistan\n\n\nBangladesh\n5\nBangladesh\n\n\nBenin\n5\nBenin\n\n\nBolivia\n5\nBolivia\n\n\nBurkina Faso\n5\nBurkina Faso\n\n\nBurundi\n5\nBurundi\n\n\nCameroon\n5\nCameroon\n\n\nCentral African Republic\n5\nCentral African Republic\n\n\nChad\n5\nChad\n\n\nComoros\n5\nComoros\n\n\nCongo, Dem. Rep.\n5\nCongo, Dem. Rep.\n\n\nCote d'Ivoire\n5\nCote d'Ivoire\n\n\nEgypt\n5\nEgypt\n\n\nEritrea\n5\nEritrea\n\n\nGambia\n5\nGambia\n\n\nGhana\n5\nGhana\n\n\nGuatemala\n5\nGuatemala\n\n\nGuinea\n5\nGuinea\n\n\nGuinea-Bissau\n5\nGuinea-Bissau\n\n\nHaiti\n5\nHaiti\n\n\nIndia\n5\nIndia\n\n\nIraq\n5\nIraq\n\n\nKenya\n5\nKenya\n\n\nLao\n5\nLao\n\n\nMadagascar\n5\nMadagascar\n\n\nMalawi\n5\nMalawi\n\n\nMali\n5\nMali\n\n\nMauritania\n5\nMauritania\n\n\nMozambique\n5\nMozambique\n\n\nMyanmar\n5\nMyanmar\n\n\nNepal\n5\nNepal\n\n\nNiger\n5\nNiger\n\n\nPakistan\n5\nPakistan\n\n\nPhilippines\n5\nPhilippines\n\n\nRwanda\n5\nRwanda\n\n\nSenegal\n5\nSenegal\n\n\nSierra Leone\n5\nSierra Leone\n\n\nSouth Africa\n5\nSouth Africa\n\n\nSudan\n5\nSudan\n\n\nTajikistan\n5\nTajikistan\n\n\nTanzania\n5\nTanzania\n\n\nTimor-Leste\n5\nTimor-Leste\n\n\nTogo\n5\nTogo\n\n\nTonga\n5\nTonga\n\n\nUganda\n5\nUganda\n\n\nUzbekistan\n5\nUzbekistan\n\n\nYemen\n5\nYemen\n\n\nZambia\n5\nZambia"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#contexte",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#contexte",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Contexte",
    "text": "Contexte\n\nLe jeu de données mtcars est l’un des ensembles de données les plus connus en statistiques et science des données. Il contient des informations sur les spécifications techniques et les performances de 32 modèles de voitures des années 1970. Ce dataset offre une opportunité unique d’explorer des relations entre des variables mécaniques, comme la consommation en carburant, la puissance ou encore le poids des véhicules."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#problématique",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#problématique",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Problématique",
    "text": "Problématique\n\nComment exploiter les relations entre les caractéristiques des voitures pour identifier des groupes ou des tendances qui pourraient aider à la prise de décision dans le secteur automobile ?"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectif-général",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectif-général",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Objectif général",
    "text": "Objectif général\n\nÉtudier les relations entre les caractéristiques techniques des voitures afin de dégager des tendances et des informations utiles pour la conception ou la sélection des véhicules."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectifs-spécifiques",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectifs-spécifiques",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Objectifs spécifiques",
    "text": "Objectifs spécifiques\n\n\nExplorer les relations entre la consommation en carburant (mpg) et les caractéristiques mécaniques\n\n\n\n\nIdentifier des groupes de voitures ayant des caractéristiques similaires à l’aide d’analyses descriptives et graphiques."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#matériels",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#matériels",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Matériels",
    "text": "Matériels\n\nLogiciel utilisé : RStudio avec les packages nécessaires (ggplot2, dplyr, cowplot, etc.)\nSource des données : Jeu de données intégré mtcars."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\n\nNettoyage des données : Vérification des valeurs manquantes ou aberrantes.\nAnalyse descriptive : Moyennes, médianes, écart-types pour chaque variable."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\nModélisation multivariée : Variables utilisées\n\nmpg : Consommation de carburant en miles par gallon (variable dépendante).\nwt : Poids du véhicule (en milliers de livres).\ncyl : Nombre de cylindres du moteur.\nam: Type de transmission (0 = automatique, 1 = manuelle).\ncarb : Nombre de carburateurs.\nhp : Puissance brute du moteur (en chevaux-vapeur)."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-2",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-2",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\nModélisation multivariée :\n\\[\n\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]\noù :\n\n\\(Y\\): Vecteur des valeurs observées (dépendantes ici mpg)\n\\(X\\) : Matrice des variables explicatives (indépendantes), incluant une colonne de 1 pour l’intercept.\n\\(\\beta\\) : Vecteur des coefficients estimés du modèle.\n\\(\\epsilon\\) : Vecteur des erreurs résiduelles."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-3",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-3",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\nTests de significativité des coefficients\nTest t de Student\n\nHypothèse nulle  \\(H_0\\) : le coefficient est égal à zéro (c’est-à-dire, la variable n’a pas d’effet significatif).\nHypothèse alternative \\(H_a\\) : Le coefficient est différent de zéro.\n\nSi la p-valeur est inférieure à un seuil significatif \\(p &lt; 0.05\\), nous rejetons l’hypothèse nulle et concluons que la variable a un effet significatif sur la variable dépendante."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-4",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-4",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\nSignificativité globale du modèle : Test F\n\nHypothèse nulle \\(H_0\\): Tous les coefficients sont égaux à zéro (pas de pouvoir explicatif).\nHypothèse alternative \\(H_a\\) : Au moins un coefficient est différent de zéro (le modèle est significatif).\n\nSi la p-valeur du test \\(F\\) est inférieure à \\(0.05\\), nous rejetons l’hypothèse nulle et concluons que le modèle est significatif."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-5",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-5",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\nR-carré : qualité d’ajustement\n\n\\(R^2\\) varie entre 0 et 1 :\n\nUn \\(R^2\\) proche de 1 signifie que le modèle explique bien les variations de la variable dépendante.\nUn \\(R^2\\) proche de 0 indique que le modèle n’explique que peu ou pas les variations de la variable dépendante."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-6",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-6",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\n\nVisualisations :\n\nGraphiques de dispersion (scatterplots) pour étudier les corrélations\nHistogrammes pour analyser la distribution des variables"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#présentation-de-léchantillon",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#présentation-de-léchantillon",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Présentation de l’échantillon",
    "text": "Présentation de l’échantillon\n\n\n\n\n\nTable 1 : Description des variables du jeu de données\n\n\nColonne\nNom\nDescription\n\n\n\n\n[,1]\nmpg\nMiles par gallon (US)\n\n\n[,2]\ncyl\nNombre de cylindres\n\n\n[,3]\ndisp\nCylindrée (en pouces cubes)\n\n\n[,4]\nhp\nPuissance brute (chevaux)\n\n\n[,5]\ndrat\nRapport du pont arrière\n\n\n[,6]\nwt\nPoids (en milliers de livres)\n\n\n[,7]\nqsec\nTemps pour parcourir 1/4 de mile\n\n\n[,8]\nvs\nType de moteur (0 = V, 1 = ligne droite)\n\n\n[,9]\nam\nType de transmission (0 = automatique, 1 = manuelle)\n\n\n[,10]\ngear\nNombre de vitesses avant\n\n\n[,11]\ncarb\nNombre de carburateurs\n\n\n\na R : mtcars"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#présentation-de-léchantillon-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#présentation-de-léchantillon-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Présentation de l’échantillon",
    "text": "Présentation de l’échantillon\nRésumé statistiques\n\n\n\n\nTable 2 : Résumé statistique des variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nmin\n10.400000\n4.000000\n71.1000\n52.00000\n2.7600000\n1.5130000\n14.500000\n0.0000000\n0.0000000\n3.0000000\n1.0000\n\n\nQ1.25%\n15.425000\n4.000000\n120.8250\n96.50000\n3.0800000\n2.5812500\n16.892500\n0.0000000\n0.0000000\n3.0000000\n2.0000\n\n\nQ3.75%\n22.800000\n8.000000\n326.0000\n180.00000\n3.9200000\n3.6100000\n18.900000\n1.0000000\n1.0000000\n4.0000000\n4.0000\n\n\nmed.50%\n19.200000\n6.000000\n196.3000\n123.00000\n3.6950000\n3.3250000\n17.710000\n0.0000000\n0.0000000\n4.0000000\n2.0000\n\n\nmean\n20.090625\n6.187500\n230.7219\n146.68750\n3.5965625\n3.2172500\n17.848750\n0.4375000\n0.4062500\n3.6875000\n2.8125\n\n\nmax\n33.900000\n8.000000\n472.0000\n335.00000\n4.9300000\n5.4240000\n22.900000\n1.0000000\n1.0000000\n5.0000000\n8.0000\n\n\ncount\n32.000000\n32.000000\n32.0000\n32.00000\n32.0000000\n32.0000000\n32.000000\n32.0000000\n32.0000000\n32.0000000\n32.0000\n\n\nsd\n6.026948\n1.785922\n123.9387\n68.56287\n0.5346787\n0.9784574\n1.786943\n0.5040161\n0.4989909\n0.7378041\n1.6152\n\n\nNA’s\n0.000000\n0.000000\n0.0000\n0.00000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.0000\n\n\n\nNote: aR : mtcars"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats principaux : Modélisation",
    "text": "Résultats principaux : Modélisation\nSélection de modèle en ajoutant ou en supprimant des variables pour minimiser l’AIC\n\n\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI\n      p-value\n    \n  \n  \n    wt\n-3.9\n-5.4, -2.5\n&lt;0.001\n    am\n2.9\n0.05, 5.8\n0.047\n    qsec\n1.2\n0.63, 1.8\n&lt;0.001\n  \n  \n    \n      Abbreviation: CI = Confidence Interval"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats principaux : Modélisation",
    "text": "Résultats principaux : Modélisation\nVariable wt (poids du véhicule)\n🔻 Coefficient : -3.9\n📌 Interprétation :\n\nChaque augmentation d’une unité du poids diminue la consommation estimée de 3.9 mpg.\n\n📉 Effet négatif significatif :\n\np &lt; 0.001 : plus le véhicule est lourd, moins il est économe en carburant."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation-2",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation-2",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats principaux : Modélisation",
    "text": "Résultats principaux : Modélisation\nVariable am (type de transmission)\n🔺 Coefficient : 2.9\n📌 Interprétation : - Les véhicules à transmission manuelle consomment en moyenne 2.9 mpg de plus que ceux à transmission automatique, toutes choses égales par ailleurs.\n✅ Effet significatif : - p = 0.047 (significatif au seuil de 5 %)"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation-3",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation-3",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats principaux : Modélisation",
    "text": "Résultats principaux : Modélisation\nVariable qsec (temps sur 1/4 de mile)\n🔺 Coefficient : 1.2\n📌 Interprétation : - Chaque seconde supplémentaire pour faire le 1/4 mile est associée à une augmentation de 1.2 mpg.\n✅ Effet positif et hautement significatif : - p &lt; 0.001"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-sécondaires",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-sécondaires",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats sécondaires",
    "text": "Résultats sécondaires\nRépartition des voitures par cylindres\n\nLa majorité des voitures ont 4 ou 8 cylindres."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-sécondaires-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-sécondaires-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats sécondaires",
    "text": "Résultats sécondaires\nRépartition des voitures par transmission"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html",
    "href": "FORMATIONS/logistic_regression_diabetes.html",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "",
    "text": "Le modèle logistique est une technique statistique largement utilisée pour modéliser des variables dépendantes binaires ou des proportions. Il est fondamental en économétrie, en sciences sociales, en biostatistique et dans de nombreux autres domaines."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#importation-des-bibliothèques-necessaires",
    "href": "FORMATIONS/logistic_regression_diabetes.html#importation-des-bibliothèques-necessaires",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Importation des bibliothèques necessaires",
    "text": "Importation des bibliothèques necessaires\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nimport numpy as np"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-données-et-verification-suscinte-de-leur-qualité",
    "href": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-données-et-verification-suscinte-de-leur-qualité",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Chargement des données et verification suscinte de leur qualité",
    "text": "Chargement des données et verification suscinte de leur qualité\n      Avant de commencer quoi que ce soit, il est nécessaire de préciser que les données proviennent de la plateforme kaggle (!cliquez ici pour y acceder).\n\ndf = pd.read_csv('diabetes-dataset.csv')\nprint('\\nAffichage des données\\n')\n\n\nAffichage des données\n\ndisplay(df.head(5))\n\n   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n0            6      148             72  ...                     0.627   50        1\n1            1       85             66  ...                     0.351   31        0\n2            8      183             64  ...                     0.672   32        1\n3            1       89             66  ...                     0.167   21        0\n4            0      137             40  ...                     2.288   33        1\n\n[5 rows x 9 columns]\n\nprint('\\nInformations sur les données\\n')\n\n\nInformations sur les données\n\ndisplay(df.info)\n\n&lt;bound method DataFrame.info of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;\n\nprint('\\nResumé statistique des données\\n')\n\n\nResumé statistique des données\n\ndisplay(df.describe)\n\n&lt;bound method NDFrame.describe of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-données-et-verification-suscinte-de-leur-qualité-1",
    "href": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-données-et-verification-suscinte-de-leur-qualité-1",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Chargement des données et verification suscinte de leur qualité",
    "text": "Chargement des données et verification suscinte de leur qualité\n\nAffichage des informations sur les données\n\ndf = pd.read_csv('diabetes-dataset.csv')\nprint('\\nAffichage des données\\n')\n\n\nAffichage des données\n\ndisplay(df.head(5))\n\n   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n0            6      148             72  ...                     0.627   50        1\n1            1       85             66  ...                     0.351   31        0\n2            8      183             64  ...                     0.672   32        1\n3            1       89             66  ...                     0.167   21        0\n4            0      137             40  ...                     2.288   33        1\n\n[5 rows x 9 columns]\n\nprint('\\nInformations sur les données\\n')\n\n\nInformations sur les données\n\ndisplay(df.info)\n\n&lt;bound method DataFrame.info of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;\n\nprint('\\nResumé statistique des données\\n')\n\n\nResumé statistique des données\n\ndisplay(df.describe)\n\n&lt;bound method NDFrame.describe of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;\n\n\n\n\nVérification des valeurs manquantes\n\ndf.columns.isna().sum()\n\n0\n\n\n      Il y’ a aucune valeur manquante car les données ont bien été nettoyées avant d’être mise à disposition sur kaggle.\n\n\nAffichage des statistiques des variables\n      Etant données que les informations sur les variables sont en ce moment ou j’écris indisponibles sur kaggle.\n\nprint('Affichage des valeurs uniques des variables\\n')\n\nAffichage des valeurs uniques des variables\n\nfor variable in df.columns:\n    if variable != \"DiabetesPedigreeFunction\": # je saute car ça fait beaucoup long à l'affichage\n      print(f'\\n {variable}\\n')\n      print(df[variable].unique())\n\n\n Pregnancies\n\n[ 6  1  8  0  5  3 10  2  4  7  9 11 13 15 17 12 14]\n\n Glucose\n\n[148  85 183  89 137 116  78 115 197 125 110 168 139 189 166 100 118 107\n 103 126  99 196 119 143 147  97 145 117 109 158  88  92 122 138 102  90\n 111 180 133 106 171 159 146  71 105 101 176 150  73 187  84  44 141 114\n  95 129  79   0  62 131 112 113  74  83 136  80 123  81 134 142 144  93\n 163 151  96 155  76 160 124 162 132 120 173 170 128 108 154  57 156 153\n 188 152 104  87  75 179 130 194 181 135 184 140 177 164  91 165  86 193\n 191 161 167  77 182 157 178  61  98 127  82  72 172  94 175 195  68 186\n 198 121  67 174 199  56 169 149  65 190]\n\n BloodPressure\n\n[ 72  66  64  40  74  50   0  70  96  92  80  60  84  30  88  90  94  76\n  82  75  58  78  68 110  56  62  85  86  48  44  65 108  55 122  54  52\n  98 104  95  46 102 100  61  24  38 106 114]\n\n SkinThickness\n\n[35 29  0 23 32 45 19 47 38 30 41 33 26 15 36 11 31 37 42 25 18 24 39 27\n 21 34 10 60 13 20 22 28 54 40 51 56 14 17 50 44 12 46 16  7 52 43 48  8\n 49 63 99]\n\n Insulin\n\n[  0  94 168  88 543 846 175 230  83  96 235 146 115 140 110 245  54 192\n 207  70 240  82  36  23 300 342 304 142 128  38 100  90 270  71 125 176\n  48  64 228  76 220  40 152  18 135 495  37  51  99 145 225  49  50  92\n 325  63 284 119 204 155 485  53 114 105 285 156  78 130  55  58 160 210\n 318  44 190 280  87 271 129 120 478  56  32 744 370  45 194 680 402 258\n 375 150  67  57 116 278 122 545  75  74 182 360 215 184  42 132 148 180\n 205  85 231  29  68  52 255 171  73 108  43 167 249 293  66 465  89 158\n  84  72  59  81 196 415 275 165 579 310  61 474 170 277  60  14  95 237\n 191 328 250 480 265 193  79  86 326 188 106  65 166 274  77 126 330 600\n 185  25  41 272 321 144  15 183  91  46 440 159 540 200 335 387  22 291\n 392 178 127 510  16 112]\n\n BMI\n\n[33.6 26.6 23.3 28.1 43.1 25.6 31.  35.3 30.5  0.  37.6 38.  27.1 30.1\n 25.8 30.  45.8 29.6 43.3 34.6 39.3 35.4 39.8 29.  36.6 31.1 39.4 23.2\n 22.2 34.1 36.  31.6 24.8 19.9 27.6 24.  33.2 32.9 38.2 37.1 34.  40.2\n 22.7 45.4 27.4 42.  29.7 28.  39.1 19.4 24.2 24.4 33.7 34.7 23.  37.7\n 46.8 40.5 41.5 25.  25.4 32.8 32.5 42.7 19.6 28.9 28.6 43.4 35.1 32.\n 24.7 32.6 43.2 22.4 29.3 24.6 48.8 32.4 38.5 26.5 19.1 46.7 23.8 33.9\n 20.4 28.7 49.7 39.  26.1 22.5 39.6 29.5 34.3 37.4 33.3 31.2 28.2 53.2\n 34.2 26.8 55.  42.9 34.5 27.9 38.3 21.1 33.8 30.8 36.9 39.5 27.3 21.9\n 40.6 47.9 50.  25.2 40.9 37.2 44.2 29.9 31.9 28.4 43.5 32.7 67.1 45.\n 34.9 27.7 35.9 22.6 33.1 30.4 52.3 24.3 22.9 34.8 30.9 40.1 23.9 37.5\n 35.5 42.8 42.6 41.8 35.8 37.8 28.8 23.6 35.7 36.7 45.2 44.  46.2 35.\n 43.6 44.1 18.4 29.2 25.9 32.1 36.3 40.  25.1 27.5 45.6 27.8 24.9 25.3\n 37.9 27.  26.  38.7 20.8 36.1 30.7 32.3 52.9 21.  39.7 25.5 26.2 19.3\n 38.1 23.5 45.5 23.1 39.9 36.8 21.8 41.  42.2 34.4 27.2 36.5 29.8 39.2\n 38.4 36.2 48.3 20.  22.3 45.7 23.7 22.1 42.1 42.4 18.2 26.4 45.3 37.\n 24.5 32.2 59.4 21.2 26.7 30.2 46.1 41.3 38.8 35.2 42.3 40.7 46.5 33.5\n 37.3 30.3 26.3 21.7 36.4 28.5 26.9 38.6 31.3 19.5 20.1 40.8 23.4 28.3\n 38.9 57.3 35.6 49.6 44.6 24.1 44.5 41.2 49.3 46.3]\n\n Age\n\n[50 31 32 21 33 30 26 29 53 54 34 57 59 51 27 41 43 22 38 60 28 45 35 46\n 56 37 48 40 25 24 58 42 44 39 36 23 61 69 62 55 65 47 52 66 49 63 67 72\n 81 64 70 68]\n\n Outcome\n\n[1 0]\n\n\nAu vu de ces valeurs, on peut dire que (vu qu’il n’y a aucune description des disponible sur kaggle):\n\npregnancies represente le nombre de grossesses contractées;\nglucose represente la quantité de glucose dans le sang;\nBloodPressure represente la pression sanguine;\nSkinThickness represente l’épaisseur du pli cutané tricipital;\nBMI correspond à l’Indice de Masse Corporelle (IMC)\nAge de la patiente\nInsulin représente la concentration sérique d’insuline mesurée (généralement en micro-unités par millilitre (μU/ml))\nDiabetesPedigreeFunction représente une mesure de la prédisposition génétique au diabète\nOutcome represente l’état de la patiente (atteinte ou non du diabète)"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#analyse-exploratoire-des-données",
    "href": "FORMATIONS/logistic_regression_diabetes.html#analyse-exploratoire-des-données",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Analyse exploratoire des données",
    "text": "Analyse exploratoire des données\n      Cette analyse est effectuée dans l’optique de mieux comprendre les données afin de pouvoir bien spécifier le modèle logistique.\n\nAnalyse descriptives rapides (Voir la distribution des données)\n\n# Création de la figure avec une grille 3 lignes x 2 colonnes\nfig, axes = plt.subplots(3, 2, figsize=(12, 12))\n\n# Premier sous-graphe : Distribution du nombre de grossesses\nsns.histplot(data=df['Pregnancies'], ax=axes[0, 0])\naxes[0, 0].set_title(\"Distribution du nombre de grossesses\")\n\n# Deuxième sous-graphe : Distribution du niveau de glucose\nsns.histplot(data=df['Glucose'], ax=axes[0, 1])\naxes[0, 1].set_title(\"Distribution du niveau de glucose\")\n\n# Troisième sous-graphe : Distribution de la pression sanguine\nsns.histplot(data=df['BloodPressure'], ax=axes[1, 0])\naxes[1, 0].set_title(\"Distribution de la pression sanguine\")\n\n# Quatrième sous-graphe : Distribution de l'épaisseur du pli cutané (SkinThickness)\nsns.histplot(data=df['SkinThickness'], ax=axes[1, 1])\naxes[1, 1].set_title(\"Distribution de l'épaisseur du pli cutané\")\n\n# Cinquième sous-graphe : Distribution de l'insuline\nsns.histplot(data=df['Insulin'], ax=axes[2, 0])\naxes[2, 0].set_title(\"Distribution de l'insuline\")\n\n# Sixième sous-graphe : Distribution de l'IMC (BMI)\nsns.histplot(data=df['BMI'], ax=axes[2, 1])\naxes[2, 1].set_title(\"Distribution de l'IMC\")\n\n# Ajustement automatique des espaces pour\n# éviter le chevauchement des titres et labels\nplt.tight_layout()\n\n# Affichage de la figure\nplt.show()\n\n\n\n\n\n\nVerification de la colinéarité\n      En effet avant de spécifier un modèle, il faut s’assurer qu’il n’y a pas multicolinéarité. C’est-à-dire verifier que les variables ne sont pas corrélées entre elles ce qui permettra d’éviter de fausses estimations.\n\n# Sélectionner que les variables numériques des données\ndf_variables_numeriques = df.select_dtypes(include=[np.number])\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(df_variables_numeriques.corr(), annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Heatmap de correlation des variables numériques du jeu de données')\nplt.tight_layout()\nplt.show()\n\n\n\n\n      Ce corrélollogramme montre que les variables ne sont pas linéairement corrélées entre elle. Donc on peut ajuster le modèle de regression logistique."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#spécification-et-évalution-du-modèle-logistique",
    "href": "FORMATIONS/logistic_regression_diabetes.html#spécification-et-évalution-du-modèle-logistique",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Spécification et évalution du modèle logistique",
    "text": "Spécification et évalution du modèle logistique\n      A ce niveau, j’ai partitionné les données en ammont dans le but de faire du machine learning (ajustement, prediction et validation du modèle) plus tard (dans la section suivante). Nous avons les données d’entrainement qui constituent 80% des données et des données de test qui en constituent 20. Ici j’ajuste juste un modèle de regression logistique aux données que j’essaie d’interpreter.\n\n# Les bibliothèques de machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\nfrom sklearn.inspection import permutation_importance\n\n\n# Separation des variables explicatives and de la variable dépendante\n\n# X : matrice des variables explicatives\nX = df.drop('Outcome', axis=1)\n\n# y : variable dépendante\ny = df['Outcome']\n\n# partition des données en données de tests et d'entrainement\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nimport statsmodels.api as sm\n\n# Ajout d'une colonne de 1 pour l'intercept (obligatoire dans statsmodels)\nX_train_const = sm.add_constant(X_train)\n\n# Création du modèle logistique\nmodel = sm.Logit(y_train, X_train_const)\n\n# Ajustement du modèle\nresult = model.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.467835\n         Iterations 6\n\n# Affichage du résumé avec les p-values\ndisplay(result.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                Outcome   No. Observations:                  614\nModel:                          Logit   Df Residuals:                      605\nMethod:                           MLE   Df Model:                            8\nDate:                Sat, 28 Jun 2025   Pseudo R-squ.:                  0.2752\nTime:                        15:51:52   Log-Likelihood:                -287.25\nconverged:                       True   LL-Null:                       -396.34\nCovariance Type:            nonrobust   LLR p-value:                 9.311e-43\n============================================================================================\n                               coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nconst                       -9.0359      0.837    -10.802      0.000     -10.675      -7.396\nPregnancies                  0.0645      0.036      1.791      0.073      -0.006       0.135\nGlucose                      0.0341      0.004      8.055      0.000       0.026       0.042\nBloodPressure               -0.0139      0.006     -2.260      0.024      -0.026      -0.002\nSkinThickness                0.0031      0.008      0.397      0.691      -0.012       0.019\nInsulin                     -0.0018      0.001     -1.782      0.075      -0.004       0.000\nBMI                          0.1026      0.017      5.948      0.000       0.069       0.136\nDiabetesPedigreeFunction     0.6945      0.330      2.107      0.035       0.049       1.341\nAge                          0.0371      0.011      3.400      0.001       0.016       0.058\n============================================================================================"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#qualité-dajustement",
    "href": "FORMATIONS/logistic_regression_diabetes.html#qualité-dajustement",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Qualité d’Ajustement",
    "text": "Qualité d’Ajustement\n\nLog-Likelihood : -287.25. Un log-vraisemblance plus élevé (moins négatif) indique un meilleur ajustement.\nPseudo R-squared : 0.2752. Cela signifie que le modèle explique environ 27.52% de la variabilité dans les données, ce qui indique un ajustement modéré. Dans les modèles linéaires généralisés, il est fréquent d’avoir des pseudo-R2 un peu faible.\nLLR p-value : 9.311e-43, très faible, indiquant que le modèle est significatif globalement."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#adéquation-du-modèle",
    "href": "FORMATIONS/logistic_regression_diabetes.html#adéquation-du-modèle",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Adéquation du Modèle",
    "text": "Adéquation du Modèle\n\nConvergence : Le modèle a convergé en 6 itérations, suggérant un bon comportement de l’algorithme d’optimisation.\nDf Model : 8, indiquant 8 variables explicatives."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#interprétation-des-coefficients-1",
    "href": "FORMATIONS/logistic_regression_diabetes.html#interprétation-des-coefficients-1",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Interprétation des Coefficients",
    "text": "Interprétation des Coefficients\nLa probabilité \\(p\\) que \\(y = 1\\) (c’est-à-dire que la patiente ait le diabète) est donnée par la fonction sigmoïde :\n\\(p = \\frac{1}{1 + \\exp(-\\beta)}\\)\noù : - \\(\\beta\\) est le coefficient du modèle de régression logistique.\n\n\\(\\exp(-\\beta)\\) représente l’exponentielle de \\(-\\beta\\).\n\nAinsi, cette fonction transforme la valeur linéaire ( ) en une probabilité entre 0 et 1.\n\nIntercept (-9.0359) : Lorsque toutes les variables sont à 0, la probabilité prédite que y=1 est proche de 0.\nGlucose (0.0341, p&lt;0.001) : Une augmentation de 1 unité de glucose augmente significativement les odds de l’issue y=1.\nBMI (0.1026, p&lt;0.001) : Indique une relation positive forte entre l’IMC et l’issue.\nBloodPressure (-0.0139, p=0.024) : Relation négative significative, mais l’effet est faible.\nDiabetesPedigreeFunction (0.6945, p=0.035) : Un antécédent familial a un impact positif significatif.\nAge (0.0371, p=0.001) : L’âge est un facteur significatif.\nSkinThickness et Insulin : Effet non significatif (au seuil de risque \\(\\alpha\\) = 0,05)."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#conclusion-1",
    "href": "FORMATIONS/logistic_regression_diabetes.html#conclusion-1",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Conclusion",
    "text": "Conclusion\nLe modèle a une bonne capacité prédictive mais n’explique pas toute la variabilité. Certaines variables sont significatives (Glucose, BMI, Age), alors que d’autres, comme l’Insuline, ne le sont pas."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#machine-learning",
    "href": "FORMATIONS/logistic_regression_diabetes.html#machine-learning",
    "title": "Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Machine learning",
    "text": "Machine learning\n\nAjustement du modèle aux données d’apprentissage\n\n# Initialisation et entrainnement du classificateur (Regression Logistique)\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=1000)\n\n# faire les prediction sur les données de test\ny_pred = model.predict(X_test)\n\n# calcul du score de l'exactitude\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy Score: {accuracy:.4f}')\n\nAccuracy Score: 0.7468\n\n\n      L’accuracy score de 0.7468 signifie que le modèle a correctement classé 74.68% des échantillons dans le jeu de test. Cette métrique donne une indication de la proportion des prédictions correctes par rapport au nombre total d’observations. Plus l’accuracy est proche de 1 (ou 100%), plus le modèle est performant.\n\n\nEvaluation du modèle\n\n\nMatrice de confusion\n\n\n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Matrice de confusion')\nplt.xlabel('Données prédites')\nplt.ylabel('Données observées')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\\[\n\\textbf{Vrais Positifs (VP)} = 37 \\quad \\text{(Modèle prédit que la patiente a le diabète et c'est correct)}\n\\] \\[\n\\textbf{Faux Positifs (FP)} = 21 \\quad \\text{(Modèle prédit que la patiente a le diabète, mais c'est incorrect)}\n\\]\n\\[\n\\textbf{Faux Négatifs (FN)} = 18 \\quad \\text{(Modèle prédit que la patiente n'a pas le diabète, mais c'est incorrect)}\n\\]\n\\[\n\\textbf{Vrais Négatifs (VN)} = 78 \\quad \\text{(Modèle prédit que la patiente n'a pas le diabète et c'est correct)}\n\\]\n\n\n\nMétriques de performance\n\n\n\n\\[\n\\text{Précision} = \\frac{\\text{VP}}{\\text{VP} + \\text{FP}} = \\frac{37}{37 + 21} = \\frac{37}{58} \\approx 0.6379\n\\]\n\\[\n\\textbf{Rappel} (Recall) :\n\\text{Rappel} = \\frac{\\text{VP}}{\\text{VP} + \\text{FN}} = \\frac{37}{37 + 18} = \\frac{37}{55} \\approx 0.6727\n\\]\n\\[\n\\textbf{Score F1} (F1-Score) :\n\\text{F1-Score} = 2 \\times \\frac{\\text{Précision} \\times \\text{Rappel}}{\\text{Précision} + \\text{Rappel}} = 2 \\times \\frac{0.6379 \\times 0.6727}{0.6379 + 0.6727} \\approx 0.6548\n\\]\n\\[\n\\textbf{Exactitude} (Accuracy) :\n\\text{Exactitude} = \\frac{\\text{VP} + \\text{VN}}{\\text{Total}} = \\frac{37 + 78}{37 + 78 + 21 + 18} = \\frac{115}{154} \\approx 0.7468\n\\]\n\n\nCourbe de ROC\n\n\n\ny_prob = model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6,4))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Courbe ROC (Aire = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('Taux de Faux Positifs')\nplt.ylabel('Taux de Vrais Positifs')\nplt.title('Caractéristique de Performance du Modèle (Courbe ROC)')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n      La courbe ROC (Receiver Operating Characteristic) est un graphique qui permet d’évaluer la performance d’un modèle de classification binaire. Elle trace la relation entre :\n\nLe Taux de Vrais Positifs (TPR, True Positive Rate) : La proportion des vrais positifs parmi les cas positifs réels.\nLe Taux de Faux Positifs (FPR, False Positive Rate) : La proportion des faux positifs parmi les cas négatifs réels.\n\n      La courbe ROC montre comment le modèle se comporte pour différents seuils de décision. Un modèle parfait aura une courbe qui monte rapidement vers le coin supérieur gauche (haute TPR et faible FPR), tandis qu’un modèle aléatoire suivra la diagonale du graphique (FPR = TPR).\n      L’Aire Sous la Courbe (AUC) mesure la qualité globale du modèle. Une AUC proche de 1 indique un excellent modèle, tandis qu’une AUC proche de 0.5 indique un modèle équivalent à un choix aléatoire.\nDans notre cas AUC vaut 0,81 donc notre modèle tient la route.\n\n\n\nVerifions qu’on a les même coefficients que ceux de l’ajustement à la section précédente\n\n\n\n\nmodel.intercept_\n\narray([-9.00707993])\n\n\n\n# affichage des coefficients estimés du modèle\nmodel.coef_\n\narray([[ 0.06436473,  0.03410147, -0.01387533,  0.00326297, -0.00180169,\n         0.10262329,  0.62588811,  0.03708342]])\n\n\nEt oui on a les mêmes coefficients.\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour à la page d’accueuil"
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport math\nplt.style.use('deeplearning.mplstyle')"
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#résumé",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#résumé",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Résumé",
    "text": "Résumé\n      À la suite de mes cours d’optimisation et de calcul numérique, j’ai souhaité implémenter moi-même l’algorithme de la descente de gradient. J’ai choisi comme premier cas d’application la régression linéaire, dans le but d’estimer les paramètres optimaux (w, b) qui minimisent la fonction de coût (généralement l’erreur quadratique moyenne).\nMême si, dans le cas de la régression linéaire, il existe une solution analytique explicite par la méthode des moindres carrés, cette situation est idéale pour comprendre et tester l’efficacité de la descente de gradient. En revanche, pour des modèles plus complexes comme la régression logistique, une solution analytique n’est plus disponible. Dans ces cas, on utilise systématiquement des méthodes numériques comme la descente de gradient. En pratique, il faut s’assurer que les données soient réparties de manière équilibrée selon des critères comme le sexe ou l’éducation entre les jeux d’apprentissage, de validation (avec la validation croisée) et de test, pour éviter que l’évaluation du modèle soit faussée."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#abstract",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#abstract",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Abstract",
    "text": "Abstract\n      Following my courses in optimisation and numerical computation, I wanted to implement the gradient descent algorithm myself. My first application was to linear regression, with the aim of estimating the optimal parameters (w, b) that minimise the cost function (generally the mean square error).\nEven though, in the case of linear regression, there is an explicit analytical solution using the method of least squares, this situation is ideal for understanding and testing the effectiveness of gradient descent. However, for more complex models such as logistic regression, an analytical solution is no longer available. In these cases, numerical methods such as gradient descent are systematically used. In practice, care must be taken to ensure that the data is distributed evenly according to criteria such as gender or education between the training, validation (with cross-validation) and test sets, to avoid distorting the evaluation of the model."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#introduction",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#introduction",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Introduction",
    "text": "Introduction\n      L’objectif de ce document est de présenter l’implémentation de l’algorithme de descente de gradient dans le cas de la régression linéaire multiple, afin de mieux comprendre le principe d’optimisation itérative. On commence par un rappel de la régression linéaire classique, avant de dériver la fonction de coût et ses gradients par rapport aux paramètres. Ensuite, l’algorithme est appliqué à un jeu de données simulé pour illustrer sa convergence et les effets du taux d’apprentissage.\nDans une deuxième partie, nous discuterons des limites de l’approche analytique, notamment dans des contextes où la descente de gradient devient incontournable (régression logistique, réseaux de neurones, etc.)."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#un-peu-de-formalisme",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#un-peu-de-formalisme",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Un peu de formalisme",
    "text": "Un peu de formalisme\n      Nous souhaitons minimiser une fonction objective, appelée également fonction de coût. Avant de chercher une méthode numérique pour effectuer cette minimisation, il est essentiel de s’assurer que la fonction admet bien un minimum, et que celui-ci est unique.\nEn effet, une fonction peut présenter plusieurs minima locaux, et l’objectif est généralement d’atteindre le minimum global. Pour garantir l’existence d’un minimum global, on fait appel au théorème de Weierstrass, qui stipule qu’une fonction continue sur un ensemble compact atteint un minimum (et un maximum).\nMais pour garantir l’unicité du minimum, on utilise la notion de convexité. Une fonction strictement convexe sur un domaine convexe possède un unique minimum qui est donc le minimum global.\n\n💡 Si votre fonction de coût est concave (au lieu d’être convexe), il suffit d’en prendre l’opposé. La maximisation d’une fonction concave revient à minimiser son opposée, qui sera convexe.\n\nAinsi, la stricte convexité de la fonction objective est une propriété cruciale : elle permet de garantir l’unicité du minimum et donc la convergence de l’algorithme vers une solution bien définie.\nJe n’entrerai pas dans trop de details mathématiques. Si vous voulez en savoir plus (condition d’application du théorème de weierstrass -&gt; sémi-continuité + espace de contraintes borné ou semi-continuité + coercivité + espace de contraintes fermé) consultez cette page : &lt;https://en.wikipedia.org/wiki/Extreme_value_theorem&gt; et faites des recherches supplémentaires."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#algorithme-de-la-descente-de-gradient-cas-de-la-regression-linéaire",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#algorithme-de-la-descente-de-gradient-cas-de-la-regression-linéaire",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Algorithme de la descente de gradient : cas de la regression linéaire",
    "text": "Algorithme de la descente de gradient : cas de la regression linéaire\n\nFonction de coût de la regression linéaire\n\nLa fonction de coût utilisée pour la régression linéaire est la (ou Mean Squared Error, MSE). Elle s’écrit comme suit :\n\\[\n\\begin{equation}\n    J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\n\\end{equation}\n\\]\noù :\nEn notation vectorielle, on peut réécrire la fonction de coût sous la forme :\n\\[\n\\begin{equation}\n    J(w, b) = \\frac{1}{2m} (\\hat{Y} - Y)^T (\\hat{Y} - Y)\n\\end{equation}\n\\]\noù :\n\\[\n\\begin{equation}\n    J(w, b) = \\frac{1}{2m} \\left\\| \\hat{Y} - Y \\right\\|^2\n\\end{equation}\n\\]\n\n🔻 Descente de Gradient\n\nL’objectif de la descente de gradient est de déterminer les paramètres optimaux \\(w\\) et \\(b\\) qui minimisent la fonction de coût. Pour cela, on utilise un algorithme itératif qui met progressivement à jour ces paramètres dans le sens opposé au gradient.\nÀ chaque itération, les nouvelles valeurs de \\(w\\) et \\(b\\) doivent idéalement conduire à une diminution de la fonction de coût. Si la fonction augmente, cela peut être dû à :\n\nun taux d’apprentissage (\\(\\alpha\\)) trop élevé, provoquant une divergence ;\nune erreur de code, par exemple un mauvais calcul du gradient.\n\nEn revanche, si la fonction de coût diminue trop lentement, cela peut signifier :\n\nun taux d’apprentissage trop faible, causant une convergence lente ou incomplète.\n\n\n⚠️ Dans certains cas, la fonction de coût peut onduler à chaque itération (oscillations), souvent à cause d’une mauvaise normalisation ou d’un \\(\\alpha\\) mal ajusté."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#vérification-de-la-convergence",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#vérification-de-la-convergence",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "🔎 Vérification de la convergence",
    "text": "🔎 Vérification de la convergence\nPour s’assurer que l’algorithme converge correctement, on peut :\n\nTracer graphiquement la valeur de la fonction de coût à chaque itération.\nFixer un seuil de tolérance :\n\n\\[\n\\text{Si } \\|\\nabla J(w, b)\\| &lt; \\varepsilon, \\text{ alors on arrête l'algorithme.}\n\\]"
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#mise-à-jour-des-paramètres",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#mise-à-jour-des-paramètres",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "⚙️ Mise à jour des paramètres",
    "text": "⚙️ Mise à jour des paramètres\nLa descente de gradient met à jour simultanément les poids et le biais selon la règle :\n\\[\nw := w - \\alpha \\cdot \\frac{\\partial J(w, b)}{\\partial w}\n\\]\n\\[\nb := b - \\alpha \\cdot \\frac{\\partial J(w, b)}{\\partial b}\n\\]\noù :\n\n\\(J(w, b)\\) est la fonction de coût,\n\\(\\alpha\\) est le learning rate."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#remarques-pratiques",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#remarques-pratiques",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "🧪 Remarques pratiques",
    "text": "🧪 Remarques pratiques\n\nUn taux d’apprentissage dynamique (adaptatif) peut améliorer la convergence.\nDes techniques comme momentum, RMSprop ou Adam sont des variantes plus stables.\n\n\n\n\n\n\nFigure 1: Learning example\n\n\n\n\n\n\n\n\n\n\nDescente de Gradient et Régularisation\n\n\n\nL’algorithme de descente de gradient est une méthode d’optimisation utilisée pour ajuster les paramètres d’un modèle en minimisant une fonction de coût. Il repose sur le calcul du gradient (ou pente) de cette fonction par rapport aux paramètres, et sur une mise à jour itérative jusqu’à convergence.\nCependant, en pratique, on rencontre souvent le problème de surapprentissage (overfitting), notamment lorsque :\n\nLe nombre de variables est élevé par rapport au nombre d’observations.\nCertaines variables explicatives n’apportent que peu ou pas d’information utile.\nLe modèle devient trop complexe, capturant le bruit au lieu du signal.\n\nPour pallier cela, plusieurs stratégies existent :\n\n🔄 Augmenter la taille du jeu de données : plus de données permet de mieux généraliser.\n🧠 Sélectionner judicieusement les variables : par des techniques comme le feature selection, on garde seulement les plus pertinentes.\n🛡️ Appliquer une régularisation (comme Lasso ou Ridge) : on pénalise la complexité du modèle pour éviter l’ajustement excessif.\n\n👉 Ces aspects seront peut-être explorés plus en détail dans une prochaine publication à travers un modèle de régression logistique appliqué à des données réelles, où la sélection de variables et la régularisation joueront un rôle central.\n\n\n\n\n\n\n\n\nLa Régression Polynomiale\n\n\n\nLa régression polynomiale est une extension de la régression linéaire où l’on introduit des puissances supplémentaires des variables explicatives pour capturer des relations non linéaires entre les variables.\nExemple :\n\\[\ny = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_d x^d + \\varepsilon\n\\]\nCela permet au modèle de s’adapter à des courbes complexes, mais augmente également le risque de surapprentissage. Plus le degré ( d ) est élevé, plus le modèle est flexible, mais moins il généralise bien si les données ne sont pas suffisantes.\n➡️ Il est donc essentiel de combiner cette approche avec des techniques de validation croisée et de régularisation, pour trouver le bon compromis entre biais et variance."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#applications",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#applications",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Applications",
    "text": "Applications\n      Dans cette section, nous coommençons d’abord par simuler un jeu de données synthétiques représentant des individus caractérisés par leur âge, leur niveau d’éducation, leur sexe, ainsi que leur salaire annuel. Le salaire est généré à partir d’un modèle probabiliste basé sur des salaires de base associés à chaque niveau d’éducation, auxquels s’ajoute un effet linéaire de l’âge, et une part d’aléa simulée à l’aide d’une distribution normale.\n\n\n# generating data\n# fixing generator seed\nnp.random.seed(42)\n\n# number of observation\nsize = 1000\n\n# creating age variable\nages = np.random.randint(low=21, high=66, size=size)\n\n# creating education level variable\nlevels = ['BAC', 'Bachelor degree', 'Msc', 'PhD']\neduc_level = np.random.choice(levels, size=size, p=[0.1, 0.2, 0.3, 0.4])\n\n# creating sexe variable\nsexes = ['M', 'F']\nsexe = np.random.choice(sexes, size=size)\n\n\n# creating annual salary (Euro) based on the different informations above\nbase_salary = {\n    'BAC': 25000,\n    'Bachelor degree': 30000,\n    'Msc': 45000,\n    'PhD': 75000\n}\n\nsalary = np.asarray([\n  round(np.random.normal(base_salary[edu_lvl] + (age*2), 100)) \n  for edu_lvl, age in zip(educ_level, ages)\n])\n\ndf = pd.DataFrame(\n    {\n        'salary': salary,\n        'ages': ages,\n        'sex': sexe,\n        'education': educ_level\n    }\n)\n\n\nAlgorithme du calcul de la fonction de coût\n\n\n\ndef compute_cost_fn(x:np.ndarray, y:np.ndarray, w:np.array, b:float) -&gt; float:\n    \"\"\"\n        Calculates the cost function (MSE divided by 2)\n        for multivariate linear regression.\n\n        Args:\n            x (np.ndarray): Matrix of explanatory variables (shape: [m, n])\n            y (np.ndarray): Vector of target values (shape: [m,])\n            w (np.ndarray): Weight vector (shape: [n,])\n            b (float): Bias (scalar)\n\n        Returns:\n            float: Value of the cost function\n    \"\"\"\n    m = x.shape[0]\n    y_hat = x@w + b\n    squared_error = np.sum((y - y_hat)**2)\n    cost_fn = squared_error/(2*m)\n    return cost_fn\n\n\nAlgorithme du calcul du gradient de la fonction de coût\n\n\ndef compute_gradient(x:np.ndarray, y:np.ndarray, w:np.array, b:float) -&gt; tuple:\n    \"\"\"\n    Calculates the gradient of the cost function (MSE) with respect to weights w and bias b.\n\n    Args:\n        x (np.ndarray): Matrix of input features of form (m, n),\n        where m is the number of observations\n                        and n is the number of explanatory variables.\n        y (np.ndarray): Vector of target values (m, ).\n        w (np.ndarray): Weight vector (n, ).\n        b (float): Scalar bias.\n\n    Returns:\n        tuple: A pair (fw_prime, fb_prime) containing :\n            - fw_prime (np.ndarray): The gradient of the cost function with respect to w, \n            of the form (n, ).\n            - fb_prime (float): The gradient of the cost function with respect to b.\n    \"\"\"\n    m = x.shape[0]\n    y_hat = x @ w + b\n    error = y_hat - y\n    fw_prime = (x.T @ error)/m\n    fb_prime = np.sum(error)/m\n    return fw_prime, fb_prime\n\n\nAlgorithme de la descente de gradient\n\n\nfrom typing import Callable\nfrom copy import deepcopy\ndef gradient_descent(\n    x: np.ndarray,\n    y: np.ndarray,\n    alpha: float,\n    w_in: np.array,\n    b_in: float,\n    max_iter: int,\n    tolerance: float,\n    cost_fn: Callable,\n    gradient_compute_fn: Callable) -&gt; dict:\n    \"\"\"\n    Performs gradient descent to adjust\n    the parameters w and b.\n\n    Args:\n        x (np.ndarray): Input data (m, n)\n        y (np.ndarray): Target (m,)\n        w_in (np.ndarray): Initial weight (n,)\n        b_in (float): Initial bias\n        alpha (float): Learning rate\n        max_iter (int): Maximum number of iterations\n        tolerance (float): Convergence threshold\n        cost_fn (Callable): Cost function\n        gradient_compute_fn (Callable): Gradient calculation function\n\n    Returns:\n        dict: History of parameters and cost at each iteration\n    \"\"\"\n\n    w = deepcopy(w_in)\n    b = b_in\n    cost_fn_and_params_hist = {}\n    for i in range(max_iter):\n        fw_i, fb_i = gradient_compute_fn(x, y, w, b)\n        w = w - alpha*fw_i\n        b = b - alpha*fb_i\n        cost_fn_i = cost_fn(x, y, w, b)\n        cost_fn_and_params_hist[i] = {\n            'w': w.copy(),\n            'b': b,\n            'cost_fn': cost_fn_i\n        }\n        if i % 15000 == 0: # you can adjust: I don't want to display all the iterations\n            print(\n                f'Iteraion {i} : \\n|(w, b) = ({w}, {b:4f}) | cost_fn = {cost_fn_i:4f}|'\n            )\n        if np.linalg.norm(fw_i) &lt; tolerance and abs(fb_i) &lt; tolerance:\n            print(\n                f'The algorithm converges at the {i}-th iteration'\n                f'\\nThen we have (w, b) = ({w}, {b:4f}) & cost_fn = {cost_fn_i:4f}'\n                )\n            break\n    return cost_fn_and_params_hist"
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#analyse-exploratoire-relation-entre-salaire-et-variables-explicatives",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#analyse-exploratoire-relation-entre-salaire-et-variables-explicatives",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "📊 Analyse exploratoire : Relation entre salaire et variables explicatives",
    "text": "📊 Analyse exploratoire : Relation entre salaire et variables explicatives\nIci on veut predire le salaire d’un individu en fonction de son niveau d’éducation, de son âge et de son sexe.\n\nPremières lignes de la table\n\n\ndf_5 = df.loc[:5, :]\n\n\ndf_5_r = py$df_5\ndisplay_table(df_5_r, \"\", nrow_ = 5)\n\n\n\nTable 1: The first lines of our database\n\n\nsalary\nages\nsex\neducation\n\n\n\n\n75213\n59\nF\nPhD\n\n\n45269\n49\nF\nMsc\n\n\n45060\n35\nM\nMsc\n\n\n75109\n63\nF\nPhD\n\n\n45063\n28\nM\nMsc\n\n\n\n\n\n\n\nRésumé statistique de la table\n\n\ndf.describe() # displays the statistical summary of numerical variables\n\n             salary         ages\ncount   1000.000000  1000.000000\nmean   52498.163000    43.000000\nstd    20121.852889    12.945562\nmin    24882.000000    21.000000\n25%    30139.000000    32.000000\n50%    45133.000000    44.000000\n75%    75065.000000    54.000000\nmax    75441.000000    65.000000\n\n\n\nDistribution du salaire en fonction des features\n\n\nX_features = df.drop('salary', axis=1).columns.to_list()\nX_features\n\n['ages', 'sex', 'education']\n\n\n\nfig, axes = plt.subplots(1, 3, figsize = (12, 8))\nfor i in range(len(axes)):\n    axes[i].scatter(y=df['salary'], x=df[df.columns.to_list()[i + 1]])\n    axes[i].set_xlabel(X_features[i].capitalize(), fontsize=10)\n    axes[i].set_ylabel('Salary', size=10)\n    axes[i].set_xlabel(X_features[i].capitalize(), size=10)\n    axes[i].tick_params(axis='x', rotation=45)\n    axes[i].set_title(f'Relation between salary and {X_features[i].capitalize()}', size=10)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure 2: Distribution of variables as a function of salary\n\n\n\n\n\n👴 Relation entre le salaire et l’âge\n\n🔍 Observation :\n\nLes salaires sont regroupés autour de quelques valeurs fixes (25k, 30k, 45k, 75k), indépendamment de l’âge.\nLa distribution semble en paliers, pas de tendance linéaire visible.\n\n💡 Interprétation :\n\nIl n’existe pas de relation linéaire claire entre l’âge et le salaire.\nL’âge n’est pas un facteur explicatif majeur du salaire dans cet échantillon.\n👉 Il faudrait explorer d’autres variables comme l’éducation, ect..\n\n\n\n\n\n🚻 Relation entre le salaire et le sexe\n\n🔍 Observation :\n\nLes salaires des femmes (F) et des hommes (M) sont répartis de manière similaire.\nAucune différence salariale flagrante n’apparaît sur ce graphique.\n\n💡 Interprétation :\n\nLe sexe ne semble pas avoir d’influence directe sur le salaire ici.\n📌 Une analyse plus fine (ex. tests statistiques) serait nécessaire pour confirmer l’absence d’écart significatif (mais ce n’est pas le but ici, Cliquez ici pour voir une publication dans laquelle je réalise des tests statistiques afin de selectionner les variables essentielles pour la spécification de mon modèle).\n\n\n\n\n\n🎓 Relation entre le salaire et le niveau d’éducation\n\n🔍 Observation :\n\nLe salaire augmente avec le niveau de diplôme :\n\nPhD 🧑🔬 &gt; MSc 👨🎓 &gt; Bachelor 👨🏫 &gt; BAC 🎒\n\nCette progression est claire et ordonnée.\n\n💡 Interprétation :\n\nLe niveau d’éducation est un facteur prédictif fort du salaire.\nIl existe une relation croissante et logique entre diplôme obtenu et rémunération.\n\n\n\n\n\n✅ Conclusion de l’analyse exploratoire rapide\n\n🧠 De prime abord, on pourrait penser qu’il existe une relation non linéaire entre les variables explicatives et le salaire.\nEn réalité, seule la variable 🎓 niveau d’éducation montre une relation cohérente et croissante.\nNi 👴 l’âge, ni 🚻 le sexe ne présentent d’influence claire sur le salaire.\nCette analyse souligne l’importance d’une exploration visuelle et statistique avant toute modélisation."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#data-preprocessing",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#data-preprocessing",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\n\n\n\n\n\nCode pour partitionner les données\n\n\n\nLa fonction suivante permet de diviser vos données en ensembles d’entraînement et de test pour la validation du modèle. Il est important de s’assurer que les deux ensembles contiennent toutes les catégories ou modalités des variables catégorielles (indice : stratification). La fonction ci-dessous ne prend pas cela en charge automatiquement. Cependant, grâce à la taille de notre jeu de données et à l’utilisation de la graine aléatoire fixée à 42, nous pouvons garantir que ces caractéristiques sont bien présentes dans les deux ensembles.\n\n\n&gt;&gt;&gt; train['education'].unique()\narray(['Bachelor degree', 'PhD', 'Msc', 'BAC'], dtype=object)\n&gt;&gt;&gt; test['education'].unique()\narray(['Msc', 'PhD', 'BAC', 'Bachelor degree'], dtype=object)\n&gt;&gt;&gt;\n\ndef create_data_partition(df, train_ratio):\n    \"\"\"\n        Creates a random partition of the DataFrame into training and test sets.\n\n        Args:\n            df (pd.DataFrame): the complete DataFrame\n            train_ratio (float): the proportion of rows to be used for training (ex: 0.8)\n\n        Returns:\n            tuple: (train_df, test_df)\n    \"\"\"\n    nrow_df = df.shape[0]\n    nrow_train = round(nrow_df*train_ratio)\n    train_idx = np.random.choice(df.index, size=nrow_train, replace=False)\n    test_idx = df.index.difference(train_idx)\n    train_df = df.iloc[train_idx]\n    test_df = df.iloc[test_idx]\n\n    return (train_df, test_df)\n\n      Une fois les données simulées, nous procédons à leur séparation en deux sous-ensembles distincts : un ensemble d’entraînement, utilisé pour ajuster le modèle, et un ensemble de test, destiné à l’évaluer. La fonction create_data_partition réalise une partition aléatoire selon un ratio défini (ici 80% pour l’entraînement, 20% pour le test). Ensuite, les variables explicatives (ages, education, sex) sont traitées via une pipeline de prétraitement : les variables numériques sont standardisées (centrées-réduites) tandis que les variables catégorielles sont transformées en indicatrices (encodage one-hot, sans la première modalité). Enfin, la variable cible (salary) est également standardisée pour assurer une convergence efficace de l’algorithme de descente de gradient.\n\n# data preprocessing\n\n# geting train and test datasets\nnp.random.seed(42)\ntrain, test = create_data_partition(df, 0.8)\n\n\n# selecting features\nX = train.drop('salary', axis=1)\n\n# selecting target variable\ny = train['salary']\n\n# standardization of the target variable\nscaler_y = StandardScaler()\nonehot_encoder_educ = OneHotEncoder()\ny_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n\n# one-hot encodings & standardization : pipeline treatment\nnumeric_features = ['ages']\ncategorical_features = ['education', 'sex']\n\npreprocessor = ColumnTransformer(transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n])\n\n# transform numerical values by standardize them and categorical one by dummy them\nX_processed = preprocessor.fit_transform(X=X)\n\n\n\n\n\n\n\n⚠️ Important : fit_transform() vs transform()\n\n\n\nToujours utiliser fit_transform() uniquement sur les données d’entraînement, et transform() sur les données de validation ou de test.\nCela s’applique à tous les types de preprocessing, notamment :\n\n🟦 StandardScaler : la moyenne et l’écart-type doivent être appris sur le train uniquement.\n🟧 OneHotEncoder : les catégories doivent être identifiées à partir du train et appliquées de manière cohérente au test.\n\n❌ Ne jamais faire fit_transform() sur le test, car cela introduit du data leakage (les données de test influencent le modèle)."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#application-de-lalgorithme-de-la-descente-de-gradient",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#application-de-lalgorithme-de-la-descente-de-gradient",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Application de l’algorithme de la descente de gradient",
    "text": "Application de l’algorithme de la descente de gradient\n      Comme mentionné précédemment, le choix du learning rate (ou taux d’apprentissage) est un paramètre crucial dans l’algorithme de descente de gradient. Un taux trop élevé peut empêcher la convergence du modèle, tandis qu’un taux trop faible peut rendre l’apprentissage extrêmement lent. Afin d’identifier un taux optimal, plusieurs valeurs sont testées, et celle qui permet de minimiser au mieux la fonction de coût est retenue.\n\nalphas = [1e-1, 1e-2] # , 1e-3, 1e-4, 1e-5 à ajouter si vous voulez, je veux juste reduire l'affichage\nn_features = X_processed.shape[1]\nw = np.zeros(n_features)\nb = np.random.randint(100, size=1)[0]\nb = int(b)\ntolerance = 1e-3\nmax_iters = 50000\ncost_fn = compute_cost_fn\ngradient_fn = compute_gradient\nall_histories = {}\nfor alpha in alphas:\n    print(f'Runing the algorithm for alpha : {alpha}\\n')\n    w = np.zeros(n_features)\n    b = int(np.random.randint(100, size=1)[0])\n    history = gradient_descent(\n        X_processed,\n        y_scaled,\n        alpha=alpha,\n        w_in=w,\n        b_in=b,\n        max_iter=max_iters,\n        tolerance=tolerance,\n        cost_fn=cost_fn,\n        gradient_compute_fn=gradient_fn)\n    print('\\n')\n    all_histories[str(alpha)] = history\n\nRuning the algorithm for alpha : 0.1\n\nIteraion 0 : \n|(w, b) = ([-0.00367445 -0.04214402 -0.03787291  0.00503207 -0.04740491], 0.900000) | cost_fn = 0.855335|\nThe algorithm converges at the 1687-th iteration\nThen we have (w, b) = ([ 1.30308246e-03  2.22807855e-01  9.66238813e-01  2.45274791e+00\n -5.24625714e-04], -1.332871) & cost_fn = 0.000037\n\n\nRuning the algorithm for alpha : 0.01\n\nIteraion 0 : \n|(w, b) = ([-0.00036745 -0.0262144  -0.03431229 -0.04528429 -0.05781549], 11.880000) | cost_fn = 70.351204|\nIteraion 15000 : \n|(w, b) = ([ 2.13935724e-03  1.41295729e-01  8.87862367e-01  2.37691272e+00\n -1.16515913e-03], -1.261175) | cost_fn = 0.000502|\nThe algorithm converges at the 20966-th iteration\nThen we have (w, b) = ([ 1.30383689e-03  2.22734320e-01  9.66168107e-01  2.45267950e+00\n -5.25203562e-04], -1.332807) & cost_fn = 0.000037\n\n\n      Ici, on constate directement que notre algorithme a convergé (critère de tolérance) pour chacun des taux d’apprentissage (learning rate) testés.\nAprès cet entraînement, il est intéressant de déterminer quel learning rate a permis d’obtenir le meilleur résultat. Le code suivant est particulièrement utile lorsqu’on teste plusieurs valeurs de learning rate (plus de deux).\n\nbest_alpha = None\nmin_cost = float('inf')\n\nfor alpha, hist in all_histories.items():\n    costs = [v['cost_fn'] for v in hist.values()]\n    min_cost_alpha = min(costs)\n    print(f\"Alpha {alpha} ➤ Min cost: {min_cost_alpha:.5f}\")\n    if min_cost_alpha &lt; min_cost:\n        min_cost = min_cost_alpha\n        best_alpha = alpha\n\nAlpha 0.1 ➤ Min cost: 0.00004\nAlpha 0.01 ➤ Min cost: 0.00004\n\nprint(f\"\\n✅ Best alpha: {best_alpha} with min cost: {min_cost:.2f}\")\n\n\n✅ Best alpha: 0.1 with min cost: 0.00\n\n\nAvec ces deux learning rates, la fonction de coût atteint 0, ce qui signifie que l’algorithme a bien convergé puisque la fonction de coût est toujours positive.\n\ndef get_best_w_b(cost_fn_and_params_hist: dict):\n    hist_len = len(cost_fn_and_params_hist)\n    best_of_hist = cost_fn_and_params_hist[hist_len-1]\n    return best_of_hist\n  \ndef predict(w, b, X):\n    return X @ w + b\n  \nhist = all_histories['0.01']\nbest = get_best_w_b(hist)\n\n      On peut maintenant calculer nos prédictions et vérifier leur qualité par rapport aux données réelles. Rappelons que les données ont été standardisées, il faudra donc ramener les prédictions à leur échelle d’origine pour une interprétation correcte.\n\n# standardised predictions\nypred_scaled = predict(best['w'], best['b'], X_processed).reshape((-1, 1))\nprint(f'Standardised data (first 4 values): {ypred_scaled[:4]}\\n\\n')\n\nStandardised data (first 4 values): [[-1.11148687]\n [ 1.12097355]\n [ 1.11835771]\n [-1.1107826 ]]\n\n# normal scale\ny_pred = scaler_y.inverse_transform(ypred_scaled)\nprint(f'Data on normal scale (first 4 values): {y_pred[:4]}\\n\\n')\n\nData on normal scale (first 4 values): [[30000.2280983 ]\n [75092.97779228]\n [75040.14127095]\n [30014.45331559]]\n\n# display of optimal parameters\nprint(f'Optimum parameters : {best}')\n\nOptimum parameters : {'w': array([ 1.30383689e-03,  2.22734320e-01,  9.66168107e-01,  2.45267950e+00,\n       -5.25203562e-04]), 'b': -1.332806620067705, 'cost_fn': 3.722342588474543e-05}\n\n\nL’erreur quadratique moyenne sur les données d’apprentissage est donc 0.00610.\n\nfig, axes = plt.subplots(1, 3, figsize = (12, 8))\nfor i in range(len(axes)):\n    axes[i].scatter(train[X_features[i]], train['salary'], label='target')\n    axes[i].scatter(train[X_features[i]], y_pred, c='orange', label='predicted')\n    axes[i].set_ylabel('Salary', size=10)\n    axes[i].set_xlabel(X_features[i].capitalize(), size=10)\n    axes[i].tick_params(axis='x', rotation=45)\n    axes[i].set_title(f'Relation between salary and {X_features[i].capitalize()}', size=10)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure 3: Predcitions vs Real data\n\n\n\n\n      On peut voir que l’ajustement est très satisfaisant, mais il reste à déterminer s’il ne s’agit pas d’un surapprentissage. Pour cela nous allons utiliser les données de test, pour évaluer le modèle entrainé."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#evaluation-du-modèle",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#evaluation-du-modèle",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Evaluation du modèle",
    "text": "Evaluation du modèle\n      Pour évaluer le modèle nous utilisons la validation croisée car elle est robuste en terme d’évaluation de performance d’un modèle.\n\nBut principal : Évaluer la performance d’un modèle de manière fiable et robuste, en réduisant le biais dû à une simple séparation train/test.\nÉtape 1 : Partition des données\nDiviser l’ensemble des données en k sous-ensembles (ou « folds ») de taille à peu près égale.\nÉtape 2 : Boucle sur les folds\nPour chaque fold (de 1 à k) :\n\nUtiliser ce fold comme ensemble de test (validation).\n\nUtiliser les k-1 autres folds comme ensemble d’entraînement.\n\nÉtape 3 : Entraînement\nEntraîner le modèle uniquement sur les données d’entraînement (les k-1 folds).\nÉtape 4 : Évaluation\nTester le modèle entraîné sur le fold de test (le fold laissé de côté), calculer une métrique de performance (ex : erreur quadratique moyenne).\nÉtape 5 : Agrégation\nRépéter les étapes 2 à 4 pour chaque fold, puis calculer la moyenne (et éventuellement l’écart-type) des performances obtenues sur chaque fold.\nAvantages :\n\nMeilleure estimation de la généralisation du modèle sur des données nouvelles.\n\nRéduit le sur-apprentissage lié à un seul découpage train/test.\n\nUtilise efficacement toutes les données pour entraînement et validation.\n\nInconvénients :\n\nCoût computationnel plus élevé, car le modèle est entraîné k fois.\n\nPeut être sensible au choix de k (souvent 5 ou 10).\n\n\n\n\n\n\n\nFigure 4: Illustration of a cross-validation process\n\n\n\n\n\nfrom sklearn.metrics import r2_score\n\ndef k_fold_cross_validation(df, k, alpha, max_iters, tolerance, cost_fn, gradient_fn):\n    \"\"\"\n    Perform k-fold cross-validation using gradient descent.\n\n    Args:\n        df (pd.DataFrame): full dataset (training set)\n        k (int): number of folds\n        alpha (float): learning rate\n        max_iters (int): maximum number of iterations for gradient descent\n        tolerance (float): convergence threshold\n        cost_fn (Callable): cost function\n        gradient_fn (Callable): gradient computation function\n\n    Returns:\n        dict: dictionnaire avec listes des mse_train, mse_test, r2_train, r2_test\n    \"\"\"\n    # Shuffle the DataFrame index\n    df = df.sample(frac=1).reset_index(drop=True)\n    fold_size = len(df) // k\n    \n    mse_train_list = []\n    mse_test_list = []\n    r2_train_list = []\n    r2_test_list = []\n    \n    for fold in range(k):\n        # Define start and end indices for the test fold\n        start = fold * fold_size\n        end = (fold + 1) * fold_size if fold != k - 1 else len(df)\n        \n        # Split the data into test and train folds\n        test_df = df.iloc[start:end]\n        train_df = pd.concat([df.iloc[:start], df.iloc[end:]], axis=0)\n        \n        # Prepare features and target for train and test\n        X_train = train_df.drop('salary', axis=1)\n        y_train = train_df['salary']\n        X_test = test_df.drop('salary', axis=1)\n        y_test = test_df['salary']\n        \n        print('-'*10)\n        print(f\"\\nFold {fold+1}:\")\n        print(\"Train unique education:\", train_df['education'].unique())\n        print(\"Test unique education:\", test_df['education'].unique())\n        print(\"Train unique sex:\", train_df['sex'].unique())\n        print(\"Test unique sex:\", test_df['sex'].unique())\n        print()\n        \n        numeric_features = ['ages']\n        categorical_features = ['education', 'sex']\n\n        preprocessor = ColumnTransformer(transformers=[\n            ('num', StandardScaler(), numeric_features),\n            ('cat', OneHotEncoder(drop='first'), categorical_features)\n        ])\n        \n        # Preprocess the data using our pipeline\n        X_train_processed = preprocessor.fit_transform(X_train)\n        y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n        \n        X_test_processed = preprocessor.transform(X_test)\n        y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n        \n        # Initialize parameters\n        n_features = X_train_processed.shape[1]\n        w = np.zeros(n_features)\n        b = 0\n        \n        # Run gradient descent on training data\n        history = gradient_descent(\n            X_train_processed,\n            y_train_scaled,\n            alpha=alpha,\n            w_in=w,\n            b_in=b,\n            max_iter=max_iters,\n            tolerance=tolerance,\n            cost_fn=cost_fn,\n            gradient_compute_fn=gradient_fn\n        )\n        \n        # Retrieve best parameters from last iteration\n        last_iter = max(history.keys())\n        w_best = history[last_iter]['w']\n        b_best = history[last_iter]['b']\n        \n        # Predictions\n        y_pred_train = X_train_processed @ w_best + b_best\n        y_pred_test = X_test_processed @ w_best + b_best\n        \n        # Compute cost function (cost_fn returns half MSE)\n        train_cost_half = cost_fn(X_train_processed, y_train_scaled, w_best, b_best)\n        test_cost_half = cost_fn(X_test_processed, y_test_scaled, w_best, b_best)\n        \n        # Compute full MSE (because Gradient descent divided it by 2)\n        train_mse = 2 * train_cost_half\n        test_mse = 2 * test_cost_half\n        \n        # Compute R2 scores\n        train_r2 = r2_score(y_train_scaled, y_pred_train)\n        test_r2 = r2_score(y_test_scaled, y_pred_test)\n        \n        print(f'Fold {fold + 1}/{k} - Train MSE: {train_mse:.5f} | Test MSE: {test_mse:.5f}')\n        print(f'Fold {fold + 1}/{k} - Train RMSE: {np.sqrt(train_mse):.5f} | Test RMSE: {np.sqrt(test_mse):.5f}')\n        print(f'Fold {fold + 1}/{k} - Train R²: {train_r2:.5f} | Test R²: {test_r2:.5f}')\n        \n        mse_train_list.append(train_mse)\n        mse_test_list.append(test_mse)\n        r2_train_list.append(train_r2)\n        r2_test_list.append(test_r2)\n        \n        print('-'*10)\n        print('\\n'*3)\n    \n    print(f'\\nMedian Train MSE over {k} folds: {np.median(mse_train_list):.5f}')\n    print(f'Median Test MSE over {k} folds: {np.median(mse_test_list):.5f}')\n    print(f'Median Train R² over {k} folds: {np.median(r2_train_list):.5f}')\n    print(f'Median Test R² over {k} folds: {np.median(r2_test_list):.5f}')\n    results =  {\n        \"mse_train\": mse_train_list,\n        \"mse_test\": mse_test_list,\n        \"r2_train\": r2_train_list,\n        \"r2_test\": r2_test_list\n    }\n    return results\n\n\nnp.random.seed(42)\nk = 5\nalpha = 0.01\nmax_iters = 10000\ntolerance = 1e-4\n\nresults = k_fold_cross_validation(train, k, alpha, max_iters, tolerance, compute_cost_fn, compute_gradient)\n\n----------\n\nFold 1:\nTrain unique education: ['Bachelor degree' 'Msc' 'PhD' 'BAC']\nTest unique education: ['PhD' 'Msc' 'Bachelor degree' 'BAC']\nTrain unique sex: ['F' 'M']\nTest unique sex: ['F' 'M']\n\nFold 1/5 - Train MSE: 0.00113 | Test MSE: 0.00112\nFold 1/5 - Train RMSE: 0.03362 | Test RMSE: 0.03346\nFold 1/5 - Train R²: 0.99887 | Test R²: 0.99889\n----------\n\n\n\n\n----------\n\nFold 2:\nTrain unique education: ['PhD' 'Msc' 'Bachelor degree' 'BAC']\nTest unique education: ['Bachelor degree' 'Msc' 'PhD' 'BAC']\nTrain unique sex: ['F' 'M']\nTest unique sex: ['F' 'M']\n\nFold 2/5 - Train MSE: 0.00106 | Test MSE: 0.00099\nFold 2/5 - Train RMSE: 0.03263 | Test RMSE: 0.03142\nFold 2/5 - Train R²: 0.99894 | Test R²: 0.99894\n----------\n\n\n\n\n----------\n\nFold 3:\nTrain unique education: ['PhD' 'Msc' 'Bachelor degree' 'BAC']\nTest unique education: ['Bachelor degree' 'PhD' 'Msc' 'BAC']\nTrain unique sex: ['F' 'M']\nTest unique sex: ['F' 'M']\n\nFold 3/5 - Train MSE: 0.00090 | Test MSE: 0.00067\nFold 3/5 - Train RMSE: 0.03005 | Test RMSE: 0.02598\nFold 3/5 - Train R²: 0.99910 | Test R²: 0.99932\n----------\n\n\n\n\n----------\n\nFold 4:\nTrain unique education: ['PhD' 'Msc' 'Bachelor degree' 'BAC']\nTest unique education: ['PhD' 'Bachelor degree' 'BAC' 'Msc']\nTrain unique sex: ['F' 'M']\nTest unique sex: ['F' 'M']\n\nFold 4/5 - Train MSE: 0.00155 | Test MSE: 0.00231\nFold 4/5 - Train RMSE: 0.03941 | Test RMSE: 0.04801\nFold 4/5 - Train R²: 0.99845 | Test R²: 0.99786\n----------\n\n\n\n\n----------\n\nFold 5:\nTrain unique education: ['PhD' 'Msc' 'Bachelor degree' 'BAC']\nTest unique education: ['Msc' 'PhD' 'BAC' 'Bachelor degree']\nTrain unique sex: ['F' 'M']\nTest unique sex: ['F' 'M']\n\nFold 5/5 - Train MSE: 0.00106 | Test MSE: 0.00100\nFold 5/5 - Train RMSE: 0.03251 | Test RMSE: 0.03157\nFold 5/5 - Train R²: 0.99894 | Test R²: 0.99898\n----------\n\n\n\n\n\nMedian Train MSE over 5 folds: 0.00106\nMedian Test MSE over 5 folds: 0.00100\nMedian Train R² over 5 folds: 0.99894\nMedian Test R² over 5 folds: 0.99894\n\n\n      On peut remaquer qu’à chaque fold nos modalités sont présentes et dans les données de test et dans celles de l’apprentissage.\nLa validation croisée en 5 plis (K-Fold Cross-Validation) a été utilisée pour évaluer la performance du modèle. Les valeurs ci-dessous correspondent à la racine carrée de la fonction de coût (Root Mean Square Error, RMSE), calculée sur des données de sortie standardisées.\n\nRésultats par pli (RMSE Train vs Test)\n\n✅ Fold 1 :0.03362 vs 0.03346\n✅ Fold 2 :0.03263 vs 0.03142\n✅ Fold 3 :0.03005 vs 0.02598\n✅ Fold 4 :0.03941 vs 0.04801\n✅ Fold 5 :0.03251 vs 0.03157\n\n      Dans 4 des 5 folds, le MSE_test ≤ MSE_train, ce qui est surprenant mais pas impossible si la validation est bien faite et que les données sont très régulières.\nL’écart est faible dans tous les cas sauf pour le fold 4, où :\nMSE_test = 0.00231 &gt; MSE_train = 0.00155\nCela suggère un légère suradaptation (overfitting) pour ce fold, mais l’écart reste raisonnable.\n\n\nVisualisation de la somme erreurs quadratiques par pli\n\nmse_train_list = results['mse_train']\nmse_test_list = results['mse_test']\nr2_train_list = results['r2_train']\nr2_test_list = results['r2_test']\n\nmse_median_train = np.median(mse_train_list)\nmse_median_test = np.median(mse_test_list)\nr2_median_train = np.median(r2_train_list)\nr2_median_test = np.median(r2_test_list)\n\n\nfolds = np.arange(1, 6)\n\nfig, axes = plt.subplots(1, 2, figsize=(12,6.5))\n\n# --MSE--\n\naxes[0].plot(folds, mse_train_list, label='Train MSE', linestyle='--', marker='o', linewidth=1.5)\naxes[0].plot(folds, mse_test_list, label='Test MSE', linestyle='--', marker='o', linewidth=1.5)\naxes[0].axhline(y=mse_median_train, color='r', linestyle='--', linewidth=1.5, label=f'Train Median MSE = {mse_median_train:.5f}')\naxes[0].axhline(y=mse_median_test, color='g', linestyle='--', linewidth=1.5, label=f'Test Median MSE = {mse_median_test:.5f}')\naxes[0].set_title('MSE per fold', fontsize=10)\naxes[0].set_xlabel('Fold', fontsize=10)\naxes[0].set_ylabel('MSE (standardized scale)', fontsize=10)\naxes[0].legend(fontsize=8)\naxes[0].tick_params(axis='both', which='major', labelsize=8)\n\n# --R2--\n\naxes[1].plot(folds, r2_train_list, label='Train R²', linestyle='--', marker='o', linewidth=1.5)\naxes[1].plot(folds, r2_test_list, label='Test R²', linestyle='--', marker='o', linewidth=1.5)\naxes[1].axhline(y=r2_median_train, color='r', linestyle='--', linewidth=1.5, label=f'Train Median R² = {r2_median_train:.5f}')\naxes[1].axhline(y=r2_median_test, color='g', linestyle='--', linewidth=1.5, label=f'Test Median R² = {r2_median_test:.5f}')\naxes[1].set_title('R² per fold', fontsize=10)\naxes[1].set_xlabel('Fold', fontsize=10)\naxes[1].set_ylabel('R²', fontsize=10)\naxes[1].legend(fontsize=8)\naxes[1].tick_params(axis='both', which='major', labelsize=8)\n\nplt.tight_layout()\nplt.subplots_adjust(left=0.1, wspace=0.25)\nplt.show()\n\n\n\n\nFigure 5: Model Performance Metrics per Fold\n\n\n\n\n      Ce graphique présente les performances d’un modèle à travers une validation croisée à 5 plis (5-fold cross-validation), en utilisant deux métriques : MSE (Mean Squared Error) et R² (coefficient de détermination).\n\n\n📉 MSE par pli\n\nLignes bleues (Train MSE) et lignes orange (Test MSE) montrent la performance sur les données d’entraînement et de test respectivement.\nLes performances sont très similaires entre les différents plis, avec de faibles valeurs de MSE, ce qui indique une bonne qualité de prédiction.\nUne exception est observée pour le pli 4, où le Test MSE est sensiblement plus élevé (~0.0023), suggérant une possible instabilité ou une complexité locale non bien capturée par le modèle.\nLignes horizontales :\n\n🔴 Rouge pointillée : médiane du MSE d’entraînement (≈ 0.00106)\n🟢 Verte pointillée : médiane du MSE de test (≈ 0.00100)\n\n\n\nInterprétation : Le modèle généralise bien dans l’ensemble, avec un léger sur-apprentissage possible sur le pli 4.\n\n\n\n📈 R² par pli\n\nLe R² d’entraînement et de test est très élevé pour tous les plis, indiquant que le modèle explique plus de 99.88 % de la variance des données.\nUne baisse est observée pour le pli 4 avec un Test R² ≈ 0.9978, ce qui reste néanmoins très performant.\nLignes horizontales :\n\n🔴 Rouge pointillée : médiane du R² d’entraînement (≈ 0.99894)\n🟢 Verte pointillée : médiane du R² de test (≈ 0.99894)\n\n\n\n      En somme nous pouvons dirre que :\n\nLe modèle est très performant, avec des erreurs faibles et un pouvoir explicatif élevé sur l’ensemble des plis.\nLe pli 4 montre une légère instabilité, ce qui pourrait justifier une exploration complémentaire sur les données de ce pli.\nDans l’ensemble, les résultats montrent un excellent compromis biais-variance, avec une bonne généralisation."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#evaluation-du-modèle-final-sur-les-données-de-test",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#evaluation-du-modèle-final-sur-les-données-de-test",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Evaluation du modèle final sur les données de test",
    "text": "Evaluation du modèle final sur les données de test\n\n# selecting test features\nX_test = test.drop('salary', axis=1)\n\n# transforming test features (one hot encodings and standardization)\nX_test_processed = preprocessor.transform(X=X_test)\n\n# predictions on standardized test features\ny_pred_test_scaled = predict(best['w'], best['b'], X_test_processed).reshape((-1, 1))\n\n# reverse scaling\ny_pred_test = scaler_y.inverse_transform(y_pred_test_scaled).reshape(200,)\n\n\n# getting real y and changing the shape\ny_true_test = test['salary'].values.reshape(200,)\n\n      Maintenant calculons le RMSE des erreurs commises en prédisant. D’abord essayons de visualiser cela de manière séparée car il me vient à l’idée d’ordonner les différents arrays obtenus, mais avant faut que je sois sûre que les salaires réels (pas dans le sens économique du terme mais pour dire salaire observé) et ceux prédits ont la même allure.\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 8))\nabs_ = np.arange(1, 201, 1)\n\naxes[0].plot(abs_, y_true_test, c='g', label='Observed salaries', linewidth=1)\naxes[0].set_title('Observed Salaries', fontsize=10)\naxes[0].set_xlabel('Data points', fontsize=10)\naxes[0].set_ylabel('Salary', fontsize=10)\n\naxes[1].plot(abs_, y_pred_test, c='b', label='Predicted salaries', linewidth=1)\naxes[1].set_title('Predicted Salaries', fontsize=10)\naxes[1].set_xlabel('Data points', fontsize=10)\naxes[1].set_ylabel('Salary', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure 6: Observed Slaries and Predicted Salaries visualisation\n\n\n\n\n      En effet les deux courbe ont pratiquement la même allure, essayons de voir ce que ça donne quand on les arrange de manière croissante.\n\ny_pred_test_sorted = np.sort(y_pred_test)\ny_true_test_sorted = np.sort(y_true_test)\nplt.figure(figsize=(12, 8))\nabs_ = np.arange(1, 201, 1)\nplt.plot(abs_,y_true_test_sorted , c='g', label='Observed values of the salary', linewidth=1)\nplt.plot(abs_, y_pred_test_sorted, c='b',label='Predicted values of the salary', linewidth=1)\nplt.ylabel('Salary')\nplt.xlabel('Data points')\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure 7: Observed Slaries VS Predicted Salaries\n\n\n\n\n\n\n\n\n\n\nAnalyse graphique des salaires prédits vs observés\n\n\n\nLes courbes des salaires prédits et observés montrent une forte similarité.\nCela suggère que le modèle est capable de bien capturer la relation entre les variables explicatives et le salaire, indiquant ainsi une bonne capacité de généralisation sur les données de test.\n\n\n\nCalcul de la racine de l’erreur quadratique moyenne\n\n\n# RMSE on standardised salaries\ny_true_test_scaled = scaler_y.transform(y_true_test_).reshape(200, )\n\nrmse_scaled = np.sqrt(np.mean((y_pred_test_scaled - y_true_test_scaled)**2))\nprint(f\"standardized RMSE : {rmse_scaled:.2f}\\n\")\n\nstandardized RMSE : 1.38\n\n# RMSE on original wages\nrmse_original = np.sqrt(\n  (1/200)*(y_pred_test - y_true_test).T @ (y_pred_test - y_true_test)\n)\nprint(f\"original RMSE : {rmse_original:.2f}\\n\")\n\noriginal RMSE : 283.96\n\n\n\n\n\n\n\n\n⚠️ Attention à l’interprétation : overfitting ou déséquilibre ?\n\n\n\nLe fait que la RMSE sur le test soit nettement plus élevée que celle obtenue en validation croisée peut provenir de :\n🔁 Overfitting : le modèle a trop appris les spécificités du jeu d’entraînement (même via cross-validation), et généralise mal (ce qui est moins probable).\n⚖️ Déséquilibre dans les variables catégorielles : il se peut que la répartition des niveaux d’éducation ou des sexes soit différente entre le train et le test, ce qui fausse l’évaluation (Ce qui est plus probable car au debut j’avais mentionné ce soucis).\nIl est donc essentiel d’examiner les distributions des variables dans chaque ensemble pour comprendre la cause exacte.\n🧑‍🏫 Mais rappelons que le but ici n’était pas de construire le meilleur modèle, mais de montrer concrètement la mise en œuvre de l’algorithme de descente de gradient en contexte réel.\nCe genre d’écart illustre parfaitement pourquoi la généralisation est un défi fondamental en machine learning.\n\n\n\nInterprétation de la RMSE en valeur réelle\n\n      Les salaires dans notre dataset varient entre environ 24 933 € et 75 318 €, ce qui explique qu’une RMSE d’environ 284 € soit cohérente.\nPour mieux comprendre ce que signifie cette erreur moyenne, considérons l’échelle des salaires :\nla différence entre le minimum et le maximum est d’environ 50 385 €.\nAinsi, une RMSE de 284 € correspond à une erreur moyenne relative d’environ :\n\\[\n\\frac{284}{50 385} \\approx 0.0056 \\quad \\text{soit} \\quad 0{,}56\\%\n\\]"
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#conclusion-générale",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#conclusion-générale",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Conclusion générale",
    "text": "Conclusion générale\n        En résumé, la descente de gradient s’est révélée être un algorithme simple, intuitif et pourtant puissant pour la minimisation de la fonction de coût dans le cadre de la régression linéaire. En parcourant de manière itérative la direction opposée au gradient, l’algorithme permet une réduction monotone du coût à chaque itération.\nNous avons vu que ce procédé consiste à ajuster les coefficients du modèle à partir d’un point initial, en effectuant des pas proportionnels au négatif du gradient et calibrés par le taux d’apprentissage. Tant que celui-ci est bien choisi (pas trop grand, pas trop petit), la procédure converge vers un minimum local de la fonction de coût. Toutefois, nous avons aussi montré que certains cas (comme des ravins étroits ou des matrices Hessiennes mal conditionnées) peuvent ralentir la convergence ou provoquer des oscillations.\n      La faible erreur en euros (300 sur des milliers) sur les données de test, combinée à une RMSE relativement faible en validation croisée, suggère que le modèle est bien calibré et capable de généraliser efficacement. Cependant, l’écart entre l’entraînement et la validation pourrait indiquer un léger surapprentissage, bien que cet écart soit modéré.\n\nPoints clés à retenir\n\nPrincipe de fonctionnement\nLe paramètre \\(\\theta\\) est mis à jour selon \\(\\theta \\leftarrow \\theta - \\gamma \\nabla_\\theta J(\\theta)\\), avec \\(J\\) la fonction de coût et \\(\\alpha\\) le pas d’apprentissage.\nMonotonie et convergence\nÀ chaque étape, la valeur du coût décroît (tant que \\(\\alpha\\) est convenablement choisie), garantissant la progression vers un minimum local.\nLimites de la méthode\nLa descente de gradient peut rencontrer des difficultés de convergence lorsqu’on affine le minimum dans des zones étroites ou avec des gradients très pontuels. D’autres méthodes, comme le gradient conjugué ou Newton, peuvent alors offrir un gain en performance.\n\n\n\n\n\n\n\n⚠️ Attention : Influence de l’échelle des variables explicatives\n\n\n\nLorsque les variables explicatives ont des échelles très différentes, cela peut provoquer un comportement non optimal de la descente de gradient.\nPar exemple : - Une variable \\(x_1\\) variant entre 0 et 1 - Une autre variable \\(x_2\\) variant entre 10 et 50\nUn petit poids \\(w_2\\) associé à \\(x_2\\) peut entraîner une variation importante de la prédiction, tandis qu’un poids \\(w_1\\) beaucoup plus grand associé à \\(x_1\\) pourrait n’avoir qu’un effet marginal.\nConséquences : - La surface de la fonction de coût est très étirée dans certaines directions. - La descente de gradient progresse très lentement, zigzague ou peut ne jamais converger. - Le nombre d’itérations nécessaires augmente fortement.\n👉 C’est pourquoi la normalisation des variables (standardisation ou min-max scaling) est une étape cruciale avant d’entraîner un modèle linéaire avec descente de gradient.\n\n\n\n\n\n\n\n\n⚠️ Important : fit_transform() vs transform()\n\n\n\nToujours utiliser fit_transform() uniquement sur les données d’entraînement, et transform() sur les données de validation ou de test.\nCela s’applique à tous les types de preprocessing, notamment :\n\n🟦 StandardScaler : la moyenne et l’écart-type doivent être appris sur le train uniquement.\n🟧 OneHotEncoder : les catégories doivent être identifiées à partir du train et appliquées de manière cohérente au test.\n\n❌ Ne jamais faire fit_transform() sur le test, car cela introduit du data leakage (les données de test influencent le modèle).\n\n\n\n\nPerspectives\n\nAmélioration du pas (\\(\\gamma\\)) : l’usage d’un pas adaptatif ou de schémas comme l’apprentissage décroissant peut améliorer la rapidité et la robustesse de la convergence.\nExtensions avancées : l’ajout de régularisation (comme Ridge ou Lasso) ou le passage à des variantes stochastiques (SGD) ou mini-batch permet de généraliser la méthode à des ensembles plus volumineux et à la modélisation en profondeur.\nCombinaison avec d’autres algorithmes : pour pallier les inefficacités, on peut intégrer des techniques comme le momentum, AdaGrad, RMSprop ou Adam, qui corrélent le gradient pour accélérer la convergence et stabiliser l’apprentissage.\n\n\n\nFinalement\n      La descente de gradient traduit élégamment les principes fondamentaux de l’optimisation itérative : à chaque étape, un petit ajustement calculé éloigne le modèle de l’erreur, jusqu’à atteindre la valeur optimale des paramètres (). Même si elle n’est pas sans défauts, cette méthode demeure une brique essentielle en apprentissage automatique, particulièrement en régression linéaire, et sert de base à des méthodes plus sophistiquées."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#remerciements-et-retour-dexpérience",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#remerciements-et-retour-dexpérience",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Remerciements et retour d’expérience",
    "text": "Remerciements et retour d’expérience\nJ’ai été ravi d’avoir consacré près de 20 heures à la préparation et à la rédaction de cette publication.\nCet effort m’a permis de :\n\nconsolider mes acquis en Python,\n\napprofondir mes connaissances en Machine Learning, tant sur le plan théorique que pratique,\n\nrenforcer ma rigueur méthodologique dans le traitement des données, l’expérimentation et l’analyse des résultats.\n\nCe projet m’a également offert une excellente opportunité de structurer une démarche complète de modélisation, depuis la génération des données jusqu’à l’interprétation finale des performances du modèle.\nJe suis enthousiaste à l’idée de poursuivre cette exploration dans de futures publications de ce genre (travailler à la mano), notamment sur des cas réels avec des modèles plus avancés comme la régression logistique ou des approches régularisées.\nMon cours d’optimisation et de méthode de calcul numériques m’a été d’une grande utilité en 1A à l’ENSAI."
  },
  {
    "objectID": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#annexes",
    "href": "FORMATIONS/machine-learning/gradient-descent-linear-reg.html#annexes",
    "title": "Mise en œuvre de l’algorithme de la descente de gradient : Cas de la régression linéaire",
    "section": "Annexes",
    "text": "Annexes\n\n1. Standardisation des variables\n      La standardisation permet de centrer et réduire les variables numériques pour qu’elles aient une moyenne nulle et un écart-type unitaire.\nPour une variable continue \\(x\\), la standardisation est donnée par :\n\\[\nx_{\\text{std}} = \\frac{x - \\mu_x}{\\sigma_x}\n\\]\noù :\n\n\\(\\mu_x = \\dfrac{1}{n} \\sum_{i=1}^n x_i\\) est la moyenne,\n\\(\\sigma_x = \\sqrt{\\dfrac{1}{n} \\sum_{i=1}^n (x_i - \\mu_x)^2}\\) est l’écart-type.\n\nAprès avoir entraîné le modèle sur les données standardisées, on peut revenir à l’échelle réelle avec :\n\\[\n\\hat{y} = \\hat{y}_{\\text{std}} \\cdot \\sigma_y + \\mu_y\n\\]\noù \\(\\mu_y\\) et \\(\\sigma_y\\) sont la moyenne et l’écart-type de la variable cible \\(y\\).\n\n\n\n2. Codage One-Hot\nLe One-Hot Encoding transforme une variable catégorielle à \\(k\\) modalités en \\(k\\) colonnes binaires.\nSoit une variable catégorielle :\n\\[\n\\text{cat} \\in \\{c_1, c_2, \\ldots, c_k\\}\n\\]\nOn crée un vecteur :\n\\[\n\\mathbf{v} = (v_1, v_2, \\ldots, v_k) \\quad \\text{où} \\quad\nv_j =\n\\begin{cases}\n1 & \\text{si } \\text{cat} = c_j \\\\\n0 & \\text{sinon}\n\\end{cases}\n\\]\nAfin d’éviter la redondance, on supprime une modalité (ex. via drop='first') pour éviter le piège des variables muettes (dummy variable trap).\n\n\n\n3. Calcul de la RMSE\nLa Root Mean Square Error (RMSE) mesure l’écart quadratique moyen entre les valeurs prédites \\(\\hat{y}_i\\) et observées \\(y_i\\) :\n\\[\n\\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 }\n\\]\nEn notation vectorielle, si \\(\\mathbf{y}\\) est le vecteur des valeurs observées et \\(\\hat{\\mathbf{y}}\\) celui des valeurs prédites :\n\\[\n\\text{RMSE} = \\sqrt{ \\frac{1}{n} (\\mathbf{y} - \\hat{\\mathbf{y}})^T (\\mathbf{y} - \\hat{\\mathbf{y}}) }\n\\]\nCette métrique est exprimée dans l’unité de la variable cible (ici, les euros), ce qui la rend facile à interpréter dans un contexte réel.\n\nCes opérations sont fondamentales dans toute pipeline de traitement pour la régression ou tout autre algorithme supervisé."
  },
  {
    "objectID": "FORMATIONS/PAYANTES/R/FormationR.html",
    "href": "FORMATIONS/PAYANTES/R/FormationR.html",
    "title": "Djamaldbz - Formations en R en présentiel et en ligne avec Djamal et Saïd",
    "section": "",
    "text": "Ces formations sont conçues pour différents publics cibles : étudiants en pharmacie, médecine, biologie, statistiques et ceux aui sont dans des domaines nécessitant les stats ou pas. Chaque session dure 2 heures, avec une fréquence de 2 sessions par semaine. Les formations débutent le 22 février 2025.\n\n\n\n\n5 000 FCFA par session de 2 heures.\nChaque formation complète comprend 4 sessions, soit 20 000 FCFA par participant.\n\n\n\n\n\n\n\nCalendrier des Formations\n\n\nDate\nPublic.cible\nSujet\n\n\n\n\n22 février\nPharmacie\nIntroduction à R\n\n\n22 février\nMédecine\nIntroduction à R\n\n\n22 février\nBiologie\nIntroduction à R\n\n\n26 février\nStatistiques\nR pour les statisticiens\n\n\n\n\n\n\n\n\n\n\n\nObjectif : Apprendre à gérer, analyser et visualiser des données pharmacologiques.\nSessions :\n\nIntroduction à R.\nGestion des données pharmacologiques.\nVisualisation des données.\nAnalyse statistique (tests t, ANOVA).\n\n\n\n\n\n\nObjectif : Explorer des données cliniques et épidémiologiques.\nSessions :\n\nIntroduction à R.\nStatistiques descriptives.\nVisualisation des données médicales.\n\n\n\n\n\n\nObjectif : Analyser des données biologiques\nSessions :\n\nIntroduction à R.\nVisualisation des données biologiques.\nAnalyse statistique.\n\n\n\n\n\n\nObjectif : Approfondir les outils statistiques et analytiques.\nSessions :\n\nR pour les statisticiens.\nVisualisations avancées avec ggplot2.\nModélisation statistique (modèles linéaires, généralisés).\nProgrammation avancée (création de fonctions, etc …).\n\n\n\n\n\n\n\n\nRappel\n\n\n\nPour celles et ceux qui ne font pas partie des domaines mentionnés, ne vous inquiétez pas : cette formation est conçue pour être accessible et adaptée à tous les profils. Vous en tirerez pleinement profit !\n\n\n\n\n\n\n\n\nLocalisation pour la formation en présentiel\n\n\n\nLes participants doivent être au Burkina-Faso, plus précisement dans la ville de Bobo-Dioulasso. Les séances en ligne interviendront rarement. Elles serviront à donner certains details et seront une alternatives en cas d’empêchement !!!\n\n\n\n\n\n\n\n\nLocalisation pour la formation en présentiel\n\n\n\nPour les participants ayant des empechements (localisation géographique, timing etc …), une formartion ligne sera possible mais au lieu de 2h ce sera 1h30 !!!\n\n\n\n\n\n\nLes participants bénéficieront de formations pratiques, avec des cas d’utilisation adaptés à leur domaine. Inscrivez-vous dès maintenant pour réserver votre place! 😊\n\nPOUR PLUS D’INFORMATIONS !!!\n\n      Veuillez contacter le numéro whatsapp suivant : +226 57036356"
  },
  {
    "objectID": "FORMATIONS/PAYANTES/R/FormationR.html#plan-des-formations-en-r---niveau-1",
    "href": "FORMATIONS/PAYANTES/R/FormationR.html#plan-des-formations-en-r---niveau-1",
    "title": "Djamaldbz - Formations en R en présentiel et en ligne avec Djamal et Saïd",
    "section": "",
    "text": "Ces formations sont conçues pour différents publics cibles : étudiants en pharmacie, médecine, biologie, statistiques et ceux aui sont dans des domaines nécessitant les stats ou pas. Chaque session dure 2 heures, avec une fréquence de 2 sessions par semaine. Les formations débutent le 22 février 2025.\n\n\n\n\n5 000 FCFA par session de 2 heures.\nChaque formation complète comprend 4 sessions, soit 20 000 FCFA par participant.\n\n\n\n\n\n\n\nCalendrier des Formations\n\n\nDate\nPublic.cible\nSujet\n\n\n\n\n22 février\nPharmacie\nIntroduction à R\n\n\n22 février\nMédecine\nIntroduction à R\n\n\n22 février\nBiologie\nIntroduction à R\n\n\n26 février\nStatistiques\nR pour les statisticiens\n\n\n\n\n\n\n\n\n\n\n\nObjectif : Apprendre à gérer, analyser et visualiser des données pharmacologiques.\nSessions :\n\nIntroduction à R.\nGestion des données pharmacologiques.\nVisualisation des données.\nAnalyse statistique (tests t, ANOVA).\n\n\n\n\n\n\nObjectif : Explorer des données cliniques et épidémiologiques.\nSessions :\n\nIntroduction à R.\nStatistiques descriptives.\nVisualisation des données médicales.\n\n\n\n\n\n\nObjectif : Analyser des données biologiques\nSessions :\n\nIntroduction à R.\nVisualisation des données biologiques.\nAnalyse statistique.\n\n\n\n\n\n\nObjectif : Approfondir les outils statistiques et analytiques.\nSessions :\n\nR pour les statisticiens.\nVisualisations avancées avec ggplot2.\nModélisation statistique (modèles linéaires, généralisés).\nProgrammation avancée (création de fonctions, etc …).\n\n\n\n\n\n\n\n\nRappel\n\n\n\nPour celles et ceux qui ne font pas partie des domaines mentionnés, ne vous inquiétez pas : cette formation est conçue pour être accessible et adaptée à tous les profils. Vous en tirerez pleinement profit !\n\n\n\n\n\n\n\n\nLocalisation pour la formation en présentiel\n\n\n\nLes participants doivent être au Burkina-Faso, plus précisement dans la ville de Bobo-Dioulasso. Les séances en ligne interviendront rarement. Elles serviront à donner certains details et seront une alternatives en cas d’empêchement !!!\n\n\n\n\n\n\n\n\nLocalisation pour la formation en présentiel\n\n\n\nPour les participants ayant des empechements (localisation géographique, timing etc …), une formartion ligne sera possible mais au lieu de 2h ce sera 1h30 !!!\n\n\n\n\n\n\nLes participants bénéficieront de formations pratiques, avec des cas d’utilisation adaptés à leur domaine. Inscrivez-vous dès maintenant pour réserver votre place! 😊\n\nPOUR PLUS D’INFORMATIONS !!!\n\n      Veuillez contacter le numéro whatsapp suivant : +226 57036356"
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html",
    "href": "FORMATIONS/poisson_paludisme.html",
    "title": "Modélisation des données de comptage",
    "section": "",
    "text": "Le monde actuel est confronté à de multiples risques sanitaires, notamment ceux liés aux maladies vectorielles telles que le paludisme. En effet, le paludisme est la maladie la plus mortelle transmise par les moustiques dans le monde ((OMS) 2023). Selon l’OMS, plusieurs millions de personnes ont été infectées par le paludisme en 2022 (environ 249 millions), entraînant près de 608 000 décès(Mondiale de la Santé) 2023).\nPlusieurs actions ont été menées pour lutter contre ce fléau, notamment la distribution de moustiquaires, les campagnes de sensibilisation à l’hygiène, la chimioprévention saisonnière, ainsi que le traitement intermittent pour les femmes enceintes.\nDjamaland a été choisi comme pays pour la mise en oeuvre d’une intervention progressive, principalement en raison de sa forte incidence du paludisme. L’intervention comprend quatre phases et couvre l’ensemble des régions du pays.\nVoici la description de chaque phase :\n\nPhase 1 : Aucun village n’a reçu d’intervention.\nPhase 2 : Les quatre régions ont bénéficié de la distribution de moustiquaires.\nPhase 3 : En plus de la distribution de moustiquaires, des actions de sensibilisation sur les bonnes pratiques d’utilisation ont été mises en place.\nPhase 3 suite : En complément de la distribution et de la sensibilisation, un programme de partage des techniques de bonne hygiène a été intégré.\n\nLa base contenait également des informations sur les facteurs environnementaux (pression atmosphérique, vitesse du vent, indice UV, humidité relative).\nLe but de cette étude est donc d’évaluer l’impact de l’intervention durant ces différentes phases.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(MASS) \nlibrary(car)\n\n\nInformation sur les variables\n\n      On affiche ici les informations sur les variables de la base de données. On voit qu’il y’a 19 colonnes (variables) et 1040 lignes (observations).\n\ndata %&gt;%\n  glimpse()\n\nRows: 1.040\nColumns: 19\n$ Semaine                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ Région                   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R…\n$ Saison                   &lt;chr&gt; \"Seche\", \"Seche\", \"Seche\", \"Seche\", \"Seche\", …\n$ Phase                    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Taux de couverture`     &lt;dbl&gt; 0,1575155, 0,2576610, 0,1817954, 0,2766035, 0…\n$ Température              &lt;dbl&gt; 25,52402, 31,06084, 28,69764, 28,11317, 30,57…\n$ Humidité                 &lt;dbl&gt; 43,75700, 44,61046, 51,25039, 49,40113, 48,49…\n$ Pluviométrie             &lt;dbl&gt; 16,0111516, 8,2997481, 44,7344354, 38,9232328…\n$ `Vitesse du vent`        &lt;dbl&gt; 6,239284, 7,855973, 3,604419, 6,324740, 2,649…\n$ `Pression atmosphérique` &lt;dbl&gt; 1015,6362, 1008,8631, 1008,3488, 1022,2085, 1…\n$ `Indice de chaleur`      &lt;dbl&gt; 19,83300, 25,96362, 23,67617, 22,94634, 25,61…\n$ `Couverture nuageuse`    &lt;dbl&gt; 82,280557, 57,209495, 32,469455, 14,619983, 1…\n$ `Vent en hauteur`        &lt;dbl&gt; 14,668663, 8,698154, 11,789413, 14,660355, 11…\n$ `Indice UV`              &lt;dbl&gt; 7,03602105, 10,09389904, 0,64470344, 5,463245…\n$ `Température de l'eau`   &lt;dbl&gt; 33,22569, 28,26913, 28,00962, 29,71320, 28,66…\n$ `Humidité à l’ombre`     &lt;dbl&gt; 53,83643, 64,71406, 55,98915, 63,04538, 62,21…\n$ Aérosols                 &lt;dbl&gt; 1,477347, 4,530642, 34,596087, 80,345317, 18,…\n$ `Cas palustres`          &lt;dbl&gt; 97, 62, 76, 66, 163, 67, 154, 168, 79, 58, 15…\n$ Dates                    &lt;date&gt; 2021-01-08, 2021-01-15, 2021-01-22, 2021-01-…\n\n\n      On affiche ensuite un résumé statistique des variables dans le but de reperer certaines anomalies s’il y en a. Mais dans ce cas, il y’en a pas car j’ai moi même généré les données et donc j’ai veillé à ce qu’il n y ait pas de valeurs manquantes.\n\nlibrary(dplyr)\ndata %&gt;%\n  summary()\n\n    Semaine       Région      Saison          Phase   Taux de couverture\n Min.   :  1,00   R1:260   Length:1040        0:516   Min.   :0,1001    \n 1st Qu.: 65,75   R2:260   Class :character   1:104   1st Qu.:0,2019    \n Median :130,50   R3:260   Mode  :character   2:208   Median :0,4023    \n Mean   :130,50   R4:260                      3:212   Mean   :0,4301    \n 3rd Qu.:195,25                                       3rd Qu.:0,6584    \n Max.   :260,00                                       Max.   :0,8998    \n  Température       Humidité      Pluviométrie       Vitesse du vent\n Min.   :13,64   Min.   :35,41   Min.   :  0,04131   Min.   :2,001  \n 1st Qu.:23,97   1st Qu.:48,81   1st Qu.: 21,90016   1st Qu.:3,975  \n Median :27,09   Median :54,81   Median : 41,11740   Median :5,975  \n Mean   :27,08   Mean   :61,99   Mean   : 64,46301   Mean   :5,913  \n 3rd Qu.:30,21   3rd Qu.:78,54   3rd Qu.:106,59262   3rd Qu.:7,826  \n Max.   :40,19   Max.   :95,11   Max.   :199,59270   Max.   :9,998  \n Pression atmosphérique Indice de chaleur Couverture nuageuse Vent en hauteur \n Min.   : 980,4         Min.   : 8,405    Min.   : 0,0502     Min.   : 5,007  \n 1st Qu.:1006,5         1st Qu.:19,530    1st Qu.:25,7869     1st Qu.: 7,288  \n Median :1012,9         Median :22,434    Median :50,2530     Median : 9,831  \n Mean   :1012,9         Mean   :22,400    Mean   :50,5150     Mean   : 9,916  \n 3rd Qu.:1019,5         3rd Qu.:25,580    3rd Qu.:76,9193     3rd Qu.:12,654  \n Max.   :1041,6         Max.   :36,178    Max.   :99,9899     Max.   :14,995  \n   Indice UV         Température de l'eau Humidité à l’ombre    Aérosols       \n Min.   : 0,001759   Min.   :20,80        Min.   : 42,66     Min.   : 0,00251  \n 1st Qu.: 3,201972   1st Qu.:26,32        1st Qu.: 58,98     1st Qu.:23,49266  \n Median : 5,851642   Median :28,40        Median : 64,76     Median :47,47861  \n Mean   : 6,034127   Mean   :28,40        Mean   : 72,04     Mean   :48,53851  \n 3rd Qu.: 9,109514   3rd Qu.:30,54        3rd Qu.: 88,35     3rd Qu.:73,70956  \n Max.   :11,971607   Max.   :37,70        Max.   :104,38     Max.   :99,89207  \n Cas palustres       Dates           \n Min.   : 13,0   Min.   :2021-01-08  \n 1st Qu.: 58,0   1st Qu.:2022-04-06  \n Median : 93,0   Median :2023-07-03  \n Mean   :111,2   Mean   :2023-07-03  \n 3rd Qu.:145,2   3rd Qu.:2024-09-28  \n Max.   :466,0   Max.   :2025-12-26  \n\n\n      Pour cette étude, la variable d’intérêt est le nombre de nouveaux cas de paludisme enregistrés chaque semaine (t), avec des valeurs variant de 1 à 260 dans les quatre régions du pays."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#description-du-jeu-de-données",
    "href": "FORMATIONS/poisson_paludisme.html#description-du-jeu-de-données",
    "title": "Modélisation des données de comptage",
    "section": "",
    "text": "Le monde actuel est confronté à de multiples risques sanitaires, notamment ceux liés aux maladies vectorielles telles que le paludisme. En effet, le paludisme est la maladie la plus mortelle transmise par les moustiques dans le monde ((OMS) 2023). Selon l’OMS, plusieurs millions de personnes ont été infectées par le paludisme en 2022 (environ 249 millions), entraînant près de 608 000 décès(Mondiale de la Santé) 2023).\nPlusieurs actions ont été menées pour lutter contre ce fléau, notamment la distribution de moustiquaires, les campagnes de sensibilisation à l’hygiène, la chimioprévention saisonnière, ainsi que le traitement intermittent pour les femmes enceintes.\nDjamaland a été choisi comme pays pour la mise en oeuvre d’une intervention progressive, principalement en raison de sa forte incidence du paludisme. L’intervention comprend quatre phases et couvre l’ensemble des régions du pays.\nVoici la description de chaque phase :\n\nPhase 1 : Aucun village n’a reçu d’intervention.\nPhase 2 : Les quatre régions ont bénéficié de la distribution de moustiquaires.\nPhase 3 : En plus de la distribution de moustiquaires, des actions de sensibilisation sur les bonnes pratiques d’utilisation ont été mises en place.\nPhase 3 suite : En complément de la distribution et de la sensibilisation, un programme de partage des techniques de bonne hygiène a été intégré.\n\nLa base contenait également des informations sur les facteurs environnementaux (pression atmosphérique, vitesse du vent, indice UV, humidité relative).\nLe but de cette étude est donc d’évaluer l’impact de l’intervention durant ces différentes phases.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(MASS) \nlibrary(car)\n\n\nInformation sur les variables\n\n      On affiche ici les informations sur les variables de la base de données. On voit qu’il y’a 19 colonnes (variables) et 1040 lignes (observations).\n\ndata %&gt;%\n  glimpse()\n\nRows: 1.040\nColumns: 19\n$ Semaine                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ Région                   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R…\n$ Saison                   &lt;chr&gt; \"Seche\", \"Seche\", \"Seche\", \"Seche\", \"Seche\", …\n$ Phase                    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Taux de couverture`     &lt;dbl&gt; 0,1575155, 0,2576610, 0,1817954, 0,2766035, 0…\n$ Température              &lt;dbl&gt; 25,52402, 31,06084, 28,69764, 28,11317, 30,57…\n$ Humidité                 &lt;dbl&gt; 43,75700, 44,61046, 51,25039, 49,40113, 48,49…\n$ Pluviométrie             &lt;dbl&gt; 16,0111516, 8,2997481, 44,7344354, 38,9232328…\n$ `Vitesse du vent`        &lt;dbl&gt; 6,239284, 7,855973, 3,604419, 6,324740, 2,649…\n$ `Pression atmosphérique` &lt;dbl&gt; 1015,6362, 1008,8631, 1008,3488, 1022,2085, 1…\n$ `Indice de chaleur`      &lt;dbl&gt; 19,83300, 25,96362, 23,67617, 22,94634, 25,61…\n$ `Couverture nuageuse`    &lt;dbl&gt; 82,280557, 57,209495, 32,469455, 14,619983, 1…\n$ `Vent en hauteur`        &lt;dbl&gt; 14,668663, 8,698154, 11,789413, 14,660355, 11…\n$ `Indice UV`              &lt;dbl&gt; 7,03602105, 10,09389904, 0,64470344, 5,463245…\n$ `Température de l'eau`   &lt;dbl&gt; 33,22569, 28,26913, 28,00962, 29,71320, 28,66…\n$ `Humidité à l’ombre`     &lt;dbl&gt; 53,83643, 64,71406, 55,98915, 63,04538, 62,21…\n$ Aérosols                 &lt;dbl&gt; 1,477347, 4,530642, 34,596087, 80,345317, 18,…\n$ `Cas palustres`          &lt;dbl&gt; 97, 62, 76, 66, 163, 67, 154, 168, 79, 58, 15…\n$ Dates                    &lt;date&gt; 2021-01-08, 2021-01-15, 2021-01-22, 2021-01-…\n\n\n      On affiche ensuite un résumé statistique des variables dans le but de reperer certaines anomalies s’il y en a. Mais dans ce cas, il y’en a pas car j’ai moi même généré les données et donc j’ai veillé à ce qu’il n y ait pas de valeurs manquantes.\n\nlibrary(dplyr)\ndata %&gt;%\n  summary()\n\n    Semaine       Région      Saison          Phase   Taux de couverture\n Min.   :  1,00   R1:260   Length:1040        0:516   Min.   :0,1001    \n 1st Qu.: 65,75   R2:260   Class :character   1:104   1st Qu.:0,2019    \n Median :130,50   R3:260   Mode  :character   2:208   Median :0,4023    \n Mean   :130,50   R4:260                      3:212   Mean   :0,4301    \n 3rd Qu.:195,25                                       3rd Qu.:0,6584    \n Max.   :260,00                                       Max.   :0,8998    \n  Température       Humidité      Pluviométrie       Vitesse du vent\n Min.   :13,64   Min.   :35,41   Min.   :  0,04131   Min.   :2,001  \n 1st Qu.:23,97   1st Qu.:48,81   1st Qu.: 21,90016   1st Qu.:3,975  \n Median :27,09   Median :54,81   Median : 41,11740   Median :5,975  \n Mean   :27,08   Mean   :61,99   Mean   : 64,46301   Mean   :5,913  \n 3rd Qu.:30,21   3rd Qu.:78,54   3rd Qu.:106,59262   3rd Qu.:7,826  \n Max.   :40,19   Max.   :95,11   Max.   :199,59270   Max.   :9,998  \n Pression atmosphérique Indice de chaleur Couverture nuageuse Vent en hauteur \n Min.   : 980,4         Min.   : 8,405    Min.   : 0,0502     Min.   : 5,007  \n 1st Qu.:1006,5         1st Qu.:19,530    1st Qu.:25,7869     1st Qu.: 7,288  \n Median :1012,9         Median :22,434    Median :50,2530     Median : 9,831  \n Mean   :1012,9         Mean   :22,400    Mean   :50,5150     Mean   : 9,916  \n 3rd Qu.:1019,5         3rd Qu.:25,580    3rd Qu.:76,9193     3rd Qu.:12,654  \n Max.   :1041,6         Max.   :36,178    Max.   :99,9899     Max.   :14,995  \n   Indice UV         Température de l'eau Humidité à l’ombre    Aérosols       \n Min.   : 0,001759   Min.   :20,80        Min.   : 42,66     Min.   : 0,00251  \n 1st Qu.: 3,201972   1st Qu.:26,32        1st Qu.: 58,98     1st Qu.:23,49266  \n Median : 5,851642   Median :28,40        Median : 64,76     Median :47,47861  \n Mean   : 6,034127   Mean   :28,40        Mean   : 72,04     Mean   :48,53851  \n 3rd Qu.: 9,109514   3rd Qu.:30,54        3rd Qu.: 88,35     3rd Qu.:73,70956  \n Max.   :11,971607   Max.   :37,70        Max.   :104,38     Max.   :99,89207  \n Cas palustres       Dates           \n Min.   : 13,0   Min.   :2021-01-08  \n 1st Qu.: 58,0   1st Qu.:2022-04-06  \n Median : 93,0   Median :2023-07-03  \n Mean   :111,2   Mean   :2023-07-03  \n 3rd Qu.:145,2   3rd Qu.:2024-09-28  \n Max.   :466,0   Max.   :2025-12-26  \n\n\n      Pour cette étude, la variable d’intérêt est le nombre de nouveaux cas de paludisme enregistrés chaque semaine (t), avec des valeurs variant de 1 à 260 dans les quatre régions du pays."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#description-du-nombre-de-cas-pour-chaque-région",
    "href": "FORMATIONS/poisson_paludisme.html#description-du-nombre-de-cas-pour-chaque-région",
    "title": "Modélisation des données de comptage",
    "section": "Description du nombre de cas pour chaque région",
    "text": "Description du nombre de cas pour chaque région\n\np1 &lt;- ggplot(data, aes(x = Dates, y = `Cas palustres`, color = Région)) +\n  geom_line() +\n  facet_wrap(~Région, scales = \"free_y\") +\n  labs(title = \"\", x = \"Année\", y = \"Nombre de cas\") +\n  geom_vline(xintercept = as.numeric(as.Date(\"2023-06-30\")), \n           linetype = \"dashed\", color = \"darkred\", size = 0.5) +\n  geom_vline(xintercept = as.numeric(as.Date(\"2023-12-29\")), \n           linetype = \"dashed\", color = \"darkblue\", size = 0.5) +\n  geom_vline(xintercept = as.numeric(as.Date(\"2024-12-24\")), \n           linetype = \"dashed\", color = \"royalblue\", size = 0.5) +\n  theme_light() +\n  scale_x_date(date_breaks = \"12 months\", date_labels = \"%b %Y\") +  \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\np_interactif1 &lt;- ggplotly(p1)\n\np_interactif1\n\n\n\nFigure 1 : Evolution du nombre de cas de paludisme entre 2021 et 2025\n\n\n      La courbe des séries temporelles des cas de paludisme de 2021 à 2025 pour les quatre régions de l’étude montre une tendance générale à la baisse, particulièrement marquée après la mise en place des interventions. L’interpretation reste quasi pareille pour toute les regions.\nLe test de Mann-Kendall confirme statistiquement cette tendance décroissante significative (p-value &lt; 0.05), avec une diminution notable observée dans chaque région dès l’implémentation de la première phase du projet (figure @ref{fig:evolution}).\nPar ailleurs, le test de Kruskal-Wallis appliqué aux différentes phases du projet révèle une différence significative entre le nombre de cas observés avant et après les interventions (p-value &lt; 0.05), suggérant un impact positif des mesures mises en place.\nEnfin, le pic épidémique le plus élevé a été observé en 2021 dans les régions 1, 3 et 4, avec respectivement 392, 396 et 466 cas de paludisme enregistrés aux mois de septembre et octobre. Pour la région 2, le pic a été atteint en 2022, avec 384 cas observés (figure @ref{fig:evolution})."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#modélisation",
    "href": "FORMATIONS/poisson_paludisme.html#modélisation",
    "title": "Modélisation des données de comptage",
    "section": "Modélisation",
    "text": "Modélisation\n\nAnalyse de la corrélation entre les variables météorologiques\n      L’analyse de la corrélation entre les variables montre des liens de corrélation relativement faibles. De plus, le calcul de l’indice de KMO, permettant de vérifier l’adéquation des données à l’analyse en composantes principales, a montré une valeur de 0,5, confirmant le faible niveau de corrélation entre les covariables et ne justifiant ainsi pas la réalisation d’une ACP.\n\ndata_meteo &lt;- data[ , c(6:10 , 13)]\n\n##-- Calcul de la matrice de corrélation\ncor_matrix &lt;- cor(data_meteo, use = \"complete.obs\")\n\n##-- Transformation de la matrice de corrélation en format long pour ggplot2\ncor_melted &lt;- melt(cor_matrix)\ncor_melted$value &lt;- round(cor_melted$value , 2)\ncolnames(cor_melted)[3] &lt;- \"Coefficient de corrélation\"\n\n##-- Création de la heatmap\ncor_plot &lt;- ggplot(cor_melted, aes(x = Var1, y = Var2, fill = `Coefficient de corrélation`)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +  \n##-- Bleu pour négatif, rouge pour positif\n  theme_light() +\n  labs(x = \"Variables\",\n       y = \"Variables\") +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"),\n    axis.text.y = element_text(size = 10, face = \"bold\"),\n    axis.title.x = element_text(size = 10, face = \"bold\"),\n    axis.title.y = element_text(size = 10, face = \"bold\")\n  )\nggplotly(cor_plot)\n\n\n\nFigure 2 : Heatmap des Corrélations entre Variables Météorologiques\n\n\n\nlibrary(psych)\ndata_meteo &lt;- data[, 6:17]\nKMO(data_meteo)\n\nError in solve.default(r) : \n  system is computationally singular: reciprocal condition number = 2.76177e-18\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = data_meteo)\nOverall MSA =  0,5\nMSA for each item = \n           Température               Humidité           Pluviométrie \n                   0,5                    0,5                    0,5 \n       Vitesse du vent Pression atmosphérique      Indice de chaleur \n                   0,5                    0,5                    0,5 \n   Couverture nuageuse        Vent en hauteur              Indice UV \n                   0,5                    0,5                    0,5 \n  Température de l'eau     Humidité à l’ombre               Aérosols \n                   0,5                    0,5                    0,5 \n\n\n\n\nModélisation\n\nmodel &lt;- glm(`Cas palustres` ~ Température + `Taux de couverture` + Humidité + Pluviométrie + `Vitesse du vent` + `Pression atmosphérique` + Phase, \n             data = data, family = poisson())\n\n#summary(model, exponentiate = TRUE)\n\n\ntbl_regression(model, exponentiate = TRUE) %&gt;%\n  add_global_p() %&gt;%\n  modify_header(label = \"**Variables**\") %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(caption = capTab(\"Résultats de la régression de Poisson : Analyse des facteurs de risque\"))\n\n\n\n\n\n  Tableau 1 : Résultats de la régression de Poisson : Analyse des facteurs de risque\n  \n    \n      Variables\n      IRR\n      95% CI\n      p-value\n    \n  \n  \n    Température\n1,00\n1,00, 1,00\n0,002\n    Taux de couverture\n1,13\n1,07, 1,20\n&lt;0,001\n    Humidité\n1,01\n1,01, 1,01\n&lt;0,001\n    Pluviométrie\n1,00\n1,00, 1,00\n&lt;0,001\n    Vitesse du vent\n0,97\n0,97, 0,98\n&lt;0,001\n    Pression atmosphérique\n1,00\n1,00, 1,00\n&lt;0,001\n    Phase\n\n\n&lt;0,001\n        0\n—\n—\n\n        1\n0,41\n0,39, 0,42\n\n        2\n0,45\n0,43, 0,46\n\n        3\n0,49\n0,47, 0,51\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n    \n  \n  \n\n\n\n\n\nÉvaluation du modèle de Poisson\n\n      Ici le stepAIC permet de fournir une sélection de variables qui améliore le modèle (critère d’AIC que j’aborderai dans une autre publication). L’objectif est de minimiser l’AIC, donc nous devons conserver les variables dont la suppression entraîne une forte augmentation de l’AIC.\n\nDécision de suppression des variables\n\nLa sélection des variables repose sur leur impact sur l’AIC (Akaike Information Criterion). Plus l’AIC augmente après suppression d’une variable, plus cette dernière est importante pour le modèle. Les variables sont classées en deux groupes : celles à conserver absolument et celles qui ont un impact modéré. La fonction stepAIC permet de faire automatiquement la sélection des variables importante dans le modèle.\n\nmod1_poisson &lt;- stepAIC(model) \n\nStart:  AIC=22725,07\n`Cas palustres` ~ Température + `Taux de couverture` + Humidité + \n    Pluviométrie + `Vitesse du vent` + `Pression atmosphérique` + \n    Phase\n\n                           Df Deviance   AIC\n&lt;none&gt;                           16104 22725\n- Température               1    16114 22733\n- `Taux de couverture`      1    16121 22740\n- `Pression atmosphérique`  1    16132 22751\n- `Vitesse du vent`         1    16525 23144\n- Humidité                  1    17231 23850\n- Pluviométrie              1    18145 24763\n- Phase                     3    19109 25723\n\n\n\n\nA conserver absolument\n\n\nCes variables entraînent une forte augmentation de l’AIC si elles sont supprimées, ce qui indique qu’elles contribuent de manière significative à l’explication des cas palustres.\n\nPhase : +2998 d’AIC\nPluviométrie : +2038 d’AIC\nHumidité : +1125 d’AIC\n\n\n\nVariables modérément importantes\n\n\nCes variables ont un impact plus faible sur l’AIC et peuvent potentiellement être supprimées sans altérer significativement la qualité du modèle.\n\nVitesse du vent : +419 d’AIC\nPression atmosphérique : +26 d’AIC\nTaux de couverture : +15 d’AIC\nTempérature : +8 d’AIC\n\n\n\nDécision\n\n\nLes variables Phase, Pluviométrie et Humidité doivent impérativement être conservées, car leur suppression entraîne une augmentation très importante de l’AIC. En revanche, Vitesse du vent, Pression atmosphérique, Taux de couverture et Température ont un impact plus limité et peuvent être envisagées pour la suppression si nécessaire.\n\ntbl_regression(model, exponentiate = TRUE) %&gt;%\n  add_global_p() %&gt;%\n  modify_header(label = \"**Variables**\") %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(caption = capTab(\"Résultats de la régression de Poisson suite au stepAIC : Analyse des facteurs de risque\"))\n\n\n\n\n\n  Tableau 2 : Résultats de la régression de Poisson suite au stepAIC : Analyse des facteurs de risque\n  \n    \n      Variables\n      IRR\n      95% CI\n      p-value\n    \n  \n  \n    Température\n1,00\n1,00, 1,00\n0,002\n    Taux de couverture\n1,13\n1,07, 1,20\n&lt;0,001\n    Humidité\n1,01\n1,01, 1,01\n&lt;0,001\n    Pluviométrie\n1,00\n1,00, 1,00\n&lt;0,001\n    Vitesse du vent\n0,97\n0,97, 0,98\n&lt;0,001\n    Pression atmosphérique\n1,00\n1,00, 1,00\n&lt;0,001\n    Phase\n\n\n&lt;0,001\n        0\n—\n—\n\n        1\n0,41\n0,39, 0,42\n\n        2\n0,45\n0,43, 0,46\n\n        3\n0,49\n0,47, 0,51\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n    \n  \n  \n\n\n\n\n\npar(mfrow = c(2,2))\nplot(mod1_poisson)\n\n\n\n\nFigure 3 : Graphiques de diagnostic du modèle de poisson ajusté\n\n\n\n\n      L’analyse des diagnostics du modèle montre que les résidus de Pearson présentent une répartition aléatoire des points autour de zéro, suggérant l’absence de structure particulière dans les erreurs.\nDe plus, dans le graphique Q-Q, les points suivent approximativement la ligne diagonale, indiquant que les résidus sont normalement distribués, ce qui est un bon signe pour la validité des hypothèses du modèle.\nLa structure des erreurs standard de Pearson montre également une répartition équilibrée autour de la ligne rouge de référence, et un motif aléatoire est observé au niveau des écarts types de Pearson.\nTous ces éléments suggèrent une bonne adéquation du modèle aux données et confirment que les hypothèses sous-jacentes sont raisonnablement respectées.\n\n\nAnalyse de la surdispersion dans un modèle de Poisson\n\n\n\nmod1_poisson %&gt;% \n  performance::check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =    14.991\n  Pearson's Chi-Squared = 15441.190\n                p-value =   &lt; 0.001\n\n\n      Ce resultat suggère qu’il y’a surdispersion dans les données (p-values &lt; 0,05). Dans ce cas plusieurs alternatives sont possibles. Nous avons entre autres le modèle de regression binomiale négative qui est mélange de poisson-gamma et donc prend en compte un paramètre qui est celui de la dispersion. On a également le modèle quasi-poisson qui lui supprime la surdispersion présente dans les données à l’inverse du modèle binomial négatif qui l’estime.\n\n\nAlternative : Le modele binomial negative\n\n\n      En alternative au modèle de Poisson en cas de surdispersion, le modèle binomial négatif a été mentionné (Cameron and Trivedi 2013). En effet, ce modèle intègre un paramètre supplémentaire qui permet de mieux capturer la variabilité excessive des données, offrant ainsi une estimation plus fiable et adaptée aux situations où la variance des observations est supérieure à la moyenne.\n\nmodel_nb &lt;- glm.nb(`Cas palustres` ~ Température + `Taux de couverture` + Humidité + Pluviométrie + `Vitesse du vent` + `Pression atmosphérique` + Phase, \n                   data = data)\n\nmodel_nb &lt;- stepAIC(model_nb)\n\nStart:  AIC=10479,97\n`Cas palustres` ~ Température + `Taux de couverture` + Humidité + \n    Pluviométrie + `Vitesse du vent` + `Pression atmosphérique` + \n    Phase\n\n                           Df   AIC\n- Température               1 10478\n- `Pression atmosphérique`  1 10478\n- `Taux de couverture`      1 10479\n&lt;none&gt;                        10480\n- `Vitesse du vent`         1 10505\n- Humidité                  1 10544\n- Pluviométrie              1 10568\n- Phase                     3 10662\n\nStep:  AIC=10478,09\n`Cas palustres` ~ `Taux de couverture` + Humidité + Pluviométrie + \n    `Vitesse du vent` + `Pression atmosphérique` + Phase\n\n                           Df   AIC\n- `Pression atmosphérique`  1 10476\n- `Taux de couverture`      1 10477\n&lt;none&gt;                        10478\n- `Vitesse du vent`         1 10503\n- Humidité                  1 10547\n- Pluviométrie              1 10566\n- Phase                     3 10660\n\nStep:  AIC=10476,24\n`Cas palustres` ~ `Taux de couverture` + Humidité + Pluviométrie + \n    `Vitesse du vent` + Phase\n\n                       Df   AIC\n- `Taux de couverture`  1 10476\n&lt;none&gt;                    10476\n- `Vitesse du vent`     1 10502\n- Humidité              1 10545\n- Pluviométrie          1 10564\n- Phase                 3 10658\n\nStep:  AIC=10475,49\n`Cas palustres` ~ Humidité + Pluviométrie + `Vitesse du vent` + \n    Phase\n\n                    Df   AIC\n&lt;none&gt;                 10476\n- `Vitesse du vent`  1 10501\n- Humidité           1 10544\n- Pluviométrie       1 10563\n- Phase              3 11113\n\n\n\ntbl_regression(model_nb, exponentiate = TRUE) %&gt;%\n  add_global_p() %&gt;%\n  modify_header(label = \"**Variables**\") %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(caption = capTab(\"Résultats de la régression de Binomial négative : Analyse des facteurs de risque\"))\n\n\n\n\n\n  Tableau 3 : Résultats de la régression de Binomial négative : Analyse des facteurs de risque\n  \n    \n      Variables\n      IRR\n      95% CI\n      p-value\n    \n  \n  \n    Humidité\n1,01\n1,01, 1,01\n&lt;0,001\n    Pluviométrie\n1,00\n1,00, 1,00\n&lt;0,001\n    Vitesse du vent\n0,97\n0,96, 0,98\n&lt;0,001\n    Phase\n\n\n&lt;0,001\n        0\n—\n—\n\n        1\n0,44\n0,40, 0,47\n\n        2\n0,47\n0,45, 0,51\n\n        3\n0,51\n0,48, 0,55\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n    \n  \n  \n\n\n\n\n\n\nInterpretation des résultats\n\n\n\n\n\n\n\n\nIntervalles de confiances des variables météorologiques\n\n\n\nLes intervalles de confiance des variables météorologiques sont aussi petits car les données ont été générées. Et donc du coup avec de vraies données, il est possible de se retrouver avec des intervalles de confiance qui pourraient ne pas ressembler à ceux-ci.\n\n\n\nL’humidité augmente le nombre de cas de paludisme de 1% tandis que la vitesse de vent diminue le nombre de cas de paludisme de 3% (IC =[2% ; 4%]) toute chose étant égale par ailleur (l’influence des autres variables étant retirée).\nLa première phase d’interventions a permis de reduire le nombre de cas de paludisme de 56% (IC = [53% ; 60%]) par rapport à la phase 0 pendant laquelle il n’y avait pas encore d’intervention toute chose etant égale par ailleurs.\nLa seconde phase d’interventions a permis de reduire le nombre de cas de paludisme de 53% (IC = [49% ; 55%]) par rapport à la phase 0 pendant laquelle il n’y avait pas encore d’intervention toute chose etant égale par ailleurs.\nLa troisième phase d’interventions a permis de reduire le nombre de cas de paludisme de 49% (IC = [45% ; 52%]) par rapport à la phase 0 pendant laquelle il n’y avait pas encore d’intervention toute chose etant égale par ailleurs."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#annexes",
    "href": "FORMATIONS/poisson_paludisme.html#annexes",
    "title": "Modélisation des données de comptage",
    "section": "Annexes",
    "text": "Annexes\n\nDiagnostic du modèle binomial négatif\n\n\nAnalyse des résidus\n\n\n\npar(mfrow = c(2,2))\nplot(model_nb)\n\n\n\n\nFigure 4 : Graphiques de diagnostic du modèle binomial négatif ajusté\n\n\n\n\n\n\nMulticolinéarité du modèle binomial négatif\n\n\n\nplot(performance::check_collinearity(model_nb))\n\n\n\n\nFigure 5 : VIF du modèle binomial négatif\n\n\n\n\n      On remarque que toutes les variables ont un faible VIF &lt; 5. Cela suggère qu’il n’y a pas de multicolinéarité entre les variables utilisées dans le modèle.\n\n\nTest de Mann-Kendall\n      Ce test a été utilisé avec les alternatives unilatérales droite et gauche pour tester la présence de tendances strictement croissantes ou strictement décroissantes de la serie nombre de cas hebdomadire de paludisme dans chaque région d’etudes.\nHypothèses du test\n\\[\n\\begin{cases}\nH_0 : \\text{La série ne présente pas de tendance monotone (croissante ou décroissante).} \\\\\nH_1 : \\text{La série présente une tendance monotone (croissante ou décroissante).}\n\\end{cases}\n\\] Interprétation\n\nSi la p-value est inférieure au seuil de signification choisi (généralement 0,05),\nalors il y a suffisamment de preuves pour conclure que la série (nombre de cas de paludisme\nou incidences cumulées durant une phase) présente une tendance monotone.\n\nDans le cas contraire, on conclut que la série ne présente aucune tendance significative.\n\n\n\nDescription du modèle de Poisson\n      Soit (\\(Y\\)) le nombre de cas de paludisme hebdomadire Il s’agit d’une variable quantitative discrète prenant ses valeurs dans un intervalle défini. Supposons en outre que ces événements sont indépendants, c’est-à-dire que l’occurrence d’un premier cas n’affecte pas la probabilité d’en observer un autre.\nDans ce contexte, la variable (\\(Y\\)) suit une distribution de Poisson, avec un paramètre () représentant le taux moyen d’apparition d’un cas de paludisme. La probabilité d’observer une valeur donnée de (\\(Y\\)), en fonction de (), est exprimée par la formule suivante :\n\\[ P(Y = y) = \\frac{\\lambda^y}{y!} e^{-\\lambda} \\]\nLa distribution de Poisson n’a qu’un paramètre: () correspond à la fois à sa moyenne et à sa variance.\n\\[E(\\lambda) = V(\\lambda)\\] Le modèle de Poisson a été utilisé pour identifier les facteurs associés à la survenue du cas de paludisme, principalement en raison de la nature discrète de notre variable dépendante.\nLa régression de Poisson s’inscrit dans le cadre des modèles linéaires généralisés, où la variable réponse (\\(Y\\)) suit une distribution de Poisson :\n\\[ y \\sim \\text{Poisson}(\\lambda) \\]\nPuisque () doit être un nombre positif, nous utiliserons la fonction de logarithme comme lien avec le prédicteur linéaire.\n\\[ \\log{\\lambda} = \\eta = \\beta_0 + \\sum_{i = 1}^m \\beta_i x_i \\]\n\n\nEstimation des parametres\nL’estimation des paramètres d’un modèle de Poisson repose sur la méthode du maximum de vraisemblance (MV). Voici les étapes essentielles de l’estimation :\n. Fonction de Vraisemblance\nLa fonction de vraisemblance pour (n) observations est donnée par :\n\\[L(\\beta) = \\prod_{i=1}^{n} \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!}\\]\nEn prenant le logarithme, on obtient la log-vraisemblance :\n\\[\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\log(\\lambda_i) - \\lambda_i - \\log(y_i!) \\right]\\]\nEn remplaçant ( _i ) par ( e^{X_i } ), on obtient :\n\\[\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i (X_i \\beta) - e^{X_i \\beta} - \\log(y_i!) \\right]\\]\nEstimation par Maximum de Vraisemblance\nL’estimation des paramètres ( ) se fait en maximisant la log-vraisemblance. Comme il n’existe pas de solution analytique simple, on utilise des méthodes numériques telles que l’algorithme de Newton-Raphson ou la descente de gradient.\n\n\nAnalyse de la presence de surdispersion dans les données\nTel que mentionné plus haut, l’indépendance des observations est un prérequis du modèle de Poisson. Sa non-vérification peut entraîner une surdispersion des données. Cette surdispersion est quantifiée par un paramètre ( ) qui multiplie la variance attendue : pour une moyenne ( ), la variance devient donc ( ).\nPlus rarement, il peut arriver que ( &lt; 1 ), ce qui correspond à une sous-dispersion des observations. Contrairement à la surdispersion, où les observations ont tendance à être regroupées, la sous-dispersion traduit une répartition plus régulière que prévu.\nAfin de s’assurer de la pertinence du modèle choisi, une analyse de la surdispersion a été réalisée à l’aide du **test de surdispersion*. Les hypothèses du test étaient les suivantes :\n\nHypothèse nulle ((H_0)) : absence de surdispersion (le modèle de Poisson est approprié).\nHypothèse alternative ((H_1)) : présence de surdispersion (le modèle de Poisson n’est pas adapté).\n\nCritère de décision : Une p-value inférieure à 0,05 conduit au rejet de ( H_0), indiquant la présence d’une surdispersion et la nécessité d’envisager un modèle alternatif (comme le quasi-Poisson ou le Poisson négatif)."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#référence",
    "href": "FORMATIONS/poisson_paludisme.html#référence",
    "title": "Modélisation des données de comptage",
    "section": "Référence",
    "text": "Référence\n\n\nCameron, A. Colin, and Pravin K. Trivedi. 2013. Regression Analysis of Count Data. 2nd ed. Econometric Society Monographs. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9781139013567.\n\n\nMondiale de la Santé), OMS (Organisation. 2023. “World Malaria-report 2023-briefing-kit-fre.pdf.” https://cdn.who.int/media/docs/default-source/malaria/world-malaria-reports/wmr2022-regional-briefing-kit-fre.pdf?sfvrsn=7cb400ed_6&download=true.\n\n\n(OMS), Organisation MOndiale de la santé. 2023. “Global technical strategy for malaria 2016-2030.” https://iris.who.int/handle/10665/176712."
  },
  {
    "objectID": "FORMATIONS/presentations.html",
    "href": "FORMATIONS/presentations.html",
    "title": "Comment faire une présentation avec R et Quarto",
    "section": "",
    "text": "packages &lt;- c(\"ggplot2\",\"haven\", \"gtsummary\", \"corrr\", \"MASS\",\n              \"dplyr\",\"haven\", \"rstatix\", \"tidyverse\", \"ggpubr\",\n              \"glue\", \"dplyr\",\"ggspatial\", \"ggrepel\",\n              \"readxl\", \"stringr\", \"colorspace\") \n            \nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}"
  },
  {
    "objectID": "FORMATIONS/presentations.html#faire-ses-présentations-directement-avec-r-et-rstudio",
    "href": "FORMATIONS/presentations.html#faire-ses-présentations-directement-avec-r-et-rstudio",
    "title": "Comment faire une présentation avec R et Quarto",
    "section": "Faire ses présentations directement avec R et Rstudio",
    "text": "Faire ses présentations directement avec R et Rstudio\n\nPourquoi utiliser R et Rstudio pour ses présentations ?\n\n      R et RStudio offrent des outils puissants pour créer des présentations dynamiques, reproductibles et intégrées à vos analyses de données. Voici quelques raisons :\n\nIntégration parfaite des analyses et des présentations :\n\nNous pouvons combiner code, graphiques, tableaux et explications textuelles dans un seul document. Cela garantit une reproductibilité totale : les résultats sont automatiquement mis à jour si vos données changent.\n\nFlexibilité avec RMarkdown :\n\nCréez des présentations dans divers formats : HTML (slidy, reveal.js), PDF (Beamer), ou powerpoint ppt. Les formats sont hautement personnalisables pour répondre à vos besoins esthétiques et fonctionnels.\n\nSimplification du travail collaboratif :\n\nIl y’a une possibilité de garder un fichier .tex pour ceux qui sont à l’aise avec latex.\nVoir un exemple de présentation\n\n\nMaintenant allons-y !!!\n\n\n\n\n\nCommençons par une présentation revaljs\n\n\n\n\nInstaller les packages nécessaires\n\nAssurez-vous d’avoir le package revealjs installé. Si ce n’est pas le cas, installez-le avec :\ninstall.packages(\"revealjs\")\n\nCréer un fichier RMarkdown pour une présentation\n\nCréer un nouveau fichier RMarkdown :\n\nAllez dans : File &gt; New File &gt; Quarto presentation\nDans la fenêtre qui s’ouvre : Entrez un titre et un auteur. Dans l’option Default Output Format, choisissez From Template &gt; Revealjs Presentation.\n\n\nChanger l’en-tête YAML\n\nEn image voici, un descriptif visuel des 04 petites étapes pour la création du fichier avec des images :\n\n\n\n\n\n\nEtape 1\n\n\n\n\n\n\n\nEtape 2\n\n\n\n\n\n\n\n\n\nEtape 3\n\n\n\n\n\n\n\nEtape 4\n\n\n\n\n\n\n\n\n\nExplication de l’en-tête YAML"
  },
  {
    "objectID": "FORMATIONS/presentations.html#informations-générales",
    "href": "FORMATIONS/presentations.html#informations-générales",
    "title": "Comment faire une présentation avec R et Quarto",
    "section": "Informations générales",
    "text": "Informations générales\n\ntitle : Titre principal de la présentation\n\nIci : “ANALYSE EXPLORATOIRE DES DONNEES MTCARS”. C’est ce qui s’affiche en haut de la première diapositive.\n\nauthor : Nom(s) des présentateur(s)\n\nIci : “Presented by Djamal Toe”.\n\ninstitute : Institution ou organisation associée\n\nIci : “National School for Statistic and Data Analysis”.\n-date : Date de la présentation\nIci, elle est générée dynamiquement avec : 2025-06-28. Cela affichera automatiquement la date du jour où le fichier est tricoté."
  },
  {
    "objectID": "FORMATIONS/presentations.html#format-et-personnalisation-reveal.js",
    "href": "FORMATIONS/presentations.html#format-et-personnalisation-reveal.js",
    "title": "Comment faire une présentation avec R et Quarto",
    "section": "Format et personnalisation (reveal.js)",
    "text": "Format et personnalisation (reveal.js)\nLa section format: revealjs: contient des options spécifiques à la bibliothèque reveal.js, permettant de personnaliser la présentation.\n\nVitesse de transition:  transition-speed: fast définit la vitesse des transitions entre les diapositives. Options possibles : slow, normal, fast.\nAspect ratio :  aspect_ratio: \"16:9\" spécifie le ratio largeur/hauteur des diapositives. Le ratio “16:9” est idéal pour les écrans modernes (écran large). Autres options possibles : “4:3”, “3:2”, etc.\nMarges : margin: 0.02 définit l’espace vide autour du contenu de chaque diapositive. Une valeur faible (comme 0.02) maximise l’espace utilisé sur chaque diapositive.\nCentrage : center: true permet de Centrer le contenu verticalement et horizontalement sur chaque diapositive.\nPied de page : footer: “English classes with Milonnet” : Ajoute un texte en bas de chaque diapositive, comme une signature ou une note de contexte.\nLogo : logo: \"logo_ensai.png\" affiche un logo en haut à droite de chaque diapositive. L’image doit être placée dans le répertoire spécifié ou un chemin relatif correct doit être utilisé.\nCSS personnalisé : css: style.css permet d’utiliser un fichier CSS externe pour personnaliser les styles. Exemple : changer les polices, couleurs, tailles, etc. Le fichier style.css doit être dans le même répertoire ou le chemin approprié doit être indiqué.\nGestion des figure : fig_caption: yes active l’affichage des légendes sous les graphiques insérés.\nTable des matières (ToC) : toc: true active l’affichage d’une table des matières, toc-expand: false exige que les sections de la table des matières ne soient pas développées par défaut, toc-depth: 1 définit la profondeur de la hiérarchie affichée dans la table des matières (seulement les titres principaux #)."
  },
  {
    "objectID": "FORMATIONS/presentations.html#prévisualition",
    "href": "FORMATIONS/presentations.html#prévisualition",
    "title": "Comment faire une présentation avec R et Quarto",
    "section": "Prévisualition",
    "text": "Prévisualition\n      Pendant que vous faites la présentations sur Rstudio, vous pouvez la présualiser. Regardez les images ci-après :\n\n\n\n\n\n\nPrevisualisation : etape 1\n\n\n\n\n\n\n\nPrevisualisation : etape 2\n\n\n\n\n\n\n\n\n\nCompilation et Previsualisation : etape 3\n\n\n\n\n\n\n\n\n\n\n\nViewer ou Presenation ?\n\n\n\nA l’étape 2 de la prévisualisation, il se peut que la prévisualisation apparaisse dans la partie Presentation juste à droite de l’onglet Viewer encerclé en rouge sur l’image."
  },
  {
    "objectID": "FORMATIONS/presentations.html#mise-en-forme-avec-le-fichier-css",
    "href": "FORMATIONS/presentations.html#mise-en-forme-avec-le-fichier-css",
    "title": "Comment faire une présentation avec R et Quarto",
    "section": "Mise en forme avec le fichier CSS",
    "text": "Mise en forme avec le fichier CSS\n      Pour cette section ne vous inquietez pas si vous n’avez pas de connaissance en html ou en css, nous utiliserons juste un code css pour la mise en forme du titre."
  },
  {
    "objectID": "FORMATIONS/presentations.html#télécharger-le-fichier-de-la-présentation",
    "href": "FORMATIONS/presentations.html#télécharger-le-fichier-de-la-présentation",
    "title": "Comment faire une présentation avec R et Quarto",
    "section": "Télécharger le fichier de la présentation",
    "text": "Télécharger le fichier de la présentation\nAvant de télécharger le fichier, vous pouvez voir ce qu’il donne en cliquant sur ce lien\nVous pouvez télécharger le fichier d’analyse exploratoire des données mtcars au format .qmd ci-dessous.\nTélécharger le fichier .qmd\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour à la page d’accueuil"
  },
  {
    "objectID": "FORMATIONS/SIG.html",
    "href": "FORMATIONS/SIG.html",
    "title": "Création des cartes chloropètres avec R",
    "section": "",
    "text": "packages &lt;- c(\"ggplot2\",\"haven\", \"gtsummary\", \"corrr\", \"MASS\",\n              \"dplyr\",\"haven\", \"rstatix\", \"tidyverse\", \"ggpubr\",\n              \"glue\", \"dplyr\",\"ggspatial\", \"ggrepel\",\"marmap\", \n              \"readxl\", \"stringr\", \"colorspace\", \"sf\", \"viridis\",\n              \"tools\",\"ggspatial\",\"readxl\",\"openxlsx\",\"grid\",\n              \"outliers\",\"car\",\"ftExtra\",\"tibble\",\n              \"gtsummary\", \"wesanderson\", \"viridis\",\n              \"RColorBrewer\", \"knitr\", \"kableExtra\") \n            \nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}"
  },
  {
    "objectID": "FORMATIONS/SIG.html#comment-faire-des-cartes-choroplèthes-et-des-cartes-de-proportions-avec-r",
    "href": "FORMATIONS/SIG.html#comment-faire-des-cartes-choroplèthes-et-des-cartes-de-proportions-avec-r",
    "title": "Création des cartes chloropètres avec R",
    "section": "Comment faire des cartes Choroplèthes et des cartes de proportions avec R ?",
    "text": "Comment faire des cartes Choroplèthes et des cartes de proportions avec R ?\n      Les cartes choroplèthes et les cartes de proportions sont des outils puissants pour visualiser des données géospatiales dans R. Ces cartes permettent de représenter des valeurs quantitatives (par exemple, des taux de population, des moyennes) sur des zones géographiques, souvent des régions administratives comme des départements, des communes, ou des zones géographiques personnalisées.\n\nIntroduction aux Cartes Choroplèthes et Cartes de Proportions\n\nLes cartes choroplèthes colorient les régions géographiques en fonction de valeurs numériques ou de proportions, facilitant l’analyse spatiale et la compréhension des variations géographiques. Elles sont couramment utilisées pour des données socio-économiques, de santé publique, ou des analyses environnementales.\nLes cartes de proportions sont similaires mais mettent davantage l’accent sur les ratios ou proportions par rapport à une valeur totale, comme des pourcentages ou des fractions de populations.\n\nNotions de Base : Polygones, Shapefiles et Coordonnées Avant de créer ces cartes, il est important de comprendre quelques notions de base, comme les polygones et les shapefiles :\n\n\n\n\n\n\n\nPolygones\n\n\n\nUne zone géographique est souvent représentée par un polygone, une forme géométrique fermée qui peut avoir plusieurs côtés. Par exemple, une commune ou un département sur une carte peut être représentée comme un polygone.\n\n\n\n\n\n\n\n\nShapefiles\n\n\n\nCe sont un format de fichier standard pour stocker des informations géospatiales, y compris les coordonnées de points, de lignes et de polygones. Ils peuvent contenir les géométries des entités géographiques ainsi que leurs attributs (valeurs associées à chaque région, comme le revenu moyen ou le taux de chômage).\n\n\n\n\n\n\n\n\nCoordonnées géographiques\n\n\n\nLes coordonnées (latitude et longitude) permettent de positionner ces polygones sur une carte. En R, on utilise des systèmes de coordonnées géographiques et projetées pour gérer et visualiser ces données.\n\n\nPlusieurs pakages permettent de visualiser les données avec les cartes, ici nous interessons aux packages glue et sf.\n\nZone d’étude\n\nSupposons que nous menions une étude au Burkina-Faso. Par exemple, nous mésurer des indicateurs tels que le taux de mortalité, la couverture sanitaire etc … Le Burkina Faso est un pays qui compte 13 regions, mais notre etude s’étend seulement sur 8 regions. Il convient de montrer toutes les regions, puis de mettre en exègue celles qui nous concernent.\n\nPlace au code\n\n\n\nvoir/cacher le code\n\n\n###---- Chargement des shapefiles src = GADM\nroot &lt;- getwd() ##-- la racine du repertoire\n\n##- La carte du pays sans les polygones des regions, communes et/ou departements\npath0 &lt;- paste0(root,\"/DATA_SIG/BFA2/gadm41_BFA_0.shp\")\n\n##- La carte du pays avec le polygone des regions, sans ceux des communes et/ou departements\npath1 &lt;- paste0(root,\"/DATA_SIG/BFA2/gadm41_BFA_1.shp\")\n\n##- La carte du pays avec le polygone des regions, sans ceux des communes et/ou departements\npath2 &lt;- paste0(\"/DATA_SIG/BFA2/gadm41_BFA_2.shp\")\n\n##- La carte du pays avec le polygone des regions, sans ceux des communes et/ou departements\npath3 &lt;- paste0(root,\"/DATA_SIG/BFA2/gadm41_BFA_3.shp\")\n\n\n##-- selection des regions concernées\n\nstudy.area &lt;-  c(\"Boucle du Mouhoun\", \"Centre-Est\", \"Centre-Nord\",\n             \"Centre-Ouest\", \"Nord\", \"Sud-Ouest\",\n             \"Haut-Bassins\", \"Cascades\")\n\n##-- lecture des shapefiles\npays_shp &lt;- read_sf(glue(path0), quiet = T)\nregion_shp &lt;- read_sf(glue(path1), quiet = T)\n#commune_shp &lt;- read_sf(glue(path2), quiet = T)\n#province_shp &lt;- read_sf(glue(path3), quiet = T)\n\n##-- création d'une sous base avec les polygones des regions sélectionnés\n\ndata_region &lt;- region_shp %&gt;% filter(NAME_1 %in% study.area)\n\n\n##-- Study area colors\nstudy_zone_colors &lt;- c(\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\",\n                       \"#3FE1B8\", \"#9467bd\", \"#8c564b\",\n                       \"#00008B\", \"#4B0082\")\n\nstudy_zone_map &lt;- ggplot() +\n  geom_sf(data = pays_shp, aes(linewidth = \"Burkina Faso\"),fill = \"white\", color = \"black\") +\n  geom_sf(data = region_shp, aes(fill = ifelse(\n    NAME_1 %in% study.area,\n    \"Regions d'études\",\n    \"Autres regions\"\n  ) )) +\n  geom_sf_text(data = region_shp, aes(label = ifelse(\n    NAME_1 %in% study.area,\n    study.area,\n    \"\"\n  )), size = 4)+\n  ggspatial::annotation_scale(\n    location = \"br\",\n    bar_cols = c(\"black\", \"white\")\n  )  +\n  theme_light()+\n  ggspatial::annotation_north_arrow(\n    location = \"tr\", which_north = \"true\",\n    pad_x = unit(0.05, \"in\"), pad_y = unit(0.05, \"in\"),\n    style = ggspatial::north_arrow_nautical(\n      fill = c(\"black\", \"white\"),\n      line_col = \"black\"\n    )\n  )+\n  xlab(\"\")+\n  ylab(\"\")+\n  scale_linewidth_manual(values = c(1.2), name = \"\")+\n  scale_fill_manual(values = c(\"white\",\"#1f77b4\"), name=\"Zone d'étude\")+\n  theme_light() + \n  guides(\n    linewidth = guide_legend(order = 1),\n    fill = guide_legend(order = 2),\n    color = guide_legend(order = 3)\n  )\n\n\n\nstudy_zone_map\n\n\n\n\nCartographie de la zone d’étude\n\n\n\n\n\nExpliquons le code à présent\n\n\nCharger les fichier shapefiles :\n\nglue : pour preparer la structure du format (optionnel)\nreadsf : pour lire les fichiers shapefiles\n\nDefinir la zone d’étude : les fichier shapefile devient comme un dataframe, donc est manipulable au même titre que les fichiers excel, csv etc …\nOn trace d’abord la carte du pays, ensuite on ajoute la couche des regions (c’est-à-dire le shapefile des regions). On pourrait le faire simplement avec le shapefile des regions sans celui du pays.\nEnsuite on ajoute la couleur pour la zone concernée et les noms des regions sélectionnées avec geom_sf_text\nannotation_scale permet d’ajouter une barre d’échelle (scale bar) à une carte avec la position br pour dire bottom rigth (en bas à droite)\nannotation_north_arrow est utilisée pour ajouter une flèche du nord sur une carte créée avec ggplot2\nPour le reste il s’agit des fonctions qu’on utilise couramment avec ggplot2\n\n\n\nAfficher/Masquer le tableau\n\n\n\n\n\nTableau 1 : Les 10 premières lignes du shapefile\n\n\nGID_1\nGID_0\nCOUNTRY\nNAME_1\nVARNAME_1\nNL_NAME_1\nTYPE_1\nENGTYPE_1\nCC_1\nHASC_1\nISO_1\ngeometry\n\n\n\n\nBFA.1_1\nBFA\nBurkina Faso\nBoucle du Mouhoun\nNA\nNA\nRégion\nRegion\nNA\nBF.BO\nNA\nPOLYGON ((-2,73901 11,71249...\n\n\nBFA.2_1\nBFA\nBurkina Faso\nCascades\nNA\nNA\nRégion\nRegion\nNA\nBF.CD\nNA\nPOLYGON ((-4,591742 9,70225...\n\n\nBFA.7_1\nBFA\nBurkina Faso\nCentre\nNA\nNA\nRégion\nRegion\nNA\nBF.CT\nNA\nPOLYGON ((-1,2786 12,13921,...\n\n\nBFA.3_1\nBFA\nBurkina Faso\nCentre-Est\nNA\nNA\nRégion\nRegion\nNA\nBF.CE\nNA\nPOLYGON ((0,4371 11,67655, ...\n\n\nBFA.4_1\nBFA\nBurkina Faso\nCentre-Nord\nNA\nNA\nRégion\nRegion\nNA\nBF.CN\nNA\nPOLYGON ((-0,7773 12,66989,...\n\n\nBFA.5_1\nBFA\nBurkina Faso\nCentre-Ouest\nNA\nNA\nRégion\nRegion\nNA\nBF.CO\nNA\nPOLYGON ((-2,360162 11,0081...\n\n\nBFA.6_1\nBFA\nBurkina Faso\nCentre-Sud\nNA\nNA\nRégion\nRegion\nNA\nBF.CS\nNA\nPOLYGON ((-0,8624911 10,985...\n\n\nBFA.8_1\nBFA\nBurkina Faso\nEst\nNA\nNA\nRégion\nRegion\nNA\nBF.ES\nNA\nPOLYGON ((1,384436 11,44223...\n\n\nBFA.9_1\nBFA\nBurkina Faso\nHaut-Bassins\nNA\nNA\nRégion\nRegion\nNA\nBF.HB\nNA\nPOLYGON ((-4,08994 10,79044...\n\n\nBFA.10_1\nBFA\nBurkina Faso\nNord\nNA\nNA\nRégion\nRegion\nNA\nBF.NO\nNA\nPOLYGON ((-1,96586 12,67774...\n\n\n\na Source des données : GADM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCartes choroplèthes\n\n      Les cartes choroplèthes sont des représentations graphiques qui utilisent des nuances de couleurs pour illustrer des données quantitatives ou qualitatives sur des zones géographiques. Chaque zone est remplie d’une couleur qui correspond à une valeur spécifique ou à une plage de valeurs, facilitant ainsi l’analyse des variations spatiales des données.\nLes cartes choroplèthes sont idéales pour représenter des indicateurs comme le taux de mortalité, le revenu moyen, l’accès à l’eau potable, ou encore la couverture sanitaire par région.\n\nExemple de carte choroplèthe\nDans cet exemple, nous allons créer une carte choroplèthe montrant la couverture sanitaire par région au Burkina Faso, en utilisant les données fictives créées plus haut. pour les données, vous pouvez me contacter par email.\n\nEtape 1 : Charger les shapefiles et les données\n\nIci nous nous assurons que les shapefiles des régions et les données sont correctement chargés et liés entre eux. Pour cela on fait une jointure externe.\n\n##-- Joindre les données au shapefile\nregion_data &lt;- region_shp %&gt;% \n  left_join(data, by = c(\"NAME_1\" = \"Region\"))\n\nAvant de passer à l’étape 2, affichons les données générées avant jointure et ceux aprés jointures.\n\n\nAfficher/cacher le code\n\n\ntbl.avant.jointure &lt;- kbl(head(data,10)) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\ntbl.apres.jointure &lt;- kbl(head(data,10)) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;% add_footnote(label = \"Source des données :  GADM\")\n\n\n\n\nAfficher/Masquer le tableau\n\n\ntbl.avant.jointure\ntbl.apres.jointure\n\n\nLes 10 premières lignes des tables\n\n\n\n\n\nAvant jointure\n\n\nRegion\nPopulation\nTaux_Mortalite\nCouverture_Sanitaire\nAcces_Eau_Potable\n\n\n\n\nBoucle du Mouhoun\n1648219\n14,20\n65,77\n61,05\n\n\nCascades\n1466741\n8,99\n61,91\n76,19\n\n\nCentre\n843324\n14,27\n93,54\n55,47\n\n\nCentre-Est\n611816\n5,07\n62,63\n76,53\n\n\nCentre-Nord\n682418\n8,39\n93,94\n74,00\n\n\nCentre-Ouest\n999248\n11,59\n79,76\n56,87\n\n\nCentre-Sud\n1516420\n12,38\n63,39\n83,94\n\n\nEst\n789724\n5,15\n60,75\n88,57\n\n\nHauts-Bassins\n2329489\n7,37\n81,76\n55,39\n\n\nNord\n1684168\n15,00\n67,64\n67,45\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAprès jointure\n\n\nRegion\nPopulation\nTaux_Mortalite\nCouverture_Sanitaire\nAcces_Eau_Potable\n\n\n\n\nBoucle du Mouhoun\n1648219\n14,20\n65,77\n61,05\n\n\nCascades\n1466741\n8,99\n61,91\n76,19\n\n\nCentre\n843324\n14,27\n93,54\n55,47\n\n\nCentre-Est\n611816\n5,07\n62,63\n76,53\n\n\nCentre-Nord\n682418\n8,39\n93,94\n74,00\n\n\nCentre-Ouest\n999248\n11,59\n79,76\n56,87\n\n\nCentre-Sud\n1516420\n12,38\n63,39\n83,94\n\n\nEst\n789724\n5,15\n60,75\n88,57\n\n\nHauts-Bassins\n2329489\n7,37\n81,76\n55,39\n\n\nNord\n1684168\n15,00\n67,64\n67,45\n\n\n\na Source des données : GADM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEtape 2 : Créer la carte choroplèthe\n\nUtilisez ggplot2 et geom_sf() pour afficher les régions et les colorer en fonction de la couverture sanitaire.\n\n##-  Carte choroplèthe\nchoropleth_map &lt;- ggplot(region_data) +\n  geom_sf(aes(fill = Couverture_Sanitaire), color = \"black\") +\n  scale_fill_viridis_c(\n    option = \"C\",\n    name = \"Couverture Sanitaire (%)\"\n  ) +\n  ggtitle(\"Carte choroplèthe : Couverture sanitaire par région\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0.5, face = \"bold\")\n  )\n\nchoropleth_map\n\n\n\n\nCouverture sanitaire par région\n\n\n\n\n\nEtape 3 :  Ajouter des éléments décoratifs\n\nAjoutons une barre d’échelle et une flèche du nord pour rendre la carte plus informative.\n\n##- Ajout des éléments décoratifs\nchoropleth_map &lt;- choropleth_map +\n  ggspatial::annotation_scale(location = \"br\") +\n  ggspatial::annotation_north_arrow(\n    location = \"tl\", style = north_arrow_nautical()\n  ) ###-- tl pour top-left (en haut à gauche)\n\nchoropleth_map\n\n\n\n\n\nInterpréter les résultats\n\nExaminez la carte générée et répondez aux questions suivantes : - Quelles régions ont la meilleure couverture sanitaire ? - Quelles régions doivent faire l’objet d’une attention particulière pour améliorer les conditions de vie ?\n\nExtensions possibles\n\nRéalisez une carte choroplèthe pour le taux de mortalité.\nAjoutez des annotations pour les régions ayant les valeurs extrêmes.\nExpérimentez avec d’autres palettes de couleurs en utilisant scale_fill_brewer() ou scale_fill_manual() etc ….\n\n\n\n\n\n\n\nDonnées discrètes ?\n\n\n\nIl se peut qu’il n’y ait pas une variabilité importante dans les données dans ce cas, au lieu d’avoir une palette, nous aurons juste des cases de couleurs comme s’agissait d’un indicateur discrèt. Dans ce cas, recoder juste cet indicateur en un indicateur qualitatif (regrouper par classe) et ensuite utiliser scale_fill_manual() pour definir vos couleurs manuellement ou laisser R le faire tout seul. Le graphique ci-dessous en est un exemple.\n\n\n[Exemple de carte avec un indicateur recodé : Indisponible pour l’instant]\n\nCartes de proportions\n\n\n\n\nA suivre\n\n\n\n\nCartes de proportions avancées\n\n\n\n\n\n\nRetour à la page d’accueuil"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistiques, Data Science & Projets Numériques - Djamal TOE",
    "section": "",
    "text": "Bienvenue sur le site Djamal ESI, vitrine de mes explorations en statistiques, science des données et développement informatique. Vous y trouverez mes projets, recherches, publications et outils interactifs.\n\n👉 À propos de moi\n\n\n\n\nJe suis Djamal Y. TOE, élève-ingénieur à l’ENSAI, passionné par l’analyse quantitative, la programmation et les applications de l’intelligence artificielle.\nMes centres d’intérêt gravitent principalement autour du machine learning, de la biostatistique et de la gestion des risques.\nCurieux et ouvert, je reste aussi attentif à d’autres domaines d’application où mes compétences en science des données peuvent avoir un impact concret.\n\n\n\n\nRetrouvez l’ensemble de mes publications classées par thèmes (Cliquez sur la catégorie de publications que vous souhaitez consulter):\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\nStatistiques\n\n\nModélisation statistiques\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nClustering\n\n\nEDA\n\n\nReduction de dimensionalité\n\n\n\nÉtudes appliquées aux domaines de la santé, de la finance et du sport à travers des techniques statistiques et de machine learning interprétables : régression, clustering…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJava\n\n\nPython\n\n\nMySql\n\n\nAPI vocales\n\n\nJavaFX\n\n\nProgrammation orientée objet\n\n\n\nDéveloppement d’applications interactives (bureaux, vocales, connectées aux bases de données) en Java et Python, avec une attention à la modularité et à l’expérience…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nLeaflet\n\n\nsf\n\n\ntmap\n\n\nGeopandas\n\n\nFolium\n\n\nggplot2\n\n\nmatplotlib\n\n\nseaborn\n\n\n\nReprésentation visuelle de données complexes via des cartes et graphiques interactifs, notamment en santé publique et socio-économie, avec R et Python.\n\n\n\n\n\n\n\nNo matching items\n\n\n\n🔗 Accéder à la page complète : Mes publications\n\n\n\nD’autres projets seront bientôt disponibles sur le site.\n\n\n\n\n\nPour toute question, collaboration ou échange professionnel :\n\n💼 Djamal TOE sur LinkedIn\n\n\n© 2024 – Djamal DEV | Rennes, France"
  },
  {
    "objectID": "index.html#qui-suis-je",
    "href": "index.html#qui-suis-je",
    "title": "Statistiques, Data Science & Projets Numériques - Djamal TOE",
    "section": "",
    "text": "Je suis Djamal Y. TOE, élève-ingénieur à l’ENSAI, passionné par l’analyse quantitative, la programmation et les applications de l’intelligence artificielle.\nMes centres d’intérêt gravitent principalement autour du machine learning, de la biostatistique et de la gestion des risques.\nCurieux et ouvert, je reste aussi attentif à d’autres domaines d’application où mes compétences en science des données peuvent avoir un impact concret."
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Statistiques, Data Science & Projets Numériques - Djamal TOE",
    "section": "",
    "text": "Retrouvez l’ensemble de mes publications classées par thèmes (Cliquez sur la catégorie de publications que vous souhaitez consulter):\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\nStatistiques\n\n\nModélisation statistiques\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nClustering\n\n\nEDA\n\n\nReduction de dimensionalité\n\n\n\nÉtudes appliquées aux domaines de la santé, de la finance et du sport à travers des techniques statistiques et de machine learning interprétables : régression, clustering…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJava\n\n\nPython\n\n\nMySql\n\n\nAPI vocales\n\n\nJavaFX\n\n\nProgrammation orientée objet\n\n\n\nDéveloppement d’applications interactives (bureaux, vocales, connectées aux bases de données) en Java et Python, avec une attention à la modularité et à l’expérience…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nLeaflet\n\n\nsf\n\n\ntmap\n\n\nGeopandas\n\n\nFolium\n\n\nggplot2\n\n\nmatplotlib\n\n\nseaborn\n\n\n\nReprésentation visuelle de données complexes via des cartes et graphiques interactifs, notamment en santé publique et socio-économie, avec R et Python.\n\n\n\n\n\n\n\nNo matching items\n\n\n\n🔗 Accéder à la page complète : Mes publications\n\n\n\nD’autres projets seront bientôt disponibles sur le site."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Statistiques, Data Science & Projets Numériques - Djamal TOE",
    "section": "",
    "text": "Pour toute question, collaboration ou échange professionnel :\n\n💼 Djamal TOE sur LinkedIn\n\n\n© 2024 – Djamal DEV | Rennes, France"
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "",
    "text": "La détection de fraudes sur les transactions bancaires est un enjeu majeur pour les institutions financières. Les méthodes traditionnelles basées sur des règles statiques peinent à s’adapter aux schémas de fraude de plus en plus sophistiqués. Dès 2002, Bolton & Hand ont proposé un modèle statistique pour identifier les anomalies transactionnelles (Bolton and Hand 2002). Depuis, de nombreuses approches ont émergé, à la fois supervisées et non supervisées, offrant des performances variables selon la disponibilité de données étiquetées, la diversité des comportements normaux et la capacité à détecter de nouvelles formes de fraude (Phua et al. 2010; Ngai et al. 2011; Chalapathy and Chawla 2019).\n\nNote\nLe notebook ainsi que toutes ses dépendances sont disponibles sur GitHub :\nCliquez ici pour cccéder au dépôt"
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#introduction",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#introduction",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "",
    "text": "La détection de fraudes sur les transactions bancaires est un enjeu majeur pour les institutions financières. Les méthodes traditionnelles basées sur des règles statiques peinent à s’adapter aux schémas de fraude de plus en plus sophistiqués. Dès 2002, Bolton & Hand ont proposé un modèle statistique pour identifier les anomalies transactionnelles (Bolton and Hand 2002). Depuis, de nombreuses approches ont émergé, à la fois supervisées et non supervisées, offrant des performances variables selon la disponibilité de données étiquetées, la diversité des comportements normaux et la capacité à détecter de nouvelles formes de fraude (Phua et al. 2010; Ngai et al. 2011; Chalapathy and Chawla 2019).\n\nNote\nLe notebook ainsi que toutes ses dépendances sont disponibles sur GitHub :\nCliquez ici pour cccéder au dépôt"
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#méthodologie",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#méthodologie",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "Méthodologie",
    "text": "Méthodologie\n      Pour ce mini-projet, nous adopterons une approche non supervisée utilisant un Gaussian Mixture Model (GMM). Le GMM permet de modéliser la distribution sous-jacente des transactions légitimes par une combinaison de gaussiennes, et d’identifier les observations présentant une faible vraisemblance comme anomalies (Bishop 2006).\nLes étapes principales sont:\n\nPrétraitement des données:\n\nSélection des variables pertinentes (montant, temporalité, etc.);\nNettoyage, transformation et mise à l’échelle (Hastie, Tibshirani, and Friedman 2009)\n\nEstimation du GMM:\n\nChoix du nombre de composantes par critères AIC/BIC (Schwarz 1978)\nAjustement du modèle sur les données normalisées\n\nDétection des anomalies:\n\nCalcul de la log-vraisemblance pour chaque transaction\nDéfinition d’un seuil basé sur un percentile (par exemple 1%) pour isoler les transactions suspectes (Ngai et al. 2011)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#pratique",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#pratique",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "Pratique",
    "text": "Pratique\n\n\n   Unnamed: 0 TransactionID  ... AccountBalance  PreviousTransactionDate\n0           0      TX000001  ...        5112.21      2024-11-04 08:08:08\n1           1      TX000002  ...       13758.91      2024-11-04 08:09:35\n2           2      TX000003  ...        1122.35      2024-11-04 08:07:04\n3           3      TX000004  ...        8569.06      2024-11-04 08:09:06\n4           4      TX000005  ...        7429.40      2024-11-04 08:06:39\n\n[5 rows x 17 columns]\n\n\nAbout Dataset (From kaggle (link))\n\\(\\quad\\) This dataset provides a detailed look into transactional behavior and financial activity patterns, ideal for exploring fraud detection and anomaly identification. It contains 2,512 samples of transaction data, covering various transaction attributes, customer demographics, and usage patterns. Each entry offers comprehensive insights into transaction behavior, enabling analysis for financial security and fraud detection applications.\nKey Features:\n\nTransactionID: Unique alphanumeric identifier for each transaction.\nAccountID: Unique identifier for each account, with multiple transactions per account.\nTransactionAmount: Monetary value of each transaction, ranging from small everyday expenses to larger purchases.\nTransactionDate: Timestamp of each transaction, capturing date and time.\nTransactionType: Categorical field indicating ‘Credit’ or ‘Debit’ transactions.\nLocation: Geographic location of the transaction, represented by U.S. city names.\nDeviceID: Alphanumeric identifier for devices used to perform the transaction.\nIP Address: IPv4 address associated with the transaction, with occasional changes for some accounts.\nMerchantID: Unique identifier for merchants, showing preferred and outlier merchants for each account.\nAccountBalance: Balance in the account post-transaction, with logical correlations based on transaction type and amount.\nPreviousTransactionDate: Timestamp of the last transaction for the account, aiding in calculating transaction frequency.\nChannel: Channel through which the transaction was performed (e.g., Online, ATM, Branch).\nCustomerAge: Age of the account holder, with logical groupings based on occupation.\nCustomerOccupation: Occupation of the account holder (e.g., Doctor, Engineer, Student, Retired), reflecting income patterns.\nTransactionDuration: Duration of the transaction in seconds, varying by transaction type.\nLoginAttempts: Number of login attempts before the transaction, with higher values indicating potential anomalies.\n\nThis dataset is ideal for data scientists, financial analysts, and researchers looking to analyze transactional patterns, detect fraud, and build predictive models for financial security applications. The dataset was designed for machine learning and pattern analysis tasks and is not intended as a primary data source for academic publications.\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2512 entries, 0 to 2511\nData columns (total 17 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Unnamed: 0               2512 non-null   int64  \n 1   TransactionID            2512 non-null   object \n 2   AccountID                2512 non-null   object \n 3   TransactionAmount        2512 non-null   float64\n 4   TransactionDate          2512 non-null   object \n 5   TransactionType          2512 non-null   object \n 6   Location                 2512 non-null   object \n 7   DeviceID                 2512 non-null   object \n 8   IP Address               2512 non-null   object \n 9   MerchantID               2512 non-null   object \n 10  Channel                  2512 non-null   object \n 11  CustomerAge              2512 non-null   int64  \n 12  CustomerOccupation       2512 non-null   object \n 13  TransactionDuration      2512 non-null   int64  \n 14  LoginAttempts            2512 non-null   int64  \n 15  AccountBalance           2512 non-null   float64\n 16  PreviousTransactionDate  2512 non-null   object \ndtypes: float64(2), int64(4), object(11)\nmemory usage: 333.8+ KB\n\n\n\n\nUnnamed: 0                 0\nTransactionID              0\nAccountID                  0\nTransactionAmount          0\nTransactionDate            0\nTransactionType            0\nLocation                   0\nDeviceID                   0\nIP Address                 0\nMerchantID                 0\nChannel                    0\nCustomerAge                0\nCustomerOccupation         0\nTransactionDuration        0\nLoginAttempts              0\nAccountBalance             0\nPreviousTransactionDate    0\ndtype: int64\n\n\n\\(\\quad\\) Certaines variables, bien que nettoyées et sans valeurs manquantes, n’ont pas été exploitées dans l’analyse principale :\n- TransactionID\n- AccountID\n- DeviceID\n- IP Address\n- MerchantID\nToutefois, selon l’objectif visé, certaines d’entre elles pourraient s’avérer très pertinentes:\n- AccountID: détection de comptes à risque, suivi des comportements de chaque titulaire,\n- IP Address: analyse spatiale et traçage géographique des connexions,\n- MerchantID: étude du comportement des commerçants et détection d’anomalies spécifiques à certains points de vente.\nPour des raisons éthiques, ces détails ne seront pas explorées dans le cadre de cet mini-projet, car nous ne savons pas si les ID sont réels ou pas.\n\n\n   TransactionAmount  ... PreviousTransactionDate\n0              14.09  ...     2024-11-04 08:08:08\n1             376.24  ...     2024-11-04 08:09:35\n2             126.29  ...     2024-11-04 08:07:04\n3             184.50  ...     2024-11-04 08:09:06\n4              13.45  ...     2024-11-04 08:06:39\n\n[5 rows x 11 columns]\n\n\n      Les variables TransactionDate et PreviousTransactionDate peuvent nous servir à calculer une variables plus informative et utilisable qui est TimeBetweenThisTransactionAndTheLastOne qui pourrait être en heures ou en secondes en fonction des valeurs obtenues de la différence entre ces deux dates.\n\n\n   TransactionAmount  ... TimeBetweenThisTransactionAndTheLastOne\n0              14.09  ...                                13743.65\n1             376.24  ...                                11895.42\n2             126.29  ...                                11581.85\n3             184.50  ...                                13167.62\n4              13.45  ...                                 9230.25\n\n[5 rows x 10 columns]\n\n\n\n\ncount     2512.000000\nmean     11699.619928\nstd       2553.112387\nmin       7381.750000\n25%       9469.435000\n50%      11654.350000\n75%      13935.792500\nmax      16120.190000\nName: TimeBetweenThisTransactionAndTheLastOne, dtype: float64\n\n\nOn pourrait même convertir cela en nombre de jours car le minimum est de 7381.75 heures.\n\n\n   TransactionAmount  ... DaysBetweenThisTransactionAndTheLastOne\n0              14.09  ...                                   573.0\n1             376.24  ...                                   496.0\n2             126.29  ...                                   483.0\n3             184.50  ...                                   549.0\n4              13.45  ...                                   385.0\n\n[5 rows x 10 columns]\n\n\n\n\ncount    2512.000000\nmean      487.857882\nstd       106.378910\nmin       308.000000\n25%       395.000000\n50%       486.000000\n75%       581.000000\nmax       672.000000\nName: DaysBetweenThisTransactionAndTheLastOne, dtype: float64\n\n\n      Il est apparaît surprenant que qu’il y’ai autant de jour entre deux transaction. Nous n’avons pas fait d’erreur de calcul car si vous monter un peu plus haut et en regardant les colonnes TransactionDate and PreviousTransactionDate vous verrez que cet écart peut s’expliquer par diverses raisons sauf par une erreur de calcul de notre part.\nimport math\nvar_list = list(df_reduced.columns.values)\nfigsize = (12, 8)\nn_vars = len(var_list)\n# calcul automatique du nombre de colonnes si non fourni\nncols = int(math.ceil(math.sqrt(n_vars)))\nnrows = int(math.ceil(n_vars / ncols))\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\naxes = axes.flatten() \nplt.subplots_adjust(hspace=0.4, wspace=0.4)\nbins_ = 20\nfor i in range(len(var_list)):\n    var_name = var_list[i]\n    if df_reduced[var_name].dtype in ['categorical', 'object']:\n        plot_bar(df=df_reduced, var_name=var_name, ax=axes[i])\n    else:\n        plot_hist(df=df_reduced, var_name=var_name, ax=axes[i])\n\nfor j in range(len(var_list), len(axes)):\n    axes[j].set_visible(False)\n\nplt.tight_layout()\n#plt.suptitle('Variables distribution')\nplt.show()\n\n\n\n\n\nFigure 1: Selecting the optimum number of components\n\n\n\n\n      On arrive à voir que nous avons énormément de zones ou regions de transactions. On peut aussi constater que le nombre de tentatives de connexion est discret avec des valeurs faibles (à prendre en compte dans la modélisation future).\n\n\nLocation\nFort Worth          70\nLos Angeles         69\nOklahoma City       68\nCharlotte           68\nTucson              67\nPhiladelphia        67\nOmaha               65\nMiami               64\nDetroit             63\nHouston             63\nMemphis             63\nDenver              62\nKansas City         61\nBoston              61\nMesa                61\nAtlanta             61\nSeattle             61\nColorado Springs    60\nJacksonville        60\nFresno              60\nChicago             60\nAustin              59\nSan Jose            59\nRaleigh             59\nSan Antonio         59\nSan Diego           59\nIndianapolis        58\nNew York            58\nSan Francisco       57\nNashville           55\nMilwaukee           55\nLas Vegas           55\nVirginia Beach      55\nPhoenix             55\nColumbus            54\nSacramento          53\nBaltimore           51\nLouisville          51\nDallas              49\nWashington          48\nEl Paso             46\nPortland            42\nAlbuquerque         41\nName: count, dtype: int64\n\n\n      Imaginons que nous n’ayons que des données continues pour detecter les anomalies. Appliquons un modèle de mélange gaussien.\n\nLog-vraisemblance complète dans un GMM\nSoit :\n\n\\(X = \\{x_1, \\ldots, x_n\\}\\) : les données observées,\n\\(Z = \\{z_1, \\ldots, z_n\\}\\) : les variables latentes (composantes d’appartenance),\n\\(\\Theta = \\{ \\pi_k, \\mu_k, \\Sigma_k \\}_{k=1}^K\\) : les paramètres du modèle,\n\\(z_{ik} = 1\\) si \\(x_i\\) appartient à la composante \\(k\\), sinon \\(0\\).\nVraisemblance complète\n\nOn suppose que l’observation \\(x_i\\) vient de la composante \\(k\\) avec une probabilité \\(\\pi_k\\), et que la distribution conditionnelle est gaussienne :\n\\[\np(X, Z \\mid \\Theta) = \\prod_{i=1}^n \\prod_{k=1}^K \\left[ \\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right]^{z_{ik}}\n\\]\n\nLog-vraisemblance augmentée\n\nEn prenant le logarithme, on obtient la log-vraisemblance augmentée :\n\\[\n\\log p(X, Z \\mid \\Theta) = \\sum_{i=1}^n \\sum_{k=1}^K z_{ik} \\left( \\log \\pi_k + \\log \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n\\]\n\nL’algorithme EM maximise l’espérance de cette quantité (appelée Q-fonction) dans l’étape E :\n\n\\[\nQ(\\Theta \\mid \\Theta^{\\text{old}}) = \\mathbb{E}_{Z \\mid X, \\Theta^{\\text{old}}} [ \\log p(X, Z \\mid \\Theta) ]\n\\]\n\nAlgorithme EM pour un modèle de mélange gaussien (GMM)\n\n      Soit un jeu de données \\(X = \\{ x_1, x_2, \\ldots, x_n \\}\\) avec \\(n\\) observations, et un GMM avec \\(K\\) composantes. En vous épargnant de la résolution du problème : \\(\\Theta = argmax \\log p(X, Z \\mid \\Theta)\\)\n\nInitialisation\n\nInitialiser les paramètres du modèle pour chaque composante \\(k = 1, \\ldots, K\\) :\n\nLes poids : \\(\\pi_k\\), avec \\(\\sum_{k=1}^K \\pi_k = 1\\) et \\(\\pi_k &gt; 0\\),\nLes moyennes : \\(\\mu_k \\in \\mathbb{R}^d\\),\nLes matrices de covariance : \\(\\Sigma_k \\in \\mathbb{R}^{d \\times d}\\).\nÉtape 1 : Expectation (E-step)\n\nPour chaque observation \\(x_i\\), calculer la responsabilité \\(\\gamma_{ik}\\) qui est la probabilité que \\(x_i\\) appartienne à la composante \\(k\\), donnée les paramètres actuels :\n\\[\n\\gamma_{ik} = \\frac{\\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\, \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)}\n\\]\noù \\(\\mathcal{N}(x \\mid \\mu, \\Sigma)\\) est la densité de la loi normale multivariée.\n\nÉtape 2 : Maximisation (M-step)\n\nMettre à jour les paramètres \\(\\pi_k\\), \\(\\mu_k\\), \\(\\Sigma_k\\) en fonction des responsabilités calculées :\n\nMise à jour des poids :\n\n\\[\n\\pi_k = \\frac{1}{n} \\sum_{i=1}^n \\gamma_{ik}\n\\]\n\nMise à jour des moyennes :\n\n\\[\n\\mu_k = \\frac{\\sum_{i=1}^n \\gamma_{ik} x_i}{\\sum_{i=1}^n \\gamma_{ik}}\n\\]\n\nMise à jour des covariances :\n\n\\[\n\\Sigma_k = \\frac{\\sum_{i=1}^n \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T}{\\sum_{i=1}^n \\gamma_{ik}}\n\\]\n\nRépéter\n\nRépéter les étapes E et M jusqu’à convergence, c’est-à-dire jusqu’à ce que la variation de la log-vraisemblance soit très faible ou qu’un nombre maximal d’itérations soit atteint.\n\nRésumé\n\n\n\n\n\n\n\n\nÉtape\nDescription\n\n\n\n\nInitialisation\nFixer \\(\\pi_k, \\mu_k, \\Sigma_k\\) pour \\(k=1,\\ldots,K\\)\n\n\nE-step\nCalculer les responsabilités \\(\\gamma_{ik}\\)\n\n\nM-step\nMettre à jour \\(\\pi_k, \\mu_k, \\Sigma_k\\)\n\n\nRépéter\nJusqu’à convergence\n\n\n\n\n\n   TransactionAmount  ...  DaysBetweenThisTransactionAndTheLastOne\n0              14.09  ...                                    573.0\n1             376.24  ...                                    496.0\n2             126.29  ...                                    483.0\n3             184.50  ...                                    549.0\n4              13.45  ...                                    385.0\n\n[5 rows x 5 columns]\n\n\n      Nous disposons des variables suivantes : le montant de la transaction, l’âge du client, la durée de la transaction, le solde du compte, ainsi que le nombre de jours écoulés depuis la dernière transaction.\nAfin d’appliquer un modèle de mélange gaussien (GMM) à ces données, il est nécessaire de choisir un nombre optimal de composantes. En pratique, ce choix repose souvent sur des critères d’information tels que le Critère d’Information d’Akaike (AIC) ou le Critère d’Information Bayésien (BIC). Plus ces critères sont faibles, meilleur est le modèle. Toutefois, il convient de rester vigilant face au risque de surapprentissage : un modèle trop complexe (avec trop de composantes) peut s’ajuster parfaitement aux données d’apprentissage mais perdre en capacité de généralisation.\nDans le cadre de l’apprentissage non supervisé, l’évaluation du modèle est plus délicate, car nous ne disposons pas de labels permettant de valider la qualité de la segmentation. Dans certains cas, un petit échantillon d’exemples étiquetés comme anomalies est disponible, ce qui permet une évaluation ciblée du modèle entraîné. Mais dans notre situation, aucune étiquette n’est fournie, ce qui rend l’évaluation entièrement dépendante de critères internes tels que l’AIC (Akaike Information Criterion) et le BIC (Bayesian Information Criterion).\nEn complément, nous pouvons également calculer les log-vraisemblances complètes pour différents nombres de composantes, afin de visualiser la qualité d’ajustement du modèle. Enfin, une stratégie consiste à définir un seuil d’anomalie à partir de la distribution des log-vraisemblances : par exemple, en retenant le 5e percentile, les observations les moins vraisemblables (c’est-à-dire situées dans les 5 % les plus faibles) seront considérées comme potentiellement anormales.\nfrom sklearn.mixture import GaussianMixture\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Préparation des données\nX_scaled = scaler.fit_transform(df_continuous)\n\n# Paramètres\nn_components_range = range(1, 6)\naic, bic, log_lik = [], [], []\n\n# Entraînement et évaluation du modèle\nfor n in n_components_range:\n    gmm = GaussianMixture(n_components=n, random_state=RANDOM_STATE)\n    gmm.fit(X_scaled)\n    aic.append(gmm.aic(X_scaled))\n    bic.append(gmm.bic(X_scaled))\n    log_lik.append(gmm.score(X_scaled) * len(X_scaled))\n\n# Visualisation\nig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# AIC & BIC\naxes[0].plot(n_components_range, aic, label='AIC', linestyle='-', marker='o')\naxes[0].plot(n_components_range, bic, label='BIC', linestyle='-', marker='s')\naxes[0].set_title(\"AIC & BIC vs. nombre de composantes\", fontsize=10)\naxes[0].set_xlabel(\"Nombre de composantes\")\naxes[0].set_ylabel(\"Score AIC/BIC\")\naxes[0].legend()\naxes[0].grid(True)\n\n# Log-vraisemblance\naxes[1].plot(n_components_range, log_lik, label='Log-vraisemblance', color='green', marker='^')\naxes[1].set_title(\"Log-vraisemblance vs. nombre de composantes\", fontsize=10)\naxes[1].set_xlabel(\"Nombre de composantes\")\naxes[1].set_ylabel(\"Log-vraisemblance\")\naxes[1].grid(True)\n\nplt.tight_layout(pad=4)\nplt.show()\n\n\n\n\n\nFigure 2: Selecting the optimum number of components\n\n\n\n\n      On observe que plus le nombre de composantes augmente, plus les scores d’AIC et de BIC diminuent. Cependant, le BIC se stabilise à partir de la troisième composante, tandis que l’AIC continue de diminuer légèrement jusqu’à la cinquième. Cette divergence suggère qu’au-delà de trois composantes, le modèle pourrait être sujet à un surapprentissage.\nPar ailleurs, l’évolution de la log-vraisemblance complète montre une augmentation nette entre une et trois composantes, suivie d’une progression beaucoup plus faible au-delà. Ces observations concordantes justifient le choix d’un modèle avec trois composantes, qui représente un bon compromis entre qualité d’ajustement et complexité.\n\n\nGaussianMixture(n_components=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=3, random_state=42)\n\n\n\nLe 3ᵉ percentile est la valeur en dessous de laquelle se trouvent 3 % des observations dans un ensemble de données.\n\n\nhue dans sns.pairplot — Qu’est-ce que c’est ? L’argument hue sert à colorer les points en fonction d’une variable catégorielle (généralement une étiquette ou un groupe). Cela permet de visualiser les différences entre groupes dans les nuages de points.\n\nfrom copy import deepcopy\ndf_continuous_with_anomalies_obs = deepcopy(df_continuous)\ndf_continuous_with_anomalies_obs['Is anomaly'] = anomalies\n\nsns.pairplot(df_continuous_with_anomalies_obs, hue=\"Is anomaly\", palette={False: \"blue\", True: \"red\"}, corner=False, height=2.5) #diag_kind=\"hist\" ou kde pour la courbe\n#plt.suptitle('Anomalies detection with a Gaussian Mixture Model')\nplt.show()\n\n\n\n\n\nFigure 3: Anomalies detection with a Gaussian Mixture Model"
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#variables-analysées",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#variables-analysées",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "🎯 Variables analysées",
    "text": "🎯 Variables analysées\n\nCustomerAge (Âge du client)\nTransactionAmount (Montant de la transaction)\nTransactionDuration (Durée de la transaction)\nAccountBalance (Solde du compte)\nDaysBetweenThisTransactionAndTheLastOne (Jours entre deux transactions)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#lecture-du-graphique",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#lecture-du-graphique",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "Lecture du graphique",
    "text": "Lecture du graphique\n\nLa diagonale contient les distributions marginales estimées de chaque variable :\n\nEn bleu : la densité des observations normales\nEn rouge : la densité des anomalies (souvent plus discrète car peu nombreuses)\n\nLes graphiques hors-diagonale sont des nuages de points croisant deux variables à la fois :\n\nLes points rouges se situent souvent dans des zones de faible densité bleue, indiquant leur caractère atypique dans l’espace multivarié.\n\n\n\n💡 Remarque : Un point rouge mélangé à du bleu ne signifie pas une erreur du modèle, mais une anomalie faible, difficile à séparer par les seules combinaisons bivariées. L’analyse multidimensionnelle du GMM est ici essentielle."
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#interprétation-variable-par-variable",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#interprétation-variable-par-variable",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "Interprétation variable par variable",
    "text": "Interprétation variable par variable\n\n1. TransactionAmount (Montant de la transaction)\n\nDistribution : Asymétrique à droite (valeurs élevées peu fréquentes).\nAnomalies :\n\nMontants très élevés (&gt; 1 000) souvent identifiés comme atypiques.\n\nHypothèse : Retraits importants ou virements massifs peuvent signaler des comportements inhabituels (fraude, opération exceptionnelle).\n\n2. CustomerAge (Age du client)\n\nDistribution : Potentiellement bimodale (ex. : jeunes adultes et seniors).\nAnomalies :\n\nClients très jeunes (&lt; 18 ans) ou très âgés (&gt; 75 ans).\n\nHypothèse : Ces tranches sont minoritaires et peuvent être liées à des profils atypiques ou vulnérables.\n\n3. TransactionDuration (Durée de la transaction)\n\nDistribution : Relativement étalée.\nAnomalies :\n\nTrès longues (&gt; 250 s) ou très courtes (&lt; 5 s).\n\nHypothèse : Durées extrêmes peuvent refléter des problèmes techniques ou des manipulations suspectes.\n\n4. AccountBalance (Solde du compte)\n\nDistribution : Concentrée vers les faibles soldes, avec une queue à droite.\nAnomalies :\n\nSoldes très élevés (&gt; 12 000) ou très bas (≈ 0).\n\nHypothèse : Les extrêmes financiers peuvent attirer l’attention en détection d’anomalies.\n\n5. DaysBetweenThisTransactionAndTheLastOne (Jours entre deux transactions)\n\nDistribution : Dispersée.\nAnomalies :\n\nPériodes très courtes (&lt; 100 jours) ou très longues (&gt; 700 jours).\n\nHypothèse : Des écarts extrêmes dans la fréquence peuvent indiquer une activité inhabituelle."
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#interactions-clés-entre-variables",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#interactions-clés-entre-variables",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "Interactions clés entre variables",
    "text": "Interactions clés entre variables\n\nTransactionAmount × AccountBalance\n\nZone à risque : Montants et soldes simultanément élevés.\nInterprétation : Le retrait de sommes importantes depuis un compte bien rempli peut correspondre à un comportement rare ou à surveiller.\n\nTransactionAmount × TransactionDuration\n\nAnomalies dans les cas de montants élevés + durées longues.\nInterprétation : Transactions longues et coûteuses peuvent signaler un traitement manuel, un bug ou une tentative malveillante.\n\nCustomerAge × Autres variables\n\nMoins de patterns nets, mais les jeunes ou très âgés combinés à des comportements extrêmes (ex. : gros montant ou délai long) ressortent souvent comme anomalies."
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#cas-particulier-visible-dans-le-coin-bas-gauche",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#cas-particulier-visible-dans-le-coin-bas-gauche",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "Cas particulier visible dans le coin bas-gauche",
    "text": "Cas particulier visible dans le coin bas-gauche\nOn observe que des transactions nulles ou très faibles (TransactionAmount ≈ 0) mais avec une durée de traitement très longue (TransactionDuration &gt; 250 s) sont fréquemment classées comme anomalies.\n\n🧩 Cela peut correspondre à une attente anormale sans transaction effective – ce qui peut indiquer une erreur technique, une fraude ou une activité suspecte."
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#conclusion",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#conclusion",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "Conclusion",
    "text": "Conclusion\n      Le modèle GMM a permis de mettre en évidence des observations atypiques, définies comme ayant une faible probabilité d’appartenance à l’un des groupes dominants dans l’espace des variables continues.\n\nLes anomalies détectées reflètent :\nDes valeurs extrêmes univariées\nDes combinaisons de comportements rares, parfois imperceptibles dans les projections bivariées\nUtilité :\nSurveillance des fraudes\nAjustement des règles de sécurité\nCompréhension des profils inhabituels\n\n\n🔬 Limite : Cette analyse repose uniquement sur des variables numériques continues. Intégrer des variables catégorielles ou discrètes (ex. : type de transaction, canal utilisé) permettrait d’affiner la détection."
  },
  {
    "objectID": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#annexes",
    "href": "INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html#annexes",
    "title": "Détection d’anomalies dans les transactions banquaires",
    "section": "Annexes",
    "text": "Annexes\n\nHistogramme :\n\n|        ▆\n|        ▆    ▆\n|    ▆   ▆    ▆   ▆\n|▆   ▆   ▆▆  ▆▆▆ ▆▆\n+--------------------------&gt; valeur\n\nKDE (courbe lissée) :\n\n          /\\\n         /  \\     /\\\n        /    \\   /  \\\n_______/      \\_/    \\_____"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "",
    "text": "Ce document explique en détail le fonctionnement du code Python du Voxa Assistant, un assistant vocal interactif utilisant plusieurs bibliothèques pour la reconnaissance vocale, la synthèse vocale et les requêtes en ligne. J’utilise en particulier Wolfram Alpha.\n\n\nWolfram Alpha est un moteur de calcul et de réponse basé sur l’intelligence artificielle et les algorithmes symboliques. Contrairement à un moteur de recherche classique comme Google, qui fournit des liens vers des sites web, Wolfram Alpha génère directement des réponses précises basées sur des bases de connaissances et des algorithmes mathématiques avancés. Il est souvent utilisé pour des calculs, des questions scientifiques et des recherches basées sur des données structurées.\n\n\n\nRésolution d’équations mathématiques et scientifiques\n\nRecherche et analyse de données (statistiques, physique, chimie, finance, etc.)\n\nInterprétation de requêtes en langage naturel\n\nGénération de graphiques et de simulations\n\n🔗 Créer un compte Wolfram Alpha :\nSi vous souhaitez utiliser l’API de Wolfram Alpha dans votre projet, vous devez créer un compte via ce lien :\n👉 Créer un compte Wolfram Alpha Je posterai une demo sur comment creer son compte et recupérer un id pour une application. Car en effet, il existe plusieurs type d’ID qui servent à différentes type d’applications. Il fonctionne en Anglais donc nous allons écrire une fonction pour la traduction du Francais en Anglais afin de poser des questions et une pour la traduction de l’Anglais en Français pour la reponse trouvée. Vous avez bien entendu besoin de connexion pour effectuer les recherches.\n\n⚠️ MISE À JOUR IMPORTANTE DE L’API WOLFRAMALPHA – 24 JUIN 2024\nDepuis le 24 juin 2024, l’API WolframAlpha a changé.\nL’ancienne méthode avec Client.query() et next(response.results) n’est plus fiable et peut générer des erreurs (StopIteration, TypeError, etc.).\n\n\nUtilisez l’API REST v2/query directement via requests :\nimport requests\nimport xml.etree.ElementTree as ET\n\ndef wolfram_query(query):\n    url = \"https://api.wolframalpha.com/v2/query\"\n    params = {\n        \"appid\": APP_ID,\n        \"input\": query,\n        \"format\": \"plaintext\"\n    }\n    response = requests.get(url, params=params)\n    root = ET.fromstring(response.content)\n    pods = root.findall('.//pod')\n\n    for pod in pods:\n        title = pod.attrib.get('title', '').lower()\n        if any(kw in title for kw in ['result', 'definition', 'primary']):\n            txt = pod.find('.//plaintext')\n            if txt is not None and txt.text:\n                return txt.text\n    return \"Aucune réponse trouvée.\"\n\n🎙️ Et pour les commandes vocales ?\nTraduisez votre question en anglais avant l’envoi (translate_fr_en) et traduisez la réponse inversement pour l’afficher ou la vocaliser (translate_en_fr).\n💡 Exemple vocal :\nDites “Intégrale de ln(x)” → traduit en “integrate ln(x)” → réponse traitée par l’API.\n\nExemple d’utilisation\n\nimport wolframalpha\n# id_ = \"YOUR_WOLFRAMALPHA_ID\"\n##-- J'utliserai le mien que j'ai masqué\n# id_ = r.id_\nclient = wolframalpha.Client(id_)\nqueries = [\n  \"who is the president of France\",\n  \"compute 2 times 2 times ln(2)\",\n  \"derivate xln(x)\",\n  \"integrate exponential of 2x between 2 and 4\"\n]\n# print(id_)\n\n\nimport requests\nimport xml.etree.ElementTree as ET\n\ndef wolfram_query(query):\n    \"\"\"\n    Interroge l'API WolframAlpha avec une requête textuelle et extrait la réponse principale.\n\n    Paramètres :\n    -----------\n    query : str\n        La question ou l'expression mathématique à envoyer à WolframAlpha.\n\n    Retour :\n    --------\n    str ou None\n        Le texte de la réponse principale si trouvée, sinon None.\n    \n    Fonctionnement :\n    ----------------\n    - Envoie la requête à l'API WolframAlpha (format XML).\n    - Vérifie que la réponse est bien en XML.\n    - Parse le XML pour extraire les 'pods' (blocs de réponses).\n    - Recherche prioritairement un pod dont le titre contient 'result', 'definition' ou 'primary'.\n    - Sinon retourne le premier pod contenant du texte.\n    - Si aucune réponse trouvée, retourne None.\n    \"\"\"\n\n    # URL de l'API WolframAlpha pour requêtes de type 'query'\n    url = \"https://api.wolframalpha.com/v2/query\"\n\n    # Paramètres envoyés : clé API, la question, format de réponse demandé (texte brut)\n    params = {\n        \"appid\": id_,\n        \"input\": query,\n        \"format\": \"plaintext\"\n    }\n\n    # Envoi de la requête HTTP GET\n    response = requests.get(url, params=params)\n\n    # Vérification que la réponse est bien du XML\n    content_type = response.headers.get('Content-Type', '')\n    if 'xml' not in content_type:\n        raise ValueError(f\"Format inattendu : {content_type}\")\n\n    # Parsing du contenu XML de la réponse\n    root = ET.fromstring(response.content)\n\n    # Recherche de tous les pods (sections de réponse)\n    pods = root.findall('.//pod')\n\n    # Première passe : chercher un pod contenant la réponse principale\n    for pod in pods:\n        title = pod.attrib.get('title', '').lower()\n        if 'result' in title or 'definition' in title or 'primary' in title:\n            plaintext = pod.find('.//plaintext')\n            if plaintext is not None and plaintext.text:\n                print(f\"Réponse pour '{query}' : {plaintext.text}\")\n                return plaintext.text\n\n    # Seconde passe : si pas de pod \"résultat\", afficher le premier pod avec du texte\n    for pod in pods:\n        plaintext = pod.find('.//plaintext')\n        if plaintext is not None and plaintext.text:\n            print(f\"Réponse pour '{query}' : {plaintext.text}\")\n            return plaintext.text\n\n    # Si aucun pod avec texte, afficher message d'erreur\n    print(f\"Aucune réponse trouvée pour '{query}'\")\n    return None\n\n\nwolfram_query(queries[0])\n\nRéponse pour 'who is the president of France' : Emmanuel Macron (from 14/05/2017 to present)\n'Emmanuel Macron (from 14/05/2017 to present)'\n\n\n\nwolfram_query(queries[1])\n\nRéponse pour 'compute 2 times 2 times ln(2)' : 4 log(2)\n'4 log(2)'\n\n\n\nwolfram_query(queries[2])\n\nRéponse pour 'derivate xln(x)' : d/dx(x log(x)) = log(x) + 1\n'd/dx(x log(x)) = log(x) + 1'\n\n\n\nwolfram_query(queries[3])\n\nRéponse pour 'integrate exponential of 2x between 2 and 4' : integral_2^4 exp(2 x) dx = 1/2 e^4 (e^4 - 1)≈1463.2\n'integral_2^4 exp(2 x) dx = 1/2 e^4 (e^4 - 1)≈1463.2'\n\n\n\n\n\nLe script commence par l’importation des bibliothèques nécessaires :\n\nimport datetime\nimport webbrowser\nimport sys\nimport pywhatkit\nimport speech_recognition as sr\nimport pyttsx3 as ttx\nimport wikipedia\nfrom googletrans import Translator\nimport wolframalpha\n\n\ndatetime : gestion des dates et heures.\nwebbrowser : ouverture des pages web.\nsys : gestion des fonctionnalités système.\npywhatkit : exécution de commandes interactives comme la recherche YouTube.\nspeech_recognition : reconnaissance vocale.\npyttsx3 : synthèse vocale.\nwikipedia : récupération d’informations depuis Wikipédia.\ngoogletrans : traduction de texte.\nwolframalpha : moteur de réponse à des questions scientifiques et mathématiques.\n\nIntaller les avec la commande :\n\nmodules = [\n    \"pywhatkit\", \"speechrecognition\", \"pyttsx3\",\n    \"wikipedia\", \"googletrans==4.0.0-rc1\", \"wolframalpha\", \"pyaudio\"\n]\n\nimport subprocess\nimport sys\ndef install_modules():\n    for module in modules:\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", module])\n        except:\n            print(\"Quelque chose s'est mal passée\")\n            \ninstall_modules()\n\n\n\n\n\nLe code initialise pyttsx3 et affiche les voix disponibles :\n\nmoteur = ttx.init()\nvoix_disponibles = moteur.getProperty(\"voices\")\n\nfor index, voix in enumerate(voix_disponibles):\n    print(f\"Index {index} - ID: {voix.id} - Langue: {voix.languages} - Nom: {voix.name}\")\n\nIndex 0 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0 - Langue: [] - Nom: Microsoft David Desktop - English (United States)\nIndex 1 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0 - Langue: [] - Nom: Microsoft Zira Desktop - English (United States)\nIndex 2 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_FR-FR_HORTENSE_11.0 - Langue: [] - Nom: Microsoft Hortense Desktop - French\n\n\nEnsuite, une voix spécifique est sélectionnée et testée :\n\nmoteur.setProperty(\"voice\", voix_disponibles[2].id)\nmoteur.say(\"Bonjour, ceci est un test avec une autre voix.\")\nmoteur.runAndWait()\n\n\n\n\nLa classe voxaAssistant gère toutes les fonctionnalités de l’assistant vocal.\n\n\n\nclass voxaAssistant:\n    def __init__(self):\n        self.ecouteur = sr.Recognizer()\n        self.moteur = ttx.init()\n        self.voix_disponibles = self.moteur.getProperty(\"voices\")\n        self.moteur.setProperty(\"voice\", self.voix_disponibles[2].id)\n        self.moteur.setProperty(\"rate\", 170)\n        self.app_id = id_\n        self.client = wolframalpha.Client(self.app_id)\n\nCette méthode : - Initialise le moteur de reconnaissance vocale (speech_recognition)\n\nConfigure la synthèse vocale avec pyttsx3\nDéfinit la clé API pour Wolfram Alpha.\n\n\n\n\nCette fonction génère une sortie vocale à partir d’un texte donné.\n\ndef parler(self, texte):\n    self.moteur.say(texte)\n    self.moteur.runAndWait()\n\n\n\n\nCette fonction ajuste le message de salutation en fonction de l’heure.\n\ndef saluer(self):\n    heure_actuel = int(datetime.datetime.now().hour)\n    if 0 &lt;= heure_actuel &lt;= 12:\n        self.parler(\"Bonjour à vous Djamal\")\n    else:\n        self.parler(\"Bonsoir à vous Djamal\")\n\n\n\n\n\n\n\nCette fonction écoute l’utilisateur et transcrit la parole en texte.\n\ndef voxa_requete(self):\n    with sr.Microphone() as parole:\n        print(\"Entrain d'écouter ...\")\n        self.ecouteur.adjust_for_ambient_noise(parole, duration=1)\n        self.ecouteur.pause_threshold = 1.5\n        try:\n            voix = self.ecouteur.listen(parole, timeout=5, phrase_time_limit=5)\n            command = self.ecouteur.recognize_google(voix, language=\"fr\").lower()\n            print(\"Vous avez dit .... : \", command)\n            return command\n        except sr.UnknownValueError:\n            print(\"Je n'ai pas compris, veuillez répéter.\")\n            return \"\"\n        except sr.RequestError:\n            print(\"Erreur avec le service de reconnaissance vocale.\")\n            return \"\"\n\n\n\n\nSi l’utilisateur mentionne Google ou YouTube, la recherche est effectuée automatiquement.\n\nelif \"google\" in voix:\n    url = voix.split().index(\"google\")\n    elt_rechercher = voix.split()[url + 1:]\n    self.parler(\"D'accord, je lance la recherche\")\n    webbrowser.open(\"https://www.google.com/search?q=\" + \"+\".join(elt_rechercher), new=2)\n\n\nelif \"recherche sur youtube\" in voix or \"recherche sur youtube.com\" in voix:\n                url = voix.split().index(\"youtube\")\n                elt_rechercher = voix.split()[url + 1:]\n                self.parler(\"d'accord  je  lance  la  recherche\")\n                webbrowser.open(\n                    \"http://www.youtube.com/results?search_query=\"\n                    + \"+\".join(elt_rechercher),\n                    new=2,\n                )\n\n\n\n\nurl = voix.split().index(\"google\")\nelt_rechercher = voix.split()[url + 1:]\n\n\nsplit() découpe une chaîne de caractères en une liste de mots.\nIci, on cherche l’index du mot “google” pour récupérer les mots suivants, qui correspondent à la requête de l’utilisateur.\n\nExemple :\n\nEntrée : \"cherche sur google c'est quoi la capitale de la France\"\n\nAprès split() : [\"cherche\",\"sur\", \"google\", \"c\", \"'\", \"est\", \"quoi\", \"la\", \"capitale\", \"de\", \"la\", \"France\"]\n\nIndex du mot “google” : 3\n\nCe qui est recherché : [\"c\", \"'\", \"est\", \"quoi\", \"la\", \"capitale\", \"de\", \"la\", \"France\"] → Ici, on devrait prendre les mots après “google”.\n\n\n\n\n\n\nwebbrowser.open(\"https://www.google.com/search?q=\" + \"+\".join(elt_rechercher), new=2)\n\n\nelif \"youtube\" in voix:\n    s = voix.replace(\"youtube\", \"\")\n    self.parler(\"D'accord sans soucis\")\n    pywhatkit.playonyt(s)\n\n\nExplication du +.\n\nDans une URL, un espace est souvent remplacé par + ou %20.\n\nExemple : Si l’utilisateur dit “recherche machine learning sur google”, on doit transformer \"machine learning\" en \"machine+learning\" pour que Google comprenne.\n\nAutre solution : \"%20\".join(elt_rechercher) aurait aussi pu être utilisé.\n\n\n\n\n\n\n\n\n\n    def question_generale(self, voix):\n        voix = self.translate_eng_fr(voix)\n        try:\n            reponse = self.client.query(voix)\n            res = next(reponse.results).text\n            res = self.translate_fr_eng(res)\n            print(\"Un instant ...\")\n            print(res)\n            self.parler(res)\n        except:\n            self.parler(\"Je n'ai pas trouvé de réponse.\")\n\n      Ici, l’assistant vocal envoie la requête à Wolfram Alpha, récupère la réponse et la traduit en français avant de la prononcer.\nSi aucune réponse n’est trouvée, une recherche est effectuée sur Wikipédia.\n\n\n\n\ntry:\n    wikipedia.set_lang(\"fr\")\n    info = wikipedia.summary(voix, 1)\n    self.parler(str(info))\nexcept:\n    self.parler(\"Je n'ai pas bien compris\")\n\n\n\n\n\nreponse = self.client.query(voix)\nres = next(reponse.results).text\n\n\n.query(voix) : envoie la question de l’utilisateur à Wolfram Alpha.\n\nnext(reponse.results).text : récupère la première réponse retournée et extrait le texte.\n\nSi Wolfram Alpha trouve une réponse pertinente, elle est lue à haute voix.\n\n\n\n\n\n\n\n\n\n\n\n\nÉlément\nExplication\n\n\n\n\nWolfram Alpha\nMoteur de calcul intelligent répondant à des requêtes scientifiques et analytiques\n\n\nsplit()\nDécoupe une phrase en liste de mots\n\n\nquery()\nEnvoie une requête à Wolfram Alpha\n\n\njoin(“+”)\nTransforme une liste de mots en requête lisible par un moteur de recherche"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#wolfram-alpha-cest-quoi-et-à-quoi-ça-sert",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#wolfram-alpha-cest-quoi-et-à-quoi-ça-sert",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "",
    "text": "Wolfram Alpha est un moteur de calcul et de réponse basé sur l’intelligence artificielle et les algorithmes symboliques. Contrairement à un moteur de recherche classique comme Google, qui fournit des liens vers des sites web, Wolfram Alpha génère directement des réponses précises basées sur des bases de connaissances et des algorithmes mathématiques avancés. Il est souvent utilisé pour des calculs, des questions scientifiques et des recherches basées sur des données structurées.\n\n\n\nRésolution d’équations mathématiques et scientifiques\n\nRecherche et analyse de données (statistiques, physique, chimie, finance, etc.)\n\nInterprétation de requêtes en langage naturel\n\nGénération de graphiques et de simulations\n\n🔗 Créer un compte Wolfram Alpha :\nSi vous souhaitez utiliser l’API de Wolfram Alpha dans votre projet, vous devez créer un compte via ce lien :\n👉 Créer un compte Wolfram Alpha Je posterai une demo sur comment creer son compte et recupérer un id pour une application. Car en effet, il existe plusieurs type d’ID qui servent à différentes type d’applications. Il fonctionne en Anglais donc nous allons écrire une fonction pour la traduction du Francais en Anglais afin de poser des questions et une pour la traduction de l’Anglais en Français pour la reponse trouvée. Vous avez bien entendu besoin de connexion pour effectuer les recherches.\n\n⚠️ MISE À JOUR IMPORTANTE DE L’API WOLFRAMALPHA – 24 JUIN 2024\nDepuis le 24 juin 2024, l’API WolframAlpha a changé.\nL’ancienne méthode avec Client.query() et next(response.results) n’est plus fiable et peut générer des erreurs (StopIteration, TypeError, etc.).\n\n\nUtilisez l’API REST v2/query directement via requests :\nimport requests\nimport xml.etree.ElementTree as ET\n\ndef wolfram_query(query):\n    url = \"https://api.wolframalpha.com/v2/query\"\n    params = {\n        \"appid\": APP_ID,\n        \"input\": query,\n        \"format\": \"plaintext\"\n    }\n    response = requests.get(url, params=params)\n    root = ET.fromstring(response.content)\n    pods = root.findall('.//pod')\n\n    for pod in pods:\n        title = pod.attrib.get('title', '').lower()\n        if any(kw in title for kw in ['result', 'definition', 'primary']):\n            txt = pod.find('.//plaintext')\n            if txt is not None and txt.text:\n                return txt.text\n    return \"Aucune réponse trouvée.\"\n\n🎙️ Et pour les commandes vocales ?\nTraduisez votre question en anglais avant l’envoi (translate_fr_en) et traduisez la réponse inversement pour l’afficher ou la vocaliser (translate_en_fr).\n💡 Exemple vocal :\nDites “Intégrale de ln(x)” → traduit en “integrate ln(x)” → réponse traitée par l’API.\n\nExemple d’utilisation\n\nimport wolframalpha\n# id_ = \"YOUR_WOLFRAMALPHA_ID\"\n##-- J'utliserai le mien que j'ai masqué\n# id_ = r.id_\nclient = wolframalpha.Client(id_)\nqueries = [\n  \"who is the president of France\",\n  \"compute 2 times 2 times ln(2)\",\n  \"derivate xln(x)\",\n  \"integrate exponential of 2x between 2 and 4\"\n]\n# print(id_)\n\n\nimport requests\nimport xml.etree.ElementTree as ET\n\ndef wolfram_query(query):\n    \"\"\"\n    Interroge l'API WolframAlpha avec une requête textuelle et extrait la réponse principale.\n\n    Paramètres :\n    -----------\n    query : str\n        La question ou l'expression mathématique à envoyer à WolframAlpha.\n\n    Retour :\n    --------\n    str ou None\n        Le texte de la réponse principale si trouvée, sinon None.\n    \n    Fonctionnement :\n    ----------------\n    - Envoie la requête à l'API WolframAlpha (format XML).\n    - Vérifie que la réponse est bien en XML.\n    - Parse le XML pour extraire les 'pods' (blocs de réponses).\n    - Recherche prioritairement un pod dont le titre contient 'result', 'definition' ou 'primary'.\n    - Sinon retourne le premier pod contenant du texte.\n    - Si aucune réponse trouvée, retourne None.\n    \"\"\"\n\n    # URL de l'API WolframAlpha pour requêtes de type 'query'\n    url = \"https://api.wolframalpha.com/v2/query\"\n\n    # Paramètres envoyés : clé API, la question, format de réponse demandé (texte brut)\n    params = {\n        \"appid\": id_,\n        \"input\": query,\n        \"format\": \"plaintext\"\n    }\n\n    # Envoi de la requête HTTP GET\n    response = requests.get(url, params=params)\n\n    # Vérification que la réponse est bien du XML\n    content_type = response.headers.get('Content-Type', '')\n    if 'xml' not in content_type:\n        raise ValueError(f\"Format inattendu : {content_type}\")\n\n    # Parsing du contenu XML de la réponse\n    root = ET.fromstring(response.content)\n\n    # Recherche de tous les pods (sections de réponse)\n    pods = root.findall('.//pod')\n\n    # Première passe : chercher un pod contenant la réponse principale\n    for pod in pods:\n        title = pod.attrib.get('title', '').lower()\n        if 'result' in title or 'definition' in title or 'primary' in title:\n            plaintext = pod.find('.//plaintext')\n            if plaintext is not None and plaintext.text:\n                print(f\"Réponse pour '{query}' : {plaintext.text}\")\n                return plaintext.text\n\n    # Seconde passe : si pas de pod \"résultat\", afficher le premier pod avec du texte\n    for pod in pods:\n        plaintext = pod.find('.//plaintext')\n        if plaintext is not None and plaintext.text:\n            print(f\"Réponse pour '{query}' : {plaintext.text}\")\n            return plaintext.text\n\n    # Si aucun pod avec texte, afficher message d'erreur\n    print(f\"Aucune réponse trouvée pour '{query}'\")\n    return None\n\n\nwolfram_query(queries[0])\n\nRéponse pour 'who is the president of France' : Emmanuel Macron (from 14/05/2017 to present)\n'Emmanuel Macron (from 14/05/2017 to present)'\n\n\n\nwolfram_query(queries[1])\n\nRéponse pour 'compute 2 times 2 times ln(2)' : 4 log(2)\n'4 log(2)'\n\n\n\nwolfram_query(queries[2])\n\nRéponse pour 'derivate xln(x)' : d/dx(x log(x)) = log(x) + 1\n'd/dx(x log(x)) = log(x) + 1'\n\n\n\nwolfram_query(queries[3])\n\nRéponse pour 'integrate exponential of 2x between 2 and 4' : integral_2^4 exp(2 x) dx = 1/2 e^4 (e^4 - 1)≈1463.2\n'integral_2^4 exp(2 x) dx = 1/2 e^4 (e^4 - 1)≈1463.2'\n\n\n\n\n\nLe script commence par l’importation des bibliothèques nécessaires :\n\nimport datetime\nimport webbrowser\nimport sys\nimport pywhatkit\nimport speech_recognition as sr\nimport pyttsx3 as ttx\nimport wikipedia\nfrom googletrans import Translator\nimport wolframalpha\n\n\ndatetime : gestion des dates et heures.\nwebbrowser : ouverture des pages web.\nsys : gestion des fonctionnalités système.\npywhatkit : exécution de commandes interactives comme la recherche YouTube.\nspeech_recognition : reconnaissance vocale.\npyttsx3 : synthèse vocale.\nwikipedia : récupération d’informations depuis Wikipédia.\ngoogletrans : traduction de texte.\nwolframalpha : moteur de réponse à des questions scientifiques et mathématiques.\n\nIntaller les avec la commande :\n\nmodules = [\n    \"pywhatkit\", \"speechrecognition\", \"pyttsx3\",\n    \"wikipedia\", \"googletrans==4.0.0-rc1\", \"wolframalpha\", \"pyaudio\"\n]\n\nimport subprocess\nimport sys\ndef install_modules():\n    for module in modules:\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", module])\n        except:\n            print(\"Quelque chose s'est mal passée\")\n            \ninstall_modules()"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#configuration-du-moteur-de-synthèse-vocale",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#configuration-du-moteur-de-synthèse-vocale",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "",
    "text": "Le code initialise pyttsx3 et affiche les voix disponibles :\n\nmoteur = ttx.init()\nvoix_disponibles = moteur.getProperty(\"voices\")\n\nfor index, voix in enumerate(voix_disponibles):\n    print(f\"Index {index} - ID: {voix.id} - Langue: {voix.languages} - Nom: {voix.name}\")\n\nIndex 0 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0 - Langue: [] - Nom: Microsoft David Desktop - English (United States)\nIndex 1 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0 - Langue: [] - Nom: Microsoft Zira Desktop - English (United States)\nIndex 2 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_FR-FR_HORTENSE_11.0 - Langue: [] - Nom: Microsoft Hortense Desktop - French\n\n\nEnsuite, une voix spécifique est sélectionnée et testée :\n\nmoteur.setProperty(\"voice\", voix_disponibles[2].id)\nmoteur.say(\"Bonjour, ceci est un test avec une autre voix.\")\nmoteur.runAndWait()"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#définition-de-la-classe-voxaassistant",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#définition-de-la-classe-voxaassistant",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "",
    "text": "La classe voxaAssistant gère toutes les fonctionnalités de l’assistant vocal.\n\n\n\nclass voxaAssistant:\n    def __init__(self):\n        self.ecouteur = sr.Recognizer()\n        self.moteur = ttx.init()\n        self.voix_disponibles = self.moteur.getProperty(\"voices\")\n        self.moteur.setProperty(\"voice\", self.voix_disponibles[2].id)\n        self.moteur.setProperty(\"rate\", 170)\n        self.app_id = id_\n        self.client = wolframalpha.Client(self.app_id)\n\nCette méthode : - Initialise le moteur de reconnaissance vocale (speech_recognition)\n\nConfigure la synthèse vocale avec pyttsx3\nDéfinit la clé API pour Wolfram Alpha.\n\n\n\n\nCette fonction génère une sortie vocale à partir d’un texte donné.\n\ndef parler(self, texte):\n    self.moteur.say(texte)\n    self.moteur.runAndWait()\n\n\n\n\nCette fonction ajuste le message de salutation en fonction de l’heure.\n\ndef saluer(self):\n    heure_actuel = int(datetime.datetime.now().hour)\n    if 0 &lt;= heure_actuel &lt;= 12:\n        self.parler(\"Bonjour à vous Djamal\")\n    else:\n        self.parler(\"Bonsoir à vous Djamal\")"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#reconnaissance-et-traitement-des-requêtes-vocales",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#reconnaissance-et-traitement-des-requêtes-vocales",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "",
    "text": "Cette fonction écoute l’utilisateur et transcrit la parole en texte.\n\ndef voxa_requete(self):\n    with sr.Microphone() as parole:\n        print(\"Entrain d'écouter ...\")\n        self.ecouteur.adjust_for_ambient_noise(parole, duration=1)\n        self.ecouteur.pause_threshold = 1.5\n        try:\n            voix = self.ecouteur.listen(parole, timeout=5, phrase_time_limit=5)\n            command = self.ecouteur.recognize_google(voix, language=\"fr\").lower()\n            print(\"Vous avez dit .... : \", command)\n            return command\n        except sr.UnknownValueError:\n            print(\"Je n'ai pas compris, veuillez répéter.\")\n            return \"\"\n        except sr.RequestError:\n            print(\"Erreur avec le service de reconnaissance vocale.\")\n            return \"\"\n\n\n\n\nSi l’utilisateur mentionne Google ou YouTube, la recherche est effectuée automatiquement.\n\nelif \"google\" in voix:\n    url = voix.split().index(\"google\")\n    elt_rechercher = voix.split()[url + 1:]\n    self.parler(\"D'accord, je lance la recherche\")\n    webbrowser.open(\"https://www.google.com/search?q=\" + \"+\".join(elt_rechercher), new=2)\n\n\nelif \"recherche sur youtube\" in voix or \"recherche sur youtube.com\" in voix:\n                url = voix.split().index(\"youtube\")\n                elt_rechercher = voix.split()[url + 1:]\n                self.parler(\"d'accord  je  lance  la  recherche\")\n                webbrowser.open(\n                    \"http://www.youtube.com/results?search_query=\"\n                    + \"+\".join(elt_rechercher),\n                    new=2,\n                )\n\n\n\n\nurl = voix.split().index(\"google\")\nelt_rechercher = voix.split()[url + 1:]\n\n\nsplit() découpe une chaîne de caractères en une liste de mots.\nIci, on cherche l’index du mot “google” pour récupérer les mots suivants, qui correspondent à la requête de l’utilisateur.\n\nExemple :\n\nEntrée : \"cherche sur google c'est quoi la capitale de la France\"\n\nAprès split() : [\"cherche\",\"sur\", \"google\", \"c\", \"'\", \"est\", \"quoi\", \"la\", \"capitale\", \"de\", \"la\", \"France\"]\n\nIndex du mot “google” : 3\n\nCe qui est recherché : [\"c\", \"'\", \"est\", \"quoi\", \"la\", \"capitale\", \"de\", \"la\", \"France\"] → Ici, on devrait prendre les mots après “google”.\n\n\n\n\n\n\nwebbrowser.open(\"https://www.google.com/search?q=\" + \"+\".join(elt_rechercher), new=2)\n\n\nelif \"youtube\" in voix:\n    s = voix.replace(\"youtube\", \"\")\n    self.parler(\"D'accord sans soucis\")\n    pywhatkit.playonyt(s)\n\n\nExplication du +.\n\nDans une URL, un espace est souvent remplacé par + ou %20.\n\nExemple : Si l’utilisateur dit “recherche machine learning sur google”, on doit transformer \"machine learning\" en \"machine+learning\" pour que Google comprenne.\n\nAutre solution : \"%20\".join(elt_rechercher) aurait aussi pu être utilisé.\n\n\n\n\n\n\n\n\n\n    def question_generale(self, voix):\n        voix = self.translate_eng_fr(voix)\n        try:\n            reponse = self.client.query(voix)\n            res = next(reponse.results).text\n            res = self.translate_fr_eng(res)\n            print(\"Un instant ...\")\n            print(res)\n            self.parler(res)\n        except:\n            self.parler(\"Je n'ai pas trouvé de réponse.\")\n\n      Ici, l’assistant vocal envoie la requête à Wolfram Alpha, récupère la réponse et la traduit en français avant de la prononcer.\nSi aucune réponse n’est trouvée, une recherche est effectuée sur Wikipédia.\n\n\n\n\ntry:\n    wikipedia.set_lang(\"fr\")\n    info = wikipedia.summary(voix, 1)\n    self.parler(str(info))\nexcept:\n    self.parler(\"Je n'ai pas bien compris\")\n\n\n\n\n\nreponse = self.client.query(voix)\nres = next(reponse.results).text\n\n\n.query(voix) : envoie la question de l’utilisateur à Wolfram Alpha.\n\nnext(reponse.results).text : récupère la première réponse retournée et extrait le texte.\n\nSi Wolfram Alpha trouve une réponse pertinente, elle est lue à haute voix.\n\n\n\n\n\n\n\n\n\n\n\n\nÉlément\nExplication\n\n\n\n\nWolfram Alpha\nMoteur de calcul intelligent répondant à des requêtes scientifiques et analytiques\n\n\nsplit()\nDécoupe une phrase en liste de mots\n\n\nquery()\nEnvoie une requête à Wolfram Alpha\n\n\njoin(“+”)\nTransforme une liste de mots en requête lisible par un moteur de recherche"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#test-du-code",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#test-du-code",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "Test du code",
    "text": "Test du code"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#conclusion",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#conclusion",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "Conclusion",
    "text": "Conclusion\nCe code met en place un assistant vocal capable de reconnaître et d’exécuter des commandes vocales en français, d’effectuer des recherches sur le web, et de répondre aux questions grâce à Wolfram Alpha et Wikipédia. Il constitue une base quelque peu solide pour un assistant personnel plus ou moins intelligent.\nTélécharger le fichier .python\nTélécharger la vidéo\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour à la page d’accueuil"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "",
    "text": "Accurate classification of brain tumors from magnetic resonance imaging (MRI) is essential for guiding therapeutic decisions and improving patient outcomes. In this study, we propose a deep learning approach based on transfer learning with the pre-trained EfficientNetB5 convolutional neural network. A dataset of 6 012 T1-weighted MR images comprising gliomas (2 004 images), meningiomas (2 004 images), and other tumor types (2 004 images) was split into training (70 %), validation (15 %), and test (15 %) sets. Images were resized to 224 × 224 pixels, normalized, and augmented through random rotations (±15°), zooms (±20 %), and horizontal flips. EfficientNetB5’s convolutional base was frozen up to layer 95, and a custom classifier head (GlobalAveragePooling2D → Dense(128, ReLU) → Dense(3, softmax)) was fine-tuned using the Adam optimizer (learning rate 1 × 10⁻⁵) with early stopping on validation loss.\nOn the independent test set (907 images), our fine-tuned EfficientNetB5 achieved an overall accuracy of 99.78 %, with only three misclassifications (2/906). Per-class metrics were as follows:\n\nGlioma: Precision 99,67 %, Recall 100 %, F1-score 99,83 % (support 302)\nMeningioma: Precision 100 %, Recall 99,34 %, F1-score 99,67 % (support 302)\nOther tumors: Precision 99,67 %, Recall 100 %, F1-score 99,83 % (support 302)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#abstract",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#abstract",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "",
    "text": "Accurate classification of brain tumors from magnetic resonance imaging (MRI) is essential for guiding therapeutic decisions and improving patient outcomes. In this study, we propose a deep learning approach based on transfer learning with the pre-trained EfficientNetB5 convolutional neural network. A dataset of 6 012 T1-weighted MR images comprising gliomas (2 004 images), meningiomas (2 004 images), and other tumor types (2 004 images) was split into training (70 %), validation (15 %), and test (15 %) sets. Images were resized to 224 × 224 pixels, normalized, and augmented through random rotations (±15°), zooms (±20 %), and horizontal flips. EfficientNetB5’s convolutional base was frozen up to layer 95, and a custom classifier head (GlobalAveragePooling2D → Dense(128, ReLU) → Dense(3, softmax)) was fine-tuned using the Adam optimizer (learning rate 1 × 10⁻⁵) with early stopping on validation loss.\nOn the independent test set (907 images), our fine-tuned EfficientNetB5 achieved an overall accuracy of 99.78 %, with only three misclassifications (2/906). Per-class metrics were as follows:\n\nGlioma: Precision 99,67 %, Recall 100 %, F1-score 99,83 % (support 302)\nMeningioma: Precision 100 %, Recall 99,34 %, F1-score 99,67 % (support 302)\nOther tumors: Precision 99,67 %, Recall 100 %, F1-score 99,83 % (support 302)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#résumé",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#résumé",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "Résumé",
    "text": "Résumé\nLa classification précise des tumeurs cérébrales à partir de l’imagerie par résonance magnétique (IRM) est essentielle pour guider les décisions thérapeutiques et améliorer les résultats pour les patients. Dans cette étude, nous proposons une approche d’apprentissage profond basée sur l’apprentissage par transfert avec le réseau neuronal convolutionnel EfficientNetB5 pré-entraîné. Un ensemble de données de 6 012 images RM pondérées en T1 comprenant des gliomes (2 004 images), des méningiomes (2 004 images), et des autres types de tumeurs (2 004 images) a été divisé en ensembles d’entraînement (70 %), ensembles de validation (15 %), et ensembles de test (15 %). Les images ont été redimensionnées à 224 × 224 pixels, normalisées et augmentées par des rotations aléatoires (±15°), des zooms (±20 %) et des retournements horizontaux. La base convolutive d’EfficientNetB5 a été gelée jusqu’à la couche 95, et une tête de classificateur personnalisée (GlobalAveragePooling2D → Dense(128, ReLU) → Dense(3, softmax)) a été affinée à l’aide de l’optimiseur Adam (taux d’apprentissage 1 × 10-⁵) avec arrêt anticipé sur la perte de validation.\nSur l’ensemble de test indépendant (907 images), notre EfficientNetB5 affiné a atteint une précision globale de 99,78 %, avec seulement trois erreurs de classification (2/906). Les mesures par classe sont les suivantes :\n\nGliome : Précision 99,67 %, Rappel 100 %, F1-score 99,83 % (support 302)\nMéningiome` : Précision 100 %, Rappel 99,34 %, Score F1 99,67 % (support 302)\nAutres tumeurs : Précision 99,67 %, Rappel 100 %, F1-score 99,83 % (support 302)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#introduction",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#introduction",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "Introduction",
    "text": "Introduction\n      Les tumeurs cérébrales représentent un enjeu majeur de santé publique en raison de leur complexité diagnostique et de leurs implications cliniques graves. Classifier précisément ces tumeurs, notamment les méningiomes, les gliomes et les tumeurs hypophysaires, est essentiel pour guider les décisions thérapeutiques et améliorer le pronostic des patients (WHO2021?).\n      Selon la 5e édition de la classification de l’Organisation Mondiale de la Santé (OMS), une approche intégrée reposant à la fois sur des critères histopathologiques et moléculaires est désormais recommandée pour le diagnostic des tumeurs du système nerveux central (WHO2021?). Cependant, l’interprétation des images médicales, en particulier des IRM cérébrales, reste un défi complexe et chronophage pour les professionnels de santé. Dans ce contexte, les méthodes d’intelligence artificielle, notamment les réseaux de neurones convolutifs (CNN), ont montré un potentiel prometteur pour automatiser la classification des tumeurs à partir d’images IRM.\n      Une revue menée par Xie et al. (Xie et al. 2022) souligne les avancées récentes dans l’application des CNN à la classification des tumeurs cérébrales, en insistant sur les défis techniques rencontrés comme le surapprentissage, le déséquilibre des classes, ou encore la nécessité d’intégrer la classification moléculaire. D’autres travaux, tels que celui de Rasheed et al. (Rasheed et al. 2023), proposent un modèle CNN personnalisé pour différencier automatiquement les IRM de trois types de tumeurs avec une grande précision, tout en mettant en avant l’importance du prétraitement des images pour améliorer la performance du modèle.\n      En parallèle, Tummala et al. (Tummala et al. 2022) introduisent une approche combinée utilisant les transformeurs visuels (Vision Transformers, ViT) avec les CNN pour augmenter la robustesse et la précision du modèle, démontrant ainsi la pertinence des modèles hybrides. Dans le même esprit, Srinivasan et al. (Srinivasan et al. 2024) conçoivent un modèle profond et hybride adapté à la classification multi-classes, en combinant plusieurs architectures CNN avec des stratégies d’optimisation.\n      Dans cette étude, nous proposons de développer un modèle basé sur un réseau de neurones convolutif (CNN) pour classifier les IRM cérébrales en trois types de tumeurs : gliomes, méningiomes et tumeurs hypophysaires. Cette approche vise à fournir un outil efficace d’aide au diagnostic, en s’appuyant sur les méthodes récentes les plus performantes issues de la littérature."
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#méthodologie",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#méthodologie",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "Méthodologie",
    "text": "Méthodologie\n\nNote\nLe notebook ainsi que toutes ses dépendances sont disponibles sur GitHub :\nCliquez ici pour cccéder au dépôt\n\n\nSource des données\n      Les données ont été téléchargées sous forme d’images depuis la plateforme Kaggle (en accès libre Cliquez ici pour accéder à la page). Elles sont réparties en trois sous-groupes :\n\nbrain_menin (2004 images) pour la méningiome, une tumeur généralement bénigne des méninges (les membranes entourant le cerveau);\nbrain_glioma (2004 images) pour le gliome, une tumeur maligne issue des cellules gliales, souvent infiltrante et agressive;\nbrain_tumor (2048 images) pour, éventuellement, les autres types de tumeurs cérébrales, souvent malignes, incluant diverses localisations et origines cellulaires.\n\nAu total, la base de données contient donc 6056 images.\n\n\nTraitement des images\n\n\n\nGestion des doublons\n\n\n\n      Il est crucial, avant d’entraîner un modèle sur une base de données d’images, de vérifier qu’elle ne contient pas d’images en double. Cela permet d’éviter de nombreux problèmes, notamment une évaluation faussée du modèle. Après vérification, nous avons identifié 44 doublons dans la base de données. Ceux-ci ont donc été supprimés de la base locale, ce qui porte le nombre total d’images à 6012 au lieu de 6056.\n\n\n\nPartionnement des données\n\n\n\n      Avant de commencer la phase de classification, les images ont été réparties aléatoirement dans trois répertoires selon les proportions suivantes:\n\ntrain : 70 % des images (entraînement)\nval : 15 % des images (validation)\ntest : 15 % des images (test)\n\nPlus explicitement :\n\nLe dossier train sert à entraîner le modèle.\nLe dossier val est utilisé pour valider le modèle à chaque itération, ce qui permet d’ajuster les paramètres et de minimiser le score de perte (calculé à partir de la fonction de perte/fonction objective) grâce à l’optimiseur (ici, Adam).\nLe dossier test permet d’évaluer la performance finale du modèle sur des données qu’il n’a jamais vues.\n\nChaque répertoire contient les trois classes de tumeurs cérébrales: brain_menin, brain_tumor et brain_glioma que les données nous fournissaient.\n\n\n\nVerification des doublons dans les différents dossiers (Train/Val/Test)\n\n\n\n      Après avoir réparti les images dans les différents dossiers, nous avons effectué un test supplémentaire afin de vérifier qu’aucune image ne se retrouvait à la fois dans les ensembles train, val ou test, que ce soit en double ou dans plusieurs ensembles simultanément. Les vérifications ont confirmé que tout était en ordre.\nCela a permis de s’assurer que chaque dossier contient des images uniques, garantissant ainsi une séparation stricte des données pour un entraînement, une validation et un test fiables du modèle.\n\n\n\nPréparation et chargement des images\n\n\n\nPour l’entraînement et la validation, les images sont traitées à l’aide de générateurs Keras (ImageDataGenerator):\n\nDimensionnement : toutes les images sont redimensionnées à 224×224 pixels, taille d’entrée standard pour de nombreux réseaux pré-entraînés.\nBatch size : on fixe le nombre d’images traitées simultanément à chaque pas d’entraînement.\nData augmentation :\n\nEntraînement :\n\nNormalisation des pixels : passage de l’échelle [0, 255] à l’échelle [0,1]\nRotation aléatoire jusqu’à ±15° (rotation_range=15)\nZoom aléatoire jusqu’à 20 % (zoom_range=0.2)\nFlip horizontal aléatoire (horizontal_flip=True)\n\nValidation et test :\n\nSeule la normalisation des pixels (de [0, 255] à [0, 1]), afin d’évaluer le modèle sur des images aux orientations et échelles réelles.\n\n\n\n\n\nModèle utilisé\n      Pour répondre à la problématique de classification des tumeurs cérébrales à partir d’images, nous avons opté pour l’utilisation d’un réseau de neurones convolutifs (CNN). Plutôt que de construire un modèle à partir de zéro — ce qui aurait été risqué compte tenu de la taille relativement modeste du jeu de données et des ressources de calcul disponibles —, nous avons choisi de recourir à une approche de transfert d’apprentissage.\n      Plus précisément, nous avons utilisé le modèle EfficientNetB5, un CNN préentraîné sur le vaste ensemble de données ImageNet. Ce modèle présente un excellent compromis entre performance, rapidité et taille du modèle, ce qui le rend particulièrement adapté pour des tâches de classification d’images médicales où les ressources peuvent être limitées.\nDans le cadre de cette approche :\n\nLes couches convolutionnelles profondes du modèle ont été conservées pour exploiter leur capacité à extraire des caractéristiques visuelles de bas niveau (bords, textures, formes, etc.);\nLes couches supérieures (à partir de la 95e couche dans notre cas) ont été désactivées (non gelées) et réentraînées sur notre propre base de données, afin d’adapter le modèle aux spécificités des tumeurs cérébrales.\n\nCette technique permet de bénéficier des connaissances générales acquises par le modèle tout en l’adaptant finement à notre problème spécifique. En effet, les modèles préentraînés comme EfficientNet ne sont pas directement adaptés aux tâches ciblées des data scientists. Il est donc crucial de les affiner (fine-tuning) sur des données spécifiques pour améliorer leur capacité à détecter des motifs propres au domaine médical, tels que les contours et anomalies propres aux IRM cérébrales.\nEnfin, construire un réseau de neurones entièrement personnalisé aurait pu exposer notre solution à des risques de surapprentissage ou à des difficultés d’optimisation, sans compter les contraintes computationnelles qui auraient ralenti considérablement le processus.\n\n\n\nConstruction du modèle avec EfficientNetB5\n\n\n\n      Pour la phase de modélisation, nous avons utilisé le modèle EfficientNetB5, préentraîné sur ImageNet. Ce modèle est particulièrement performant pour la classification d’images complexes et convient bien à des tâches médicales exigeantes en précision.\nNous avons chargé EfficientNetB5 sans ses couches de sortie (paramètre include_top=False) afin de pouvoir personnaliser l’architecture en sortie. L’entrée du modèle est spécifiée avec la taille (224, 224, 3) et le 3 correspond aux cannaux de couleurs (RGB : Rouge-Vert-Bleu) correspondant à nos images redimensionnées. Cependant IRM sont affichées en niveaux de gris souvent pour mieux visualiser les structure de cerveau. Or EfficientNet attend normalement des images de la forme (3, H, W) et les images de niveaux gris sont de la forme (1, H, W). On serait donc tenté de les convertir en “faux RGB” (3 canaux identiques). Toutefois, le mode des images a été vérifié et celles-ci sont bien en RGB. Elle sont en noires blancs, mais elles ont trois cannaux et chaque canal contiendrait les mêmes valeurs ou une version identique. Ainsi les images ont été laissées telles quelles.\nNous avons ensuite :\n\nGelé les poids du modèle préentraîné pour ne pas altérer les connaissances acquises sur ImageNet lors d’un premier entraînement ;\nAjouté un GlobalAveragePooling2D, qui réduit la dimensionnalité tout en conservant les caractéristiques importantes ;\nAjouté une couche dense de 128 neurones avec la fonction d’activation ReLU ;\nEt enfin une couche de sortie avec 3 neurones, activée par une fonction softmax pour la classification des trois types de tumeurs : méningiome, gliome et autres tumeurs cérébrales.\n\nLe modèle a été compilé avec :\n\nL’optimiseur Adam, très utilisé pour sa rapidité de convergence,\nUne taux d’apprentissage très faible (0.00001) pour éviter les grandes variations de poids à cause du gel partiel,\nLa fonction de perte categorical_crossentropy, adaptée à une classification multiclasse,\nEt comme métrique de performance : l’accuracy.\n\n\n\n\nPhase de fine-tuning (dégel progressif)\n\n\n\n      Pour mieux adapter le modèle aux spécificités de nos données, nous avons procédé à un fine-tuning partiel :\n\nLe modèle a été rendu entièrement entraînable (base_model.trainable = True) ;\nAfin d’éviter une modification brutale des poids et une possible dégradation des performances, les 95 premières couches ont été gelées, et seules les couches à partir de la 96e ont été entraînées. Cette technique permet au modèle de conserver ses caractéristiques basiques tout en affinant ses couches supérieures pour s’adapter à notre tâche spécifique ;\nLe modèle a été compilé avec le même taux d’apprentissage très faible (1e-5) afin de permettre une phase de fine-tuning progressive, en évitant des modifications brusques des poids et en assurant une convergence stable ;\nUn entraînement initialement prévu sur 50 époques a été lancé, avec des callback :\n\nEarlyStopping (monitor=val_loss, patience=3, restore_best_weights=True) pour arrêter automatiquement l’apprentissage au meilleur point de validation (après trois époques consécutifs sans que la valeur du score de perte de la validation ne soit inférieuer à sa plus pétite valeure), et un ModelCheckpoint pour sauvegarder le modèle de val_loss minimal.\nReduceLROnPlateau : un scheduler (planificateur) de taux d’apprentissage qui réduit le learning rate lorsque la performance du modèle ne s’améliore plus après un certain nombre d’époques (patience = 3)\n\n\n\n\nEvaluation du modèle\n      Une fois l’entraînement terminé, le modèle sera évalué sur un jeu de données de test indépendant afin de mesurer sa capacité à classer correctement les différentes classes de tumeurs cérébrales. Cette évaluation repose sur plusieurs métriques standard qui quantifient la performance du modèle en termes de justesse, précision et rappel.\n\n\n\nDéfinitions des métriques de classification\n\n\n\n\nTP (Vrais positifs) : nombre d’images bien classées dans leur vraie classe.\n\nFP (Faux positifs) : nombre d’images mal classées dans cette classe alors qu’elles n’y appartiennent pas.\n\nFN (Faux négatifs) : nombre d’images appartenant à cette classe mais mal classées dans une autre.\n\nTN (Vrais négatifs) : nombre d’images bien exclues de cette classe.\n\n\n\n\nFormules\n\n\n\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\n\\]\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP} \\quad\n\\]\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN} \\quad\n\\]\n\\[\nF1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\n\nCorrespondance des classes\n\n\n\n\nClasse 1 : Gliome\n\nClasse 2 : Méningiome\n\nClasse 3 : Autre tumeur"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#résultats",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#résultats",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "Résultats",
    "text": "Résultats\n\nEchantillons des images téléchargées\n      Les images ci-dessous sont des échantillons de celles qui serviront de base pour l’entraînement, la validation et le test du modèle. Celles affichées sont choisies aléatoirement dans au sein de chaque classe.\n\nTumeur méningiome\n\n\n\n\n\n\nFigure 1: Echantillons des images de la méningiome collectées\n\n\n\n\n\nTumeur gliome\n\n\n\n\n\n\nFigure 2: Echantillons des images de la gliome collectées\n\n\n\n\n\nAutres types de tumeurs\n\n\n\n\n\n\nFigure 3: Echantillons des images des autres types de tumeurs collectées\n\n\n\n\n      A l’oeil nu, il m’est personnellement impossible de pouvoir classer ses images sur la base de critères solides.\n\n\nRésultats et validation du modèle\n\n\n\nEvolution des pertes et des exacatidues durant l’entraînement\n\n\n\n\n\n\n\n\nFigure 4: Évolution de l’exactitude et de la perte pendant l’entraînement\n\n\n\n\n      Le modèle s’est arrêté à l’époque 44, mais les meilleurs poids (paramètres) ont été restaurés à l’époque 41, conformément au mécanisme d’early stopping avec une patience fixée à 3. Sur la figure ci-dessus, on observe qu’au cours des premières époques (environ jusqu’à la troisième), l’exactitude de l’entraînement (courbe verte) était légèrement supérieure à celle de la validation (courbe bleue). Toutefois, entre la quatrième et la onzième époque, l’exactitude de validation a dépassé de manière notable celle de l’entraînement. Cette progression peut s’expliquer par l’ajustement progressif des poids du modèle, sous l’effet de l’optimiseur, qui améliore les performances globales, y compris sur les données de validation.\n      Sur la suite de l’entraînement, on constate que l’exactitude de validation reste globalement légèrement supérieure à celle de l’entraînement, tandis que la perte de validation (courbe orange) reste plus basse que la perte d’entraînement (courbe rouge). Ce comportement, bien que contre-intuitif, peut s’expliquer par un ensemble de validation plus homogène ou moins bruité, ou encore par des effets de régularisation implicites induits par la structure du modèle ou les callbacks utilisés.\n      Enfin, à partir de la trentième époque environ, toutes les courbes se stabilisent autour de valeurs proches de 1 pour les exactitudes, et un peu proches de 0 pour les pertes, ce qui témoigne d’une excellente capacité de généralisation du modèle sans signe apparent de surapprentissage (overfitting).\n\n\n\nMatrice de confusion (test)\n\n\n\n\n\n\n\n\nFigure 5: Matrice de confusion\n\n\n\n\n      La matrice de confusion montre une excellente performance du modèle, avec seulement trois erreurs de classification sur 904 images de test. Le modèle atteint un rappel parfait (100 %) pour les classes gliome et autres tumeurs, et une précision parfaite (100 %) pour la classe méningiome.\nLes F1-scores dépassent 99 % dans chaque cas, confirmant une capacité remarquable à différencier les types de tumeurs cérébrales.\nCes résultats témoignent d’un modèle bien entraîné, capable de généraliser efficacement sur des données de validation, même dans un contexte de classification multiclasse sensible comme celui des diagnostics de tumeurs cérébrales.\n\n\n\nRésultats par classe\n\n\n\n\n\n\n\nTable 1: Métriques en pourentage par classe\n\n\nClasse\nPrécision\nRappel\nF1.score\nSupport\n\n\n\n\nGliome\n99.67\n100.00\n99.83\n302\n\n\nMéningiome\n100.00\n99.34\n99.67\n302\n\n\nAutre tumeur\n99.67\n100.00\n99.83\n302\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Métriques globales en pourentage\n\n\nMétrique\nPrécision\nRappel\nF1.score\nTotal\n\n\n\n\nExactitude (Accuracy)\n99.78\n99.78\n99.78\n906\n\n\nMoyenne macro\n99.67\n99.67\n99.67\n906\n\n\nMoyenne pondérée\n99.67\n99.67\n99.67\n906\n\n\n\n\n\n\n\n\n\n\n\nInterprétation des métriques\n\n\n\nLes résultats obtenus montrent que le modèle de classification des tumeurs cérébrales atteint une excellente performance sur l’ensemble de test, avec des scores de précision, rappel et F1-score supérieurs à 99 % pour chaque classe.\n\nClasse 1 (Gliome) :\n\nLa précision de 99,67 % indique que lorsque le modèle prédit un gliome, il se trompe très rarement (environ 0,33 % de faux positifs).\nLe rappel parfait à 100 % signifie que toutes les images de gliome ont été correctement détectées, sans aucun faux négatif.\nLe F1-score élevé de 99,83 % traduit un excellent compromis entre précision et rappel, assurant une classification fiable pour cette classe.\n\nClasse 2 (Méningiome) :\n\nUne précision parfaite de 100 % montre que toutes les images classées comme méningiome sont effectivement correctes, sans aucun faux positif.\nUn rappel de 99,34 % indique que quelques images de méningiome ont été classées à tort dans une autre catégorie (quelques faux négatifs).\nLe F1-score de 99,67 % confirme une très bonne performance globale, avec un bon équilibre entre détection et exactitude.\n\nClasse 3 (Autre tumeur) :\n\nLa précision de 99,67 % montre que le modèle fait très peu d’erreurs positives pour cette classe.\nLe rappel parfait à 100 % signifie qu’aucune image de cette classe n’a été manquée (pas de faux négatifs).\nLe F1-score de 99,83 % met en évidence la très haute qualité de la classification pour cette catégorie.\n\nMoyennes globales (macro et pondérée) :\n\nLes scores globaux supérieurs à 99,67 % en précision, rappel et F1-score montrent que le modèle est à la fois robuste et équilibré dans sa performance.\nLe fait que précision et rappel soient très proches dans toutes les classes indique une capacité du modèle à détecter les tumeurs avec fiabilité tout en limitant les fausses alertes.\n\n\n\n\n\nDiscussions des résultats\n      Notre modèle de classification des tumeurs cérébrales a atteint une exactitude de 99,78 % sur les données de test, avec une valeur de perte (loss) très faible de 1,35 %. La matrice de confusion révèle une excellente performance, avec très peu d’erreurs entre les classes. Par exemple, seules quelques images ont été mal classées, et aucune confusion n’a été observée pour la classe des autres tumeurs.\nAu cours de l’entraînement, le score de perte diminuait de manière continue à chaque itération, aussi bien pour l’ensemble d’apprentissage que pour celui de validation. Cette évolution parallèle et cohérente des courbes de perte constitue un indice fort d’absence de surapprentissage (overfitting). Le modèle semble ainsi avoir trouvé un bon compromis entre mémorisation des données d’entraînement et capacité de généralisation.\nCes résultats se comparent favorablement à ceux présentés dans la littérature. Tummala et al. ((Tummala et al. 2022)), utilisant un ensemble de Vision Transformers, rapportent une précision de 99,12 %, tandis que (Rasheed et al. 2023) obtiennent 98,72 % avec un modèle CNN. D’autres études, comme celles de (Srinivasan et al. 2024) et (Xie et al. 2022), rapportent également des précisions comprises entre 97 % et 99 %, mais sur des volumes de données souvent plus restreints.\n      Enfin, il convient de souligner que notre jeu de données comportait environ 6000 images, soit un volume environ deux fois plus important que dans certaines des études précédentes, ce qui pourrait contribuer à renforcer la fiabilité de l’évaluation. Toutefois, bien que ces résultats soient très encourageants, il reste important de rester prudent. Des facteurs tels que la diversité des images, la qualité des annotations, ou encore la sélection des hyperparamètres peuvent influencer les performances.\n      Ainsi, même si notre approche semble compétitive par rapport à certaines méthodes récentes, une validation sur des jeux de données externes ou en conditions cliniques réelles serait nécessaire pour évaluer pleinement sa robustesse et sa généralisabilité. Notre objectif n’est pas tant de surpasser les méthodes existantes que de proposer une solution fiable, reproductible, et adaptée au contexte spécifique de notre étude."
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#conclusion",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#conclusion",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "Conclusion",
    "text": "Conclusion\n      Les résultats obtenus à l’issue de l’entraînement et de la validation du modèle indiquent que celui-ci :\n\nApprend efficacement les motifs caractéristiques des différentes classes de tumeurs cérébrales à partir des images IRM ;\nGénéralise correctement sur des données non vues, ce qui est essentiel dans une perspective d’application clinique ;\nNe présente pas de signe manifeste de surapprentissage, comme en témoigne la faible différence entre les métriques d’entraînement et de validation (accuracy et loss).\n\nLa cohérence de l’évolution des scores de perte durant l’entraînement, tant sur les données d’apprentissage que de validation, confirme la stabilité du modèle et son bon ajustement au problème de classification multiclasse.\nLe modèle est très performant pour distinguer les différentes classes de tumeurs cérébrales sur les images IRM, avec très peu d’erreurs, ce qui est crucial dans un contexte clinique. Les faux positifs et faux négatifs sont très faibles, ce qui minimise le risque d’erreur de diagnostic.\n\n\n\nÀ nuancer\n\n\n\nLa très haute performance obtenue peut être en partie liée à la taille importante du jeu de données, qui a permis un apprentissage plus robuste. En effet, nous avons environ deux fois plus d’images que dans certaines études comparables.\nCela rend la comparaison directe avec les résultats des articles précédents plus délicate, car un jeu de données plus grand favorise généralement de meilleures performances, mais peut aussi cacher des variations dans la qualité ou la diversité des images.\nEnfin, malgré ces résultats encourageants, il est essentiel de tester le modèle sur des données externes indépendantes pour confirmer sa capacité à généraliser en conditions réelles.\n\n\n\n\nPerspectives\n\n\n      Un prolongement naturel de ce travail consisterait à explorer la localisation de la tumeur en complément de sa classification. En ce sens, l’entraînement d’un modèle de type YOLO (You Only Look Once) pourrait permettre d’identifier automatiquement les zones suspectes sur une IRM en encadrant précisément la position de la tumeur.\nCependant, la mise en œuvre d’un tel modèle demanderait des ressources de calcul importantes, notamment en raison de la complexité des architectures de détection et de la nécessité de disposer d’annotations spatiales précises (bounding boxes). Cela constitue un défi technique, mais également une étape prometteuse vers un outil d’aide au diagnostic plus complet."
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexes",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexes",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "Annexes",
    "text": "Annexes"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexes-1-data-augmentation",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexes-1-data-augmentation",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "Annexes 1 : Data augmentation",
    "text": "Annexes 1 : Data augmentation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=15,\n    zoom_range=0.2,\n    horizontal_flip=True\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_gen = train_datagen.flow_from_directory(\n    'data_ml_efficient_net/train',\n     target_size=(224, 224),\n     batch_size=32,\n     class_mode='categorical'\n)\n\nval_gen = val_datagen.flow_from_directory(\n    'data_ml_efficient_net/val',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical'\n)\n\ntest_gen = train_gen.flow_from_directory(\n    'data_ml_efficient_net/val',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical',\n    shuffle = False\n)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexe-2-construction-du-modèle",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexe-2-construction-du-modèle",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "Annexe 2 : Construction du modèle",
    "text": "Annexe 2 : Construction du modèle\n# Construction du modèle\nbase_model = EfficientNetB5(weights='imagenet', include_top=False, input_shape=(*image_size, 3))\n\n# Dégel des couches du modèle \nbase_model.trainable = True\n\n# Gel les premières couches pour ne pas tout ré-entraîner\nfor layer in base_model.layers[:95]:\n    layer.trainable = False\n\n# Ajout la tête de classification\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation='relu')(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compilation avec un LR très bas pour le fine-tuning\nmodel.compile(\n    optimizer=Adam(learning_rate=1e-5),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Callbacks\ncallbacks = [\n    EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-7,\n        verbose=1\n    ),\n    ModelCheckpoint(\n        'best_model.h5',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n]\n\n# Entraînement\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=50,\n    callbacks=callbacks\n)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexe-3-détails-de-calculs-des-métriques",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#annexe-3-détails-de-calculs-des-métriques",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "Annexe 3: Détails de calculs des métriques",
    "text": "Annexe 3: Détails de calculs des métriques\n\nMatrice de confusion (test)\n\n\\[\n\\begin{bmatrix}\n302 & 0 & 0 \\\\\n1 & 300 & 1 \\\\\n0 & 0 & 302 \\\\\n\\end{bmatrix}\n\\]\n\nClasse 1 (Gliome) :\n\n\\(TP = 302\\)\n\n\\(FP = 0\\) (images d’autres classes classées comme Gliome)\n\n\\(FN = 1\\) (images Gliome mal classées)\n\n\\[\n\\text{Précision} = \\frac{302}{302 + 1} = 0.9967\n\\]\n\\[\n\\text{Rappel} = \\frac{302}{302 + 0} = 1.0\n\\]\n\\[\nF1 = 2 \\times \\frac{0.9967 \\times 1.0}{0.9967 + 1.0} = 0.9983\n\\]\nClasse 2 (Méningiome) :\n\n\\(TP = 300\\)\n\n\\(FP = 2\\)\n\n\\(FN = 0\\)\n\n\\[\n\\text{Précision} = \\frac{300}{300} = 1\n\\]\n\\[\n\\text{Rappel} = \\frac{300}{300 + 2} = 0.9934\n\\]\n\\[\nF1 = 2 \\times \\frac{1.0 \\times 0.9934}{1.0 + 0.9934} = 0.9967\n\\]\nClasse 3 (Autre tumeur) :\n\n\\(TP = 302\\)\n\n\\(FP = 0\\)\n\n\\(FN = 1\\)\n\n\\[\n\\text{Précision} = \\frac{302}{302 + 1} = 0.9967\n\\]\n\\[\n\\text{Rappel} = \\frac{302}{302 + 0} = 1.0\n\\]\n\\[\nF1 = 2 \\times \\frac{0.9967 \\times 1.0}{0.9967 + 1.0} = 0.9983\n\\]"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#moyennes-globales",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#moyennes-globales",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "Moyennes globales",
    "text": "Moyennes globales\nSupport total : \\(300 + 300 + 307 = 907\\)\n\nMacro-average (moyenne simple) :\n\n\\[\n\\text{Précision}_{macro} = \\frac{0.9967 + 1.0 + 0.9967}{3} = 0.9978\n\\]\n\\[\n\\text{Rappel}_{macro} = \\frac{1.0 + 0.9934 + 1.0}{3} = 0.9978\n\\]\n\\[\nF1_{macro} = \\frac{0.9983 + 0.9967 + 0.9983}{3} = 0.9978\n\\]\n\nWeighted-average (moyenne pondérée) :\n\n\\[\n\\text{Précision}_{weighted} = \\frac{(302 \\times 0.9967) + (302 \\times 1.0) + (302 \\times 0.9967)}{906} = 0.9978\n\\]\n\\[\n\\text{Rappel}_{weighted} = \\frac{(302 \\times 1.0) + (302 \\times 0.9934) + (302 \\times 1.0)}{906} = 0.9978\n\\]\n\\[\nF1_{weighted} = \\frac{(302 \\times 0.9983) + (302 \\times 0.9967) + (302 \\times 0.9983)}{906} = 0.9978\n\\]"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#liste-des-sigles-et-abréviations",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#liste-des-sigles-et-abréviations",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "LISTE DES SIGLES ET ABRÉVIATIONS",
    "text": "LISTE DES SIGLES ET ABRÉVIATIONS\n\n\n\n\n\n\n\nSigle\nSignification\n\n\n\n\nCNN\nConvolutional Neural Network (Réseau de Neurones Convolutif)\n\n\nIRM\nImagerie par Résonance Magnétique\n\n\nOMS\nOrganisation Mondiale de la Santé\n\n\nViT\nVision Transformer\n\n\nReLU\nRectified Linear Unit (fonction d’activation)\n\n\nRGB\nRouge, Vert, Bleu (canaux de couleur)\n\n\nYOLO\nYou Only Look Once"
  },
  {
    "objectID": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#references-bibliographiques",
    "href": "INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html#references-bibliographiques",
    "title": "Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation",
    "section": "REFERENCES BIBLIOGRAPHIQUES",
    "text": "REFERENCES BIBLIOGRAPHIQUES\nLIVRE : L’apprentissage Profond avec Python, Les meilleures pratiques de François Chollet (Une base en optimisation et méthodes de calculs numériques pourrait être utile pour une compréhension moins superficielle du conténu du livre)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "",
    "text": "Comment cela fonctionne\n\n\n\nJe tiens tout d’abord à rapperler que je n’utilise pas de modèle NLP pour créer ce petit assistant virtuel. J’avais écris ce progamme en 2021, donc bien evidemment les outils utilisés ont évolué et donc vous pourrez l’ajuster à votre guise. Le code source sera téléchargeable à la fin de la page."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html#la-svm-cest-quoi-et-à-quoi-ça-sert",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html#la-svm-cest-quoi-et-à-quoi-ça-sert",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "LA SVM : C’est quoi et à quoi ça sert ?",
    "text": "LA SVM : C’est quoi et à quoi ça sert ?\n\nUtilité :\n\n\nPrincipe de la SVM pour la classification binaire"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html#test-du-code",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html#test-du-code",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "Test du code",
    "text": "Test du code"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html#conclusion",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html#conclusion",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "Conclusion",
    "text": "Conclusion\nCe code met en place un assistant vocal capable de reconnaître et d’exécuter des commandes vocales en français, d’effectuer des recherches sur le web, et de répondre aux questions grâce à Wolfram Alpha et Wikipédia. Il constitue une base quelque peu solide pour un assistant personnel plus ou moins intelligent.\nTélécharger le fichier .python\nTélécharger la vidéo\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour à la page d’accueuil"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "",
    "text": "Dans cet article, nous étudions la classification des tumeurs mammaires à partir d’un jeu de données biomédical déséquilibré, en combinant des approches statistiques et d’apprentissage automatique. Une analyse exploratoire incluant une analyse en composantes principales (ACP) a permis de réduire la dimensionnalité tout en préservant la structure des données, validée par une classification non supervisée par KMeans. Une modélisation statistique par régression logistique pondérée a ensuite mis en évidence deux variables fortement prédictives : le périmètre moyen et le nombre moyen de points concaves, avec des odds ratios respectifs de 9,7 et 25,0.\nDans une seconde phase, deux modèles supervisés — la régression logistique et le K plus proches voisins (KNN) — ont été comparés après sélection rigoureuse des variables. La régression logistique s’est révélée plus performante en termes de robustesse, de capacité de généralisation et de discrimination (AUC = 0,98), tandis que le KNN a offert un bon compromis entre précision et rappel. Enfin, nous suggérons que pour des objectifs purement prédictifs, des modèles plus complexes comme XGBoost pourraient être envisagés, au prix d’une interprétabilité réduite.\nMots-clés : Analyse en composantes principales, Sélection de variables, K-plus proches voisins, Régression logistique, k-means, Classification, Données médicales, Tumeurs mammaires\n\nIn this study, we address the classification of breast tumors using a highly imbalanced biomedical dataset, combining statistical methods and machine learning techniques. An exploratory analysis including Principal Component Analysis (PCA) was used to reduce dimensionality while preserving data structure, which was validated through unsupervised clustering with KMeans. A weighted logistic regression model identified two major predictors: mean perimeter and mean number of concave points, with odds ratios of 9.7 and 25.0, respectively.\nIn the second phase, two supervised models — logistic regression and k-nearest neighbors (KNN) — were compared following rigorous variable selection. Logistic regression demonstrated superior robustness, generalization ability, and discriminative power (AUC = 0.98), while KNN offered a strong balance between precision and recall. For purely predictive applications, we recommend exploring more complex models such as XGBoost, which may increase accuracy at the cost of interpretability.\nKeywords: Principal Component Analysis, Variable Selection, K-Nearest Neighbours, Logistic Regression, k-means, Classification, Medical Data, Breast Tumors.\n\n\n      Le cancer représente aujourd’hui l’une des principales causes de morbidité et de mortalité dans le monde. Selon les estimations GLOBOCAN 2020, près de 19,3 millions de nouveaux cas de cancer et 10 millions de décès liés à cette maladie ont été enregistrés à l’échelle mondiale en 2020 (Sung et al. 2021). Cette charge devrait atteindre 28,4 millions de nouveaux cas d’ici 2040, soit une augmentation de 47 %, avec une progression particulièrement marquée dans les pays en développement.\nLe cancer du sein a désormais dépassé le cancer du poumon comme cancer le plus fréquemment diagnostiqué dans le monde, avec 2,3 millions de nouveaux cas en 2020 (soit 11,7 % de l’ensemble des cancers). Il constitue également l’une des principales causes de décès par cancer chez la femme, avec environ 685 000 décès en 2020, soit 6,9 % des décès par cancer (Organization 2021; Sung et al. 2021). Cette prévalence élevée, associée à une forte létalité dans les pays à ressources limitées, fait du cancer du sein un enjeu majeur de santé publique mondiale.\nLes inégalités géographiques sont notables : alors que l’incidence du cancer du sein est plus élevée dans les pays industrialisés, les taux de mortalité y sont généralement plus faibles, grâce à une détection précoce et à une meilleure prise en charge. À l’inverse, dans de nombreux pays en transition, le diagnostic est souvent posé à un stade avancé, expliquant des taux de mortalité proportionnellement plus élevés (Sung et al. 2021).\nDans ce contexte, il devient essentiel de mieux comprendre les facteurs explicatifs de la malignité des tumeurs mammaires afin de guider les stratégies de dépistage, de prévention et de traitement. L’analyse de données cliniques et la modélisation statistique peuvent contribuer à cet objectif en identifiant des variables discriminantes et en développant des modèles prédictifs robustes.\nC’est dans cette perspective que s’inscrit ce travail, qui poursuit deux objectifs complémentaires :\n\nIdentifier les variables cliniques les plus explicatives du caractère bénin ou malin d’une tumeur mammaire;\nAjuster un modèle de machine learning capable de prédire efficacement la nature de la tumeur à partir de ces variables.\n\nEn atteignant ces objectifs, notre démarche vise à renforcer l’applicabilité des outils prédictifs dans un cadre médical, tout en assurant une meilleure compréhension des dimensions sous-jacentes à la classification tumorale."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#introduction",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#introduction",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "",
    "text": "Le cancer représente aujourd’hui l’une des principales causes de morbidité et de mortalité dans le monde. Selon les estimations GLOBOCAN 2020, près de 19,3 millions de nouveaux cas de cancer et 10 millions de décès liés à cette maladie ont été enregistrés à l’échelle mondiale en 2020 (Sung et al. 2021). Cette charge devrait atteindre 28,4 millions de nouveaux cas d’ici 2040, soit une augmentation de 47 %, avec une progression particulièrement marquée dans les pays en développement.\nLe cancer du sein a désormais dépassé le cancer du poumon comme cancer le plus fréquemment diagnostiqué dans le monde, avec 2,3 millions de nouveaux cas en 2020 (soit 11,7 % de l’ensemble des cancers). Il constitue également l’une des principales causes de décès par cancer chez la femme, avec environ 685 000 décès en 2020, soit 6,9 % des décès par cancer (Organization 2021; Sung et al. 2021). Cette prévalence élevée, associée à une forte létalité dans les pays à ressources limitées, fait du cancer du sein un enjeu majeur de santé publique mondiale.\nLes inégalités géographiques sont notables : alors que l’incidence du cancer du sein est plus élevée dans les pays industrialisés, les taux de mortalité y sont généralement plus faibles, grâce à une détection précoce et à une meilleure prise en charge. À l’inverse, dans de nombreux pays en transition, le diagnostic est souvent posé à un stade avancé, expliquant des taux de mortalité proportionnellement plus élevés (Sung et al. 2021).\nDans ce contexte, il devient essentiel de mieux comprendre les facteurs explicatifs de la malignité des tumeurs mammaires afin de guider les stratégies de dépistage, de prévention et de traitement. L’analyse de données cliniques et la modélisation statistique peuvent contribuer à cet objectif en identifiant des variables discriminantes et en développant des modèles prédictifs robustes.\nC’est dans cette perspective que s’inscrit ce travail, qui poursuit deux objectifs complémentaires :\n\nIdentifier les variables cliniques les plus explicatives du caractère bénin ou malin d’une tumeur mammaire;\nAjuster un modèle de machine learning capable de prédire efficacement la nature de la tumeur à partir de ces variables.\n\nEn atteignant ces objectifs, notre démarche vise à renforcer l’applicabilité des outils prédictifs dans un cadre médical, tout en assurant une meilleure compréhension des dimensions sous-jacentes à la classification tumorale."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#données-utilisées",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#données-utilisées",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "1. Données utilisées",
    "text": "1. Données utilisées\n      L’étude s’appuie sur le jeu de données Wisconsin Breast Cancer Diagnostic (WBCD), mis à disposition par l’Université du Wisconsin sur kaggle (Wolberg, W.H., et al., 1995). Il contient 569 observations issues d’analyses de prélèvements mammaires, chacune décrite par 30 variables numériques mesurant des caractéristiques morphologiques des noyaux cellulaires (moyenne, écart-type et valeur extrême de la texture, surface, concavité, etc.). La variable cible est binaire et indique si la tumeur est maligne (M) ou bénigne (B)."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#prétraitement-des-données",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#prétraitement-des-données",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "2. Prétraitement des données",
    "text": "2. Prétraitement des données\n      Les étapes suivantes ont été réalisées :\n\nSuppression de l’identifiant non informatif (id),\nfiltrage des variables : seules les mesures de moyenne (se terminant par _mean) ont été retenues pour l’analyse initiale;\nstandardisation : toutes les variables ont été centrées et réduites afin d’éviter que leur échelle influence les analyses;\nvérification de la complétude : le jeu de données ne contient pas de valeurs manquantes."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#analyse-en-composantes-principales-acp",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#analyse-en-composantes-principales-acp",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "3. Analyse en Composantes Principales (ACP)",
    "text": "3. Analyse en Composantes Principales (ACP)\n      Une ACP a été effectuée sur les variables normalisées afin de réduire la dimensionnalité tout en préservant l’information maximale (Jolliffe and Cadima 2016; Abdi and Williams 2010). Les deux premières composantes principales ont été retenues car elles expliquent ensemble plus de 80% de la variance totale.\nL’ACP a permis de :\n\nvisualiser les individus dans un plan factoriel 2D (tumeur maligne vs bénigne);\nétudier la structure des variables à l’aide du cercle des corrélations;\nidentifier les variables les plus contributives à l’axe de séparation entre les deux classes;\n\nCette étape a aussi guidé la sélection finale des variables explicatives, en complément des tests statistiques."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#classification-non-supervisée-par-k-means",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#classification-non-supervisée-par-k-means",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "4. Classification non supervisée par K-Means",
    "text": "4. Classification non supervisée par K-Means\n      Afin d’analyser la structure intrinsèque des données, une classification non supervisée a été réalisée à l’aide de l’algorithme K-Means (MacQueen 1967). Le nombre de clusters a été fixé à k = 2, en cohérence avec les deux catégories diagnostiques.\nL’algorithme a été appliqué sur les composantes principales issues de l’ACP, ce qui présente deux avantages :\n\nréduction du bruit (moins de dimensions);\nvisualisation claire des regroupements.\n\nLes clusters obtenus ont ensuite été comparés aux vraies classes pour évaluer le pouvoir de séparation naturelle des données. Cette approche permet aussi de valider la pertinence des axes factoriels retenus dans l’ACP."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#sélection-des-variables-explicatives",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#sélection-des-variables-explicatives",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "5. Sélection des variables explicatives",
    "text": "5. Sélection des variables explicatives\n      Une série de tests statistiques a été conduite pour identifier les variables présentant une différence significative entre les tumeurs bénignes et malignes :\n\nNormalité des variables testée via le test de Shapiro-Wilk;\nHomogénéité des variances testée avec le test de Levene;\nChoix du test de comparaison approprié (ANOVA ou test de Kruskal-Wallis);\nCalcul de l’indice \\(\\eta^2\\) (eta squared) pour estimer la proportion de variance expliquée."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#modélisation-statistique---regression-logistique",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#modélisation-statistique---regression-logistique",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "6. Modélisation statistique - Regression logistique",
    "text": "6. Modélisation statistique - Regression logistique\n      La régression logistique binaire a été utilisée pour estimer la probabilité qu’une tumeur soit maligne en fonction des variables explicatives sélectionnées (Hosmer, Lemeshow, and Sturdivant 2013). Ce modèle statistique repose sur la relation log-linéaire entre les covariables et le logit de la probabilité d’appartenir à la classe maligne.\nL’un des avantages majeurs de la régression logistique réside dans son interprétabilité : chaque coefficient estimé peut être interprété comme un effet multiplicatif sur les odds de malignité. Ainsi, une variable avec un coefficient positif augmente la probabilité qu’une tumeur soit maligne, et inversement pour un coefficient négatif.\nLes résultats du modèle permettent non seulement de classifier les tumeurs, mais aussi d’identifier les facteurs les plus associés à la malignité, ce qui offre une lecture médico-biologique utile."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#modélisation-supervisé-machine-learnig",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#modélisation-supervisé-machine-learnig",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "7. Modélisation supervisé : Machine Learnig",
    "text": "7. Modélisation supervisé : Machine Learnig\n\n7.1 Modèles utilisés\nAfin de comparer les approches classiques et les méthodes d’apprentissage automatique, deux modèles ont été entraînés pour la prédiction du type de tumeur :\n\nRégression logistique (appliquée ici comme algorithme de machine learning) (Hosmer, Lemeshow, and Sturdivant 2013),\nK-plus proches voisins (K-NN) avec \\(k = 5\\) (Cover and Hart 1967).\n\nLe jeu de données a été aléatoirement divisé en deux sous-ensembles :\n\nEntraînement : 80 % des données,\nTest : 20 % des données.\n\n\n\n7.2 Régression logistique (Machine Learning)\n      Le modèle de régression logistique a été ajusté sur l’échantillon d’apprentissage, en prenant en compte les variables préalablement sélectionnées. L’entraînement a permis d’identifier les variables les plus influentes sur le score de prédiction, tout en conservant une bonne capacité généralisable sur les données de test.\nLes performances ont été évaluées à l’aide de l’aire sous la courbe ROC (AUC), du rappel, de la précision et du F1-score.\n\n\n7.3 K-plus proches voisins (K-NN)\nLe modèle K-NN classe une observation en fonction des classes majoritaires de ses k voisins les plus proches (Cover and Hart 1967). Le choix du paramètre k = 2 a été déterminé via validation croisée sur l’échantillon d’apprentissage.\nLa distance euclidienne a été utilisée comme métrique de proximité. Un soin particulier a été apporté pour éviter le surapprentissage, en équilibrant biais et variance lors de l’optimisation du modèle."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#évaluation-des-performances",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#évaluation-des-performances",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "8. Évaluation des performances",
    "text": "8. Évaluation des performances\n      Le modèle de régression logistique statistique a été évalué à l’aide des tests de significativité des coefficients, ainsi que de la qualité de l’ajustement global.\nQuant aux modèles de machine learning, ils ont été évalués sur l’échantillon test selon plusieurs indicateurs de performance :\n\nExactitude (Accuracy) : proportion globale de bonnes classifications,\nPrécision (Precision) : proportion de vraies tumeurs malignes parmi les prédictions positives,\nRappel (Recall) : proportion de vraies tumeurs malignes correctement détectées,\nF1-score : compromis entre précision et rappel,\nAUC-ROC : mesure de la performance globale du modèle pour discriminer les deux classes.\n\nCes métriques permettent une comparaison objective des performances et de la robustesse des modèles utilisés.\n\n\nL’ACP est faite en amont pour éviter la multicolinéarité et améliorer la robustesse des modèles.\nLe clustering permet de découvrir la structure latente des données avant la classification."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#exploration-des-données",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#exploration-des-données",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Exploration des données",
    "text": "Exploration des données\nL’exploration des données comprend :\n\nLe résumé statistique descriptif des variables\n\nLa distribution de la variable cible\n\nL’étude des corrélations entre variables\n\nCes étapes permettent de mieux comprendre la structure et les caractéristiques de la base avant modélisation."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#description-des-variables",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#description-des-variables",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Description des variables",
    "text": "Description des variables\nLes variables de cette base sont construites à partir de l’analyse de noyaux de cellules détectés dans des images médicales.\n\nid (int) : Identifiant unique de l’observation (patient).\ndiagnosis (catégorielle) Variable cible binaire :\n\nM : Malignant (maligne)\nB : Benign (bénigne)\n\nCaractéristiques mesurées\n\nPour chaque noyau de cellule, 10 mesures statistiques ont été calculées, puis la moyenne, l’écart-type (erreur standard, noté se), et la valeur extrême (worst) ont été rapportés :\n\n\n\nMesures de base :\n\n\n\nCes mesures sont disponibles en 3 versions chacune : .mean, .se, .worst\n\n\n\n\n\n\n\nVariable de base\nSignification\n\n\n\n\nradius\nRayon moyen du noyau\n\n\ntexture\nÉcart-type des valeurs de niveaux de gris\n\n\nperimeter\nPérimètre du noyau\n\n\narea\nSurface du noyau\n\n\nsmoothness\nRégularité des contours (valeurs faibles = plus lisses)\n\n\ncompactness\nCompacité = (périmètre² / surface) - 1.0\n\n\nconcavity\nGravité des concavités dans les contours\n\n\nconcave points\nNombre de points concaves sur les contours\n\n\nsymmetry\nSymétrie de la cellule\n\n\nfractal dimension\nMesure de la “rugosité” des contours\n\n\n\nChaque mesure est donc déclinée en :\n\n*_mean\n*_se\n*_worst\n\nPar exemple :\n\nradius_mean, radius_se, radius_worst\ntexture_mean, texture_se, texture_worst\n…\nfractal_dimension_mean, fractal_dimension_se, fractal_dimension_worst\n\nCe qui donne au total 30 variables quantitatives."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#résumé-des-types-de-variables",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#résumé-des-types-de-variables",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Résumé des types de variables",
    "text": "Résumé des types de variables\n\n\n\n\n\n\n\n\nType de variable\nNom\nNombre\n\n\n\n\nIdentifiant\nid\n1\n\n\nCible binaire\ndiagnosis\n1\n\n\nVariables numériques (×10 mesures ×3 stats)\n*_mean, *_se, *_worst\n30"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#analyse-exploratoire",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#analyse-exploratoire",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Analyse exploratoire",
    "text": "Analyse exploratoire\n\nQuelques statistiques descriptives\n\n\nDistribution des variables\n\n\n\n\n\n\nFigure 1: Distribution des variables\n\n\n\n\n      Ce graphique illustre la distribution des variables explicatives. On observe une hétérogénéité d’échelle importante entre les différentes variables, ce qui justifie l’application préalable d’une standardisation (centrage-réduction) avant toute analyse multivariée ou modélisation.\n      Par ailleurs, la distribution de la variable réponse révèle un déséquilibre notable entre les classes : la proportion de tumeurs bénignes est environ deux fois inférieure à celle des tumeurs malignes. Ce déséquilibre pourrait influencer les performances de certains modèles de classification, en particulier ceux sensibles à la distribution des classes, et devra être pris en compte dans l’évaluation.\n\nCorrélogramme des variables quantitatives\n\n\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0.5, 0, 'radius_mean'), Text(1.5, 0, 'texture_mean'), Text(2.5, 0, 'perimeter_mean'), Text(3.5, 0, 'area_mean'), Text(4.5, 0, 'smoothness_mean'), Text(5.5, 0, 'compactness_mean'), Text(6.5, 0, 'concavity_mean'), Text(7.5, 0, 'concave points_mean'), Text(8.5, 0, 'symmetry_mean'), Text(9.5, 0, 'fractal_dimension_mean')])\n\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0, 0.5, 'radius_mean'), Text(0, 1.5, 'texture_mean'), Text(0, 2.5, 'perimeter_mean'), Text(0, 3.5, 'area_mean'), Text(0, 4.5, 'smoothness_mean'), Text(0, 5.5, 'compactness_mean'), Text(0, 6.5, 'concavity_mean'), Text(0, 7.5, 'concave points_mean'), Text(0, 8.5, 'symmetry_mean'), Text(0, 9.5, 'fractal_dimension_mean')])\n\n\n\n\n\nFigure 2: Corrélogramme des variables quantitatives statistiquement significatives\n\n\n\n\n      Une analyse des corrélations entre les variables explicatives montre que certaines variables sont fortement corrélées entre elles, tandis que d’autres présentent des corrélations plus faibles. Toutefois, une simple observation graphique ne permet pas de conclure sur la significativité statistique de ces associations.\nPour approfondir cette analyse, un matrice de corrélations de pearson annotée avec les p-values a été utilisée. Cela permet de distinguer :\n\nles corrélations statistiquement significatives,\ndes associations qui, bien que visuellement marquées, ne sont pas significatives au seuil de 5%,\nainsi que des variables non corrélées entre elles, mais corrélées à d’autres.\nCette redondance entre variables justifie pleinement le recours à une analyse en composantes principales (ACP)."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#analyse-en-composantes-principales",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#analyse-en-composantes-principales",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Analyse en composantes principales",
    "text": "Analyse en composantes principales\n      Pour cette partie spéciquement ainsi que pour la partie statistique du modèle logistique, nous utiliserons le langage R au lieu de python car il est plus facile à prendre en main (avis personnel). Mais pour la partie machine learning nous utiliserons le langage Python.\nL’ ACP permettra d’éliminer les variables corrélées entre elles en ne gardant que les plus contributives à la formation des axes que nous choisirons (pour plus de détails visitez ma publication Reduction de dimensionnalité, clustering non supervisé).\n      Les deux premiers axes factoriels ont été sélectionné car ceux-ci permettent de conserver plus de 80% de l’information contenue dans les données (voir annexe 1).\n\nAnalyses des variables\n\n\n\n\n\n\nFigure 3: Cartes de la representation des variables sur les dimensions 1 et 2\n\n\n\n\n      Cette figure met en évidence les principales relations entre les variables dans le plan des premières composantes principales. Elle permet de formuler plusieurs observations importantes :\n\nLa variable texture_mean apparaît comme la moins contributive. Elle présente une très faible corrélation avec les composantes principales retenues, ce qui indique qu’elle n’apporte pas d’information significative pour la séparation entre les groupes.\nLes variables radius_mean, area_mean et perimeter_mean sont très fortement corrélées entre elles. Elles contribuent de manière presque équivalente à la construction des axes principaux de l’analyse. Cette redondance est attendue, car ces variables décrivent toutes la taille du noyau de la cellule, selon des relations géométriques bien établies :\n\\[\n\\text{Périmètre} \\approx 2\\pi \\times \\text{Rayon}, \\quad \\text{Aire} \\approx \\pi \\times \\text{Rayon}^2\n\\]\nCes relations expliquent que ces trois variables transmettent une information similaire. Pour éviter la redondance, il est judicieux de n’en conserver qu’une seule dans les analyses ultérieures.\nLes variables smoothness_mean et symmetry_mean sont corrélées, mais smoothness_mean se distingue par une contribution plus forte à l’analyse. Elle décrit la régularité des contours, un critère pertinent pour la caractérisation morphologique, tandis que symmetry_mean apporte une information redondante et de moindre poids explicatif.\nLes variables concavity_mean et concave points_mean sont également corrélées. Toutefois, concave points_mean est retenue car elle contribue davantage à la structure globale révélée par l’ACP. Ces variables sont liées à la présence d’irrégularités dans les contours cellulaires, souvent caractéristiques des tumeurs malignes.\nD’autres variables, comme compactness_mean, fractal_dimension_mean et smoothness_mean, apportent des informations complémentaires. Par exemple :\n\ncompactness_mean exprime le rapport entre la surface et le périmètre, donc la forme générale,\nfractal_dimension_mean mesure la complexité des contours, et reflète l’irrégularité des bords.\n\n\nBien que certaines variables soient corrélées, elles capturent des aspects distincts de la morphologie des noyaux. Cela justifie leur présence dans une approche multivariée.\n\nSélection de variables redondantes : cas de radius_mean, area_mean et perimeter_mean\nPour choisir entre les variables radius_mean, area_mean et perimeter_mean, qui sont fortement corrélées, une procédure rigoureuse de sélection est mise en place, fondée sur l’analyse de la variance (ANOVA). Avant d’appliquer ce test, deux conditions doivent être vérifiées :\n\nNormalité des données dans chaque groupe (B et M) via le test de Shapiro-Wilk.\nHomogénéité des variances entre les groupes via le test de Levene.\n\nEn fonction des résultats, le test statistique approprié est sélectionné :\n\nSi les deux conditions sont remplies, une ANOVA classique est appliquée.\nSi les données sont normales mais les variances sont différentes, on utilise l’ANOVA de Welch.\nEn cas de non-normalité, un test non paramétrique est privilégié :\n\nWilcoxon-Mann-Whitney pour deux groupes,\nou Kruskal-Wallis pour plus de deux groupes.\n\n\nCe processus a été automatisé à l’aide d’une fonction personnalisée. Les détails des tests (Shapiro-Wilk, Levene et choix final) sont présentés en annexe.\n      Une analyse statistique a été menée pour évaluer si certaines variables différencient significativement les tumeurs bénignes des tumeurs malignes. Les tests de normalité (par groupe), les tests d’homogénéité de variances, ainsi que le test non paramétrique de Wilcoxon-Mann-Whitney ont été appliqués pour cela.\nConcernant la variable radius_mean, les tests de normalité indiquent une distribution non normale, notamment pour le groupe M (p = 0.0019), et le test de Levene rejette l’hypothèse d’homogénéité des variances (p = 0). Le test de Wilcoxon-Mann-Whitney donne un p-value de 0, indiquant une différence significative entre les distributions des deux groupes. On conclut donc que radius_mean permet de distinguer les tumeurs selon leur nature.\nDe même, pour area_mean, la normalité n’est pas respectée dans les deux groupes(p = 0.0228 pour B, et p = 0 pour M), les variances ne sont pas homogènes (p = 0), et le test de Wilcoxon-Mann-Whitney rejette également l’égalité des distributions entre les groupes (p = 0). Cette variable est donc statistiquement discriminante.\nEnfin, perimeter_mean présente aussi une non-normalité pour le groupe M (p = 4e-04) et des variances inégales (p = 0). Le test de Wilcoxon conclut à une différence significative des distributions entre les deux types de tumeurs.\nCes résultats soutiennent l’idée que ces trois variables contribuent de manière significative à la séparation entre tumeurs bénignes et malignes, et justifient leur inclusion dans les étapes de sélection des variables explicatives.\nCependant, ces variables sont fortement corrélées entre elles (\\(r &gt; 0.95\\)). Une telle redondance peut induire des problèmes de multicolinéarité dans les modèles statistiques ou de machine learning, rendant l’interprétation des coefficients difficile et dégradant la performance prédictive.\nPour remédier à cela, nous proposons de sélectionner une seule variable représentative parmi ces variables fortement corrélées. Le critère retenu est le rapport de corrélation avec la variable cible (diagnosis). Ainsi, parmi les variables redondantes, nous conservons celle ayant la plus forte corrélation absolue avec la classe à prédire. Ce choix permet de maximiser la contribution informative tout en éliminant les doublons.\n\nradius_mean : \\(\\eta^2 = 0{,}5329416\\)\n\narea_mean : \\(\\eta^2 = 0{,}5026581\\)\n\nperimeter_mean : \\(\\eta^2 = 0{,}5515075\\)\n\nNous retenons donc perimeter_mean, qui présente le rapport de corrélation le plus élevé avec la variable réponse.\n\n\nVariables sélectionnées\nLes variables retenues pour l’analyse finale sont :\n\nperimeter_mean : mesure de la taille du noyau,\nsmoothness_mean : régularité des contours,\nfractal_dimension_mean : complexité des bords,\nconcave points_mean : présence de concavités.\nConclusion partielle\n\n      L’analyse en composantes principales (ACP) a mis en évidence des groupes de variables fortement corrélées, traduisant des caractéristiques géométriques proches. Afin d’éviter la redondance d’information et les effets de multicolinéarité, nous avons conservé, pour chaque groupe, la variable la plus représentative.\nCette sélection permet de construire un modèle plus parcimonieux, tout en conservant des dimensions complémentaires — liées à la forme, la texture ou la complexité morphologique — essentielles à une bonne discrimination des types de tumeurs."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#les-kmeans",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#les-kmeans",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Les Kmeans",
    "text": "Les Kmeans\n\n\n\n\n\n      Comme les deux premières composantes principales expliquent plus de 80 % de la variance totale, nous les retenons pour la suite de l’analyse. Cette réduction de dimension permet de conserver l’essentiel de l’information tout en facilitant la visualisation et les analyses ultérieures.\nDans le cadre de la classification non supervisée, nous appliquons ensuite l’algorithme K-Means avec un nombre de clusters fixé à 2, en cohérence avec la nature binaire de la variable réponse (diagnosis). L’objectif est d’évaluer si les observations peuvent être regroupées automatiquement en deux groupes distincts, reflétant ou non la distinction entre tumeurs bénignes et malignes.\n      La classification K-Means appliquée aux deux premières composantes principales permet une séparation relativement nette des observations en deux groupes.\nLe tableau ci-dessous présente la répartition des observations selon les clusters identifiés par l’algorithme K-Means et leur étiquette réelle (diagnosis).\n\n\n\nCluster\nBénigne (B)\nMaligne (M)\n\n\n\n\n1\n8\n163\n\n\n2\n349\n49\n\n\n\nOn observe que :\n\nCluster 1 est majoritairement composé de tumeurs malignes (163 cas M contre 8 cas B), ce qui suggère que ce groupe correspond essentiellement aux observations de type malin.\nCluster 2 regroupe principalement des tumeurs bénignes (349 cas B contre 49 cas M), ce qui permet de l’associer globalement à la classe bénigne.\n\nCette séparation indique que la structure latente des données, projetée dans le sous-espace factoriel, permet déjà une discrimination naturelle entre les deux types de tumeurs, sans supervision. Cela renforce l’idée que les variables sélectionnées sont pertinentes pour la classification.\n\n\n\nReprésentations graphiques\n\n\n\n\n\n\n\n\nFigure 4: Classification des patients par la méthode des K-moyennes\n\n\n\n\n      Sur cette figure On observe que les patients sont pratiquement linéairement séparables dans le plan défini par les deux premières composantes principales.\nCela signifie que la projection sur ces deux dimensions met bien en évidence une séparation claire entre les deux groupes (bénins et malins), ce qui est cohérent avec la qualité du clustering obtenu par k-means.\n\nCette bonne séparation visuelle corrobore la pertinence des variables sélectionnées et confirme que les composantes principales résument efficacement la variance utile à la distinction des diagnostics. Nous pouvons passer à présent aux différents modèles de prédiction (regression logistique et k plus plus proches voisins)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#influence-des-caractéristiques-tumorales-sur-le-diagnostic-quelle-importance-pour-chaque-variable",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#influence-des-caractéristiques-tumorales-sur-le-diagnostic-quelle-importance-pour-chaque-variable",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Influence des caractéristiques tumorales sur le diagnostic : quelle importance pour chaque variable ?",
    "text": "Influence des caractéristiques tumorales sur le diagnostic : quelle importance pour chaque variable ?\n    Pour répondre à cette question, nous avons procédé à une modélisation statistique à l’aide d’une régression logistique classique. Les variables explicatives ont été standardisées (centrées et réduites) afin de faciliter la comparaison de leurs effets respectifs et de stabiliser l’estimation des coefficients. Cette standardisation permet une interprétation relative : un coefficient traduit alors l’effet d’une variation d’un écart-type de la variable sur la probabilité d’un diagnostic malin. Toutefois, cela rend l’interprétation directe dans l’unité d’origine moins intuitive.\nPar ailleurs, la variable cible diagnosis présente un déséquilibre important : les cas bénins (B) sont nettement plus nombreux que les cas malins (M). Ce déséquilibre peut induire un biais dans l’apprentissage, poussant le modèle à privilégier la classe majoritaire, au détriment de la détection des cas malins, pourtant critiques sur le plan médical.\nPour corriger ce biais, nous avons introduit une pondération des observations dans le modèle logistique. Plus précisément, chaque observation de la classe minoritaire (M) a reçu un poids proportionnel au rapport entre les proportions des classes dans l’échantillon.\nCette stratégie permet d’obtenir un modèle plus équitable dans sa capacité à prédire correctement les deux types de tumeurs, en particulier les cas malins qui constituent l’enjeu principal de détection dans un contexte médical.\n\n\n\n\n\n\nTable 1:  Résultats de la regression logistique \n  \n    \n      Characteristic\n      OR\n      95% CI\n      p-value\n    \n  \n  \n    (Intercept)\n0.97\n0.71, 1.34\n0.9\n    perimeter_mean\n9.64\n3.43, 28.9\n&lt;0.001\n    smoothness_mean\n1.19\n0.70, 2.05\n0.5\n    fractal_dimension_mean\n0.77\n0.46, 1.26\n0.3\n    concave points_mean\n25.1\n8.38, 82.0\n&lt;0.001\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, OR = Odds Ratio\n    \n  \n  \n\n\n\n\n\nInterprétation :\n      Nous avons ajusté un modèle de régression logistique sur les variables standardisées en tenant compte du déséquilibre entre les classes grâce à une pondération.\n\nUne augmentation d’un écart-type du périmètre moyen (perimeter_mean) multiplie par environ 9.7 les chances d’avoir un diagnostic malin, ce qui est très significatif (p &lt; 0.001).\nUne augmentation d’un écart-type du nombre moyen de points concaves (concave points_mean) multiplie par environ 25.0 les chances d’avoir un diagnostic malin, indiquant un effet très fort et hautement significatif (p &lt; 0.001).\nLes variables lissage moyen (smoothness_mean) et dimension fractale moyenne (fractal_dimension_mean) n’ont pas d’effet significatif sur le diagnostic, leurs odds ratios étant proches de 1 et leurs intervalles de confiance incluant 1.\n\nCes résultats montrent que le périmètre moyen et le nombre moyen de points concaves sont des prédicteurs majeurs du diagnostic malin.\nLa standardisation des variables permet une interprétation harmonisée des coefficients sur une même échelle, facilitant la comparaison de l’impact relatif de chaque variable."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#prédiction-du-diagnostic-par-apprentissage-automatique-logit-vs-knn",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#prédiction-du-diagnostic-par-apprentissage-automatique-logit-vs-knn",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Prédiction du diagnostic par apprentissage automatique Logit vs KNN",
    "text": "Prédiction du diagnostic par apprentissage automatique Logit vs KNN\n      Dans la partie régression logistique, nous avons utilisé l’argument class_weight='balanced' afin de corriger le déséquilibre des classes en attribuant automatiquement un poids inversement proportionnel à leur fréquence.\nPour le modèle K-plus proches voisins (K-NN), nous avons appliqué la technique SMOTE (Synthetic Minority Over-sampling Technique) afin de générer des observations synthétiques de la classe minoritaire dans les données d’apprentissage, améliorant ainsi la capacité du modèle à détecter les cas rares.\n\n\n\n\n\n\n\n(a) Resultat de notre modèle VS celui de Sci-kit Learn\n\n\n\n\n\n\n\n(b) Moyennes des erreurs quadratiques moyennes\n\n\n\n\nFigure 5: Métriques des modèles durant la validation croisée\n\n\n      La figure ci-dessus présente les métriques d’évaluations des différents modèles ajustés. Elle montre que la régression logistique affiche une excellente performance avec une accuracy moyenne de 91 % et un AUC ROC élevé de 97,5 %, témoignant d’une très bonne séparation des classes. Elle maintient un bon équilibre entre rappel (89,1 %) et précision (87,3 %), garantissant une détection fiable des positifs tout en limitant les faux positifs.\nDe son côté, le KNN avec SMOTE présente une performance solide avec une accuracy moyenne de 86,7 % et un AUC ROC de 95,5 %, reflétant une bonne capacité à distinguer les classes. Le modèle équilibre efficacement rappel (86,4 %) et précision (80,2 %), ce qui est essentiel pour détecter les cas positifs sans trop d’erreurs.\nLa régression logistique surpasse légèrement le KNN avec SMOTE en termes de performance globale, offrant une meilleure accuracy et un AUC ROC supérieur, tandis que le KNN reste compétitif grâce à un bon équilibre entre rappel et précision.\n\nOn applique SMOTE uniquement sur l’ensemble d’entraînement (jamais sur le test, pour éviter les fuites de données). On conserve le test original non modifié pour une évaluation honnête. La régression logistique gère le déséquilibre avec class_weight=‘balanced’. Le KNN ne gère pas ce paramètre, donc on utilise SMOTE pour créer artificiellement des exemples de la classe minoritaire dans l’entraînement.\n\n      Après avoir comparé les performances des modèles via la validation croisée, il est maintenant pertinent d’entraîner le modèle final sélectionné sur l’ensemble des données d’entraînement afin d’optimiser son apprentissage avant l’évaluation finale. Ceci étant le modèle a été ajusté sur données d’entraînement et ensuite évalué.\n\n\nLogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\n\nExactitude des modèles\n\n      Les modèles de régression logistique et de KNN présentent des exactitudes respectives sur les données d’entraînement de 91,21 % et 93,22 %. Sur les données de test, la régression logistique obtient une meilleure performance avec une accuracy de 93,57 %, tandis que le KNN atteint 90,06 %, indiquant que la régression logistique généralise légèrement mieux.\nLa régression logistique semble mieux généraliser que le KNN, avec une accuracy plus élevée sur les données de test malgré une performance légèrement inférieure à l’entraînement. Cela suggère que le modèle de régression logistique est moins sujet au surapprentissage et offre une meilleure robustesse pour la prédiction sur de nouvelles données. En revanche, le KNN, bien qu’ayant une meilleure performance à l’entraînement, montre une légère baisse en test, ce qui peut indiquer un certain surajustement aux données d’entraînement.\n\nLa fonction de coût ou de perte (log-loss)des modèles\n\n      La log-loss est une mesure utile pour la régression logistique, qui reflète la qualité probabiliste des prédictions.\n\nLa regression logistique :\n\nLog-loss train (0.1983) et log-loss test (0.1895) sont très proches, et la perte est même légèrement meilleure (plus faible) sur le test.\nCela indique que le modèle ne souffre pas de surapprentissage significatif — il généralise bien sur les données nouvelles.\n\nLe KNN :\n\nLe KNN n’optimise pas explicitement une fonction de perte pendant l’entraînement — c’est un algorithme instance-based, il mémorise les exemples.\nOn ne peut donc pas parler de “loss” au sens d’une fonction de coût minimisée.\nMais on peut évaluer la performance via la classification error \\((1 - accuracy)\\) ou d’autres métriques.\n\nMatrices de confusion\n\n\n\n\n\n\n\n\n(a) Matrice de confusion - Régression Logistique\n\n\n\n\n\n\n\n(b) Matrice de confusion - KNN\n\n\n\n\nFigure 6: Comparaison des ML (Logit vs KNN - Matrices de confusion)\n\n\n      La matrice de confusion, illustrée sur la figure ci-dessus, sert à évaluer la performance d’un modèle de classification en détaillant le nombre de prédictions correctes et incorrectes réparties entre les différentes classes. Elle permet de visualiser les vrais positifs, vrais négatifs, faux positifs et faux négatifs, offrant ainsi une compréhension précise des erreurs du modèle.\nDans ce cas, la matrice montre que la régression logistique réalise une meilleure classification que le KNN, avec plus de vrais positifs et vrais négatifs, et moins d’erreurs, ce qui confirme sa supériorité pour ce problème.\nPlus précisement, on observe que le modèle KNN présente une performance de prédiction légèrement inférieure par rapport au modèle de régression logistique, comme le montre la matrice de confusion :\n\n97 vrais négatifs (vrais bénins) contre 101 pour le modèle logit\n\n57 vrais positifs (vrais malins) contre 59 pour le modèle logit\n\n7 faux positifs (faux malins) contre 5 pour le modèle logit\n\n10 faux négatifs (faux bénins) contre 6 pour le modèle logit\n\nCes résultats indiquent que la régression logistique détecte un peu mieux les classes, avec moins d’erreurs de classification que le KNN.\n\nPrécision, recall, score F1, AUC\n\n\n\nRapport de classification des modèles ajustés\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\nRapport de classification - Regression logistique\n\n\n\nModèle\nClasse\nPrécision\nRecall\nF1_Score\nSupport\n\n\n\n\nRégression Logistique\nBenin\n0.9528\n0.9439\n0.9484\n107\n\n\nRégression Logistique\nMalin\n0.9077\n0.9219\n0.9147\n64\n\n\nRégression Logistique\nAccuracy\nNA\nNA\n0.9357\n171\n\n\nRégression Logistique\nMacro avg\n0.9303\n0.9329\n0.9315\n171\n\n\nRégression Logistique\nWeighted avg\n0.9359\n0.9357\n0.9358\n171\n\n\nRégression Logistique\nAUC ROC\n0.9816\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\nRapport de classification - KNN\n\n\n\nModèle\nClasse\nPrécision\nRecall\nF1_Score\nSupport\n\n\n\n\nKNN\nBenin\n0.9327\n0.9065\n0.9194\n107\n\n\nKNN\nMalin\n0.8507\n0.8906\n0.8702\n64\n\n\nKNN\nAccuracy\nNA\nNA\n0.9006\n171\n\n\nKNN\nMacro avg\n0.8917\n0.8986\n0.8948\n171\n\n\nKNN\nWeighted avg\n0.9020\n0.9006\nNA\n171\n\n\nKNN\nAUC ROC\n0.9572\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\nRapport de classification de la Régression Logistique\nExactitude globale (accuracy) : 93,03 %, ce qui signifie que le modèle classe correctement environ 93 patients sur 100.\nPrécision par classe :\n\nPour la classe bénigne (\\(B\\)), la précision est de 95,28 %, indiquant une bonne détection des cas bénins.\nPour la classe maligne (\\(M\\)), la précision est de 90,77 %, également satisfaisante. Mais cet écart de précision pourraît être attribué au déquilibre des reponses (Benin et Malin) et cela malgré la stratification et la pondération. Toutefois les métriques telles que F1-Scrore et AUC sont moins sensibles au déséquilibre des classes de la variable reponse (ici la catégorie de tumeur).\n\nRappel (sensibilité) :\n\nClasse bénigne : 94,39 %, montrant une bonne capacité à détecter les vrais positifs bénins.\n\nClasse maligne : 92,19 %, un peu moins élevée. L’explication donnée en ammont est également valide ici.\n\nF1-score : 94,84 % pour la classe bénigne et 91,47 % pour la classe maligne, indiquant un bon équilibre global entre précision et rappel.\nAUC ROC : 0,98116, proche de 1, ce qui montre une excellente capacité de discrimination.\n\n\n\nRapport de classification d uKNN (K plus proches voisins)\nExactitude globale (accuracy) : 91,26 %, moins bonne que celle la régression logistique.\nPrécision par classe :\n\nPour la classe bénigne (\\(B\\)), la précision est de 93,46 %, moins bonne que celle la régression logistique.\nPour la classe maligne (\\(M\\)), la précision est de 89, 06%, légèrement inférieure à la régression logistique.\n\nRappel (sensibilité) :\n\nClasse bénigne : 93,46 %, un peu plus faible que le rappel du modèle logit.\nClasse maligne : 89, 06%, meilleure que celui de la régression logistique.\n\nF1-score : 93,46 % pour la classe bénigne et 89,06 % pour la classe maligne. Ces scores sont tous deux inférieurs à celui de la régression logistique.\nAUC ROC : 0,9632, très élevé, mais un peu inférieur à celui de la régression logistique.\nConclusion partielle \n\nLes deux modèles présentent des performances globalement comparables pour différencier les patients bénins et malins :\n\nLa régression logistique obtient une meilleure précision globale ainsi qu’un meilleur équilibre du F1-score, notamment pour la classe bénigne, témoignant d’une bonne capacité à limiter les erreurs dans cette catégorie.\nLe KNN offre un rappel plus élevé pour la classe bénigne et une bonne AUC ROC, indiquant une capacité correcte de discrimination globale.\n\nCependant, la régression logistique se révèle plus stable et robuste, avec une meilleure capacité de généralisation. En effet, bien que le KNN affiche un rappel plus important, il présente un risque accru de surajustement, ce qui peut nuire à ses performances sur des données très différentes de celles utilisées pour l’entraînement."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-0-corrélogramme-des-variables-quantitatives",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-0-corrélogramme-des-variables-quantitatives",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Annexe 0 : Corrélogramme des variables quantitatives",
    "text": "Annexe 0 : Corrélogramme des variables quantitatives\n\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0.5, 0, 'radius_mean'), Text(1.5, 0, 'texture_mean'), Text(2.5, 0, 'perimeter_mean'), Text(3.5, 0, 'area_mean'), Text(4.5, 0, 'smoothness_mean'), Text(5.5, 0, 'compactness_mean'), Text(6.5, 0, 'concavity_mean'), Text(7.5, 0, 'concave points_mean'), Text(8.5, 0, 'symmetry_mean'), Text(9.5, 0, 'fractal_dimension_mean')])\n\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0, 0.5, 'radius_mean'), Text(0, 1.5, 'texture_mean'), Text(0, 2.5, 'perimeter_mean'), Text(0, 3.5, 'area_mean'), Text(0, 4.5, 'smoothness_mean'), Text(0, 5.5, 'compactness_mean'), Text(0, 6.5, 'concavity_mean'), Text(0, 7.5, 'concave points_mean'), Text(0, 8.5, 'symmetry_mean'), Text(0, 9.5, 'fractal_dimension_mean')])\n\n\n\n\n\nFigure 7: Corrélogramme des variables quantitatives"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-1-pourcentages-des-variances-expliquées-par-les-composantes-principales",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-1-pourcentages-des-variances-expliquées-par-les-composantes-principales",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Annexe 1 : Pourcentages des variances expliquées par les composantes principales",
    "text": "Annexe 1 : Pourcentages des variances expliquées par les composantes principales\n\n\n\n\n\nFigure 8: Diagramme des variances expliquées par les composantes principales\n\n\n\n\n      On observe le coude à partie de la troisième dimension. Mais en se basant sur le critère du taux d’inertie on a environ 80% de l’information conténue dans les données. Par conséquent notre analyse sera axée sur les deux premiers axes."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-2-hypothèses-et-interprétations-des-tests-statistiques",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-2-hypothèses-et-interprétations-des-tests-statistiques",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Annexe 2 : Hypothèses et interprétations des tests statistiques",
    "text": "Annexe 2 : Hypothèses et interprétations des tests statistiques\n\nTest de Shapiro-Wilk\nCe test permet de vérifier la normalité d’une distribution.\n\nHypothèses : \\[\n\\begin{cases}\nH_0 : \\text{Les données suivent une loi normale} \\\\\nH_1 : \\text{Les données ne suivent pas une loi normale}\n\\end{cases}\n\\]\nInterprétation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les données peuvent être considérées comme normales.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) les données ne sont pas normales.\n\n\n\n\n\nTest de Levene\nCe test permet de vérifier l’homogénéité des variances entre les groupes.\n\nHypothèses : \\[\n\\begin{cases}\nH_0 : \\text{Les variances des groupes sont égales} \\\\\nH_1 : \\text{Les variances des groupes sont différentes}\n\\end{cases}\n\\]\nInterprétation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les variances sont homogènes.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) les variances sont différentes.\n\n\n\n\n\nTest d’ANOVA classique\nCe test compare les moyennes de plusieurs groupes. Il nécessite que les données soient normales et que les variances soient homogènes.\n\nHypothèses : \\[\n\\begin{cases}\nH_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\nH_1 : \\exists \\, i \\ne j \\text{ tel que } \\mu_i \\ne \\mu_j\n\\end{cases}\n\\]\nInterprétation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les moyennes sont statistiquement égales.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) au moins une moyenne est différente.\n\n\n\n\n\nTest d’ANOVA de Welch\nCe test est une version robuste de l’ANOVA utilisée lorsque l’homogénéité des variances n’est pas respectée, mais que les données restent normales.\n\nHypothèses : \\[\n\\begin{cases}\nH_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\nH_1 : \\exists \\, i \\ne j \\text{ tel que } \\mu_i \\ne \\mu_j\n\\end{cases}\n\\]\nInterprétation de la \\(p\\)-valeur : identique à celle du test ANOVA classique.\n\n\n\n\nTest de Kruskal-Wallis\nTest non paramétrique utilisé en cas de non-normalité ou lorsque les données sont ordinales.\n\nHypothèses : \\[\n\\begin{cases}\nH_0 : \\text{Les distributions des groupes sont identiques} \\\\\nH_1 : \\text{Au moins une distribution est différente}\n\\end{cases}\n\\]\nInterprétation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les distributions sont considérées comme similaires.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) au moins une des distributions diffère significativement.\n\n\n\nRemarque : Tous ces tests renvoient une \\(p\\)-valeur qui est comparée au seuil de signification habituellement fixé à 5% (\\(\\alpha = 0.05\\)).\n\n\n\n###Analyse de : radius_mean selon diagnosis \n\n- Groupe B : p = 0.668 \n- Groupe M : p = 0.0019 \n\n- Test de Levene : p = 0 (variances non homogènes) \n\nDonnées non normales → Test de Wilcoxon-Mann-Whitney\n$$\\text{Test de Wilcoxon-Mann-Whitney}$$\n**Hypothèses :**\n- {H_0} : les distributions des deux groupes sont égales\n- {H_1} : les distributions sont différentes\n\n**Résultat du test :** p = 0 \n=&gt; Rejet de {H_0} : différence significative entre les groupes.\n\n\n\n\n###Analyse de : area_mean selon diagnosis \n\n- Groupe B : p = 0.0228 \n- Groupe M : p = 0 \n\n- Test de Levene : p = 0 (variances non homogènes) \n\nDonnées non normales → Test de Wilcoxon-Mann-Whitney\n$$\\text{Test de Wilcoxon-Mann-Whitney}$$\n**Hypothèses :**\n- {H_0} : les distributions des deux groupes sont égales\n- {H_1} : les distributions sont différentes\n\n**Résultat du test :** p = 0 \n=&gt; Rejet de {H_0} : différence significative entre les groupes.\n\n\n\n\n###Analyse de : perimeter_mean selon diagnosis \n\n- Groupe B : p = 0.7795 \n- Groupe M : p = 4e-04 \n\n- Test de Levene : p = 0 (variances non homogènes) \n\nDonnées non normales → Test de Wilcoxon-Mann-Whitney\n$$\\text{Test de Wilcoxon-Mann-Whitney}$$\n**Hypothèses :**\n- {H_0} : les distributions des deux groupes sont égales\n- {H_1} : les distributions sont différentes\n\n**Résultat du test :** p = 0 \n=&gt; Rejet de {H_0} : différence significative entre les groupes."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-4-rapport-de-corrélation",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-4-rapport-de-corrélation",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Annexe 4: Rapport de corrélation",
    "text": "Annexe 4: Rapport de corrélation\nLe rapport de corrélation \\(\\eta^2\\) est une mesure de l’effet qui quantifie la proportion de la variance expliquée par un facteur.\nIl est défini par la formule suivante :\n\\[\n\\eta^2 = \\frac{\\sum_{i=1}^{k} n_i (\\bar{y}_i - \\bar{y})^2}{\\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y})^2}\n\\]\noù :\n\n\\(k\\) est le nombre de groupes,\n\\(n_i\\) est la taille du groupe \\(i\\),\n\\(\\bar{y}_i\\) est la moyenne du groupe \\(i\\),\n\\(\\bar{y}\\) est la moyenne globale,\n\\(y_{ij}\\) est l’observation \\(j\\) du groupe \\(i\\).\n\nCette mesure permet d’évaluer l’ampleur de la différence entre les groupes, en indiquant la proportion de la variance totale attribuable à la variation entre groupes.\n\n\n[1] 0.5329416\n\n\n\n\n[1] 0.5026581\n\n\n\n\n[1] 0.5515075"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-5-pondération-des-observations-pour-corriger-le-déséquilibre-des-classes",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-5-pondération-des-observations-pour-corriger-le-déséquilibre-des-classes",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Annexe 5: Pondération des observations pour corriger le déséquilibre des classes",
    "text": "Annexe 5: Pondération des observations pour corriger le déséquilibre des classes\n      Dans notre jeu de données, la variable cible diagnosis est déséquilibrée : les tumeurs bénignes (B) sont plus fréquentes que les tumeurs malignes (M). Cette inégalité peut biaiser l’apprentissage du modèle en le poussant à favoriser la classe majoritaire, au détriment de la détection correcte des cas malins, pourtant plus critiques en pratique clinique.\nAfin de corriger ce déséquilibre, nous avons introduit une pondération des observations dans le modèle de régression logistique. Concrètement, chaque observation maligne s’est vue attribuer un poids défini comme suit :\n\\[\nw_i =\n\\begin{cases}\n1, & \\text{si } diagnosis_i = \\texttt{B} \\\\\n\\frac{p_B}{p_M}, & \\text{si } diagnosis_i = \\texttt{M}\n\\end{cases}\n\\]\noù \\(p_B\\) et \\(p_M\\) représentent respectivement les proportions de cas bénins et malins dans l’échantillon. Cette pondération permet de renforcer l’importance des observations rares (les cas malins) et de rééquilibrer l’influence des deux classes lors de l’ajustement du modèle.\nAinsi, le modèle devient plus robuste face au déséquilibre et améliore sa capacité à détecter les tumeurs malignes, ce qui est crucial dans un contexte médical."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-6-codes",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html#annexe-6-codes",
    "title": "Analyse statistique et apprentissage automatique pour la classification des tumeurs mammaires : étude de la régression logistique suivie d’une comparaison entre modèles logit et KNN",
    "section": "Annexe 6: Codes",
    "text": "Annexe 6: Codes\n\nChargement des packages, des données et standardisation des variables\n\n# Importation des librairies nécessaires\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom imblearn.over_sampling import SMOTE\n\ndf = df.loc[:, [\"diagnosis\", \"perimeter_mean\", \"smoothness_mean\", \"fractal_dimension_mean\", \"concave points_mean\"]]\n\n# Variables explicatives et cible\nX = df.drop(columns=['diagnosis'])\ny = df['diagnosis']\ny = y.map({'B': 0, 'M': 1})\n# Séparation train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Mise à l’échelle des variables\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nValidation croisée\n\nfrom sklearn.model_selection import StratifiedKFold, cross_validate\n\n# Définition du modèle logit\nlogit = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n\n# Validation croisée stratifiée à 5 plis\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Evaluation avec plusieurs métriques\nscores_logit = cross_validate(\n    logit,\n    X_train_scaled,\n    y_train,\n    cv=cv,\n    scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],\n    return_train_score=False\n)\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\n# Pipeline avec SMOTE + KNN\nknn_pipeline = ImbPipeline(steps=[\n    ('smote', SMOTE(random_state=42)),\n    ('knn', KNeighborsClassifier(n_neighbors=5))\n])\n\n# Validation croisée avec les mêmes métriques\nscores_knn = cross_validate(\n    knn_pipeline,\n    X_train_scaled,\n    y_train,\n    cv=cv,\n    scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],\n    return_train_score=False\n)\n\nAjustement du modèle sur l’ensemble d’entrainement\n\n# --- Modèle 1 : Régression Logistique ---\nlogit = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\nlogit.fit(X_train_scaled, y_train)\ny_pred_logit = logit.predict(X_test_scaled)\ny_proba_logit = logit.predict_proba(X_test_scaled)[:,1]\n\n# --- Modèle 2 : KNN ---\nsmote = SMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_res, y_train_res)\ny_pred_knn = knn.predict(X_test_scaled)\ny_proba_knn = knn.predict_proba(X_test_scaled)[:,1]\n\nEvaluation du modèle (métriques : train - test)\n\nfrom sklearn.metrics import accuracy_score\ny_train_pred_logit = logit.predict(X_train_scaled)\ntrain_accuracy_logit = accuracy_score(y_train_pred_logit, y_train)\n\ny_train_pred_knn = knn.predict(X_train_scaled)\ntrain_accuracy_knn = accuracy_score(y_train_pred_knn, y_train)\n\nprint(f\"Train Accuracy Logit: {train_accuracy_logit:.4f}\")\nprint(f\"Train Accuracy KNN: {train_accuracy_knn:.4f}\")\nfrom sklearn.metrics import accuracy_score\n\ny_train_pred_logit = logit.predict(X_train_scaled)\ntrain_accuracy_logit = accuracy_score(y_train_pred_logit, y_train)\n\ny_train_pred_knn = knn.predict(X_train_scaled)\ntrain_accuracy_knn = accuracy_score(y_train_pred_knn, y_train)\n\nprint(f\"Train Accuracy Logit: {train_accuracy_logit:.4f}\")\nprint(f\"Train Accuracy KNN: {train_accuracy_knn:.4f}\")\n\nExactitude du test des modèles\n\nacc_logit = accuracy_score(y_test, y_pred_logit)\nacc_knn = accuracy_score(y_test, y_pred_knn)\n\nprint(f\"Accuracy Régression Logistique : {acc_logit:.4f}\")\nprint(f\"Accuracy KNN : {acc_knn:.4f}\")\n\nFonction de perte\n\nfrom sklearn.metrics import log_loss\n\ntrain_loss_logit = log_loss(y_train, logit.predict_proba(X_train_scaled))\ntest_loss_logit = log_loss(y_test, logit.predict_proba(X_test_scaled))\n\nprint(f\"Log-loss train (logit): {train_loss_logit:.4f}\")\nprint(f\"Log-loss test (logit): {test_loss_logit:.4f}\")\n\nPrécision, recall, score F1, AUC\n\n#| code-fold: true\ndef eval_classif(y_true, y_pred, y_proba, model_name=\"Modèle\"):\n    print(f\"== Results for {model_name} ==\")\n    print(\"\\nClassification report : :\")\n    print(classification_report(y_true, y_pred, digits=4))\n    auc = roc_auc_score(y_true, y_proba)\n    print(f\"AUC ROC : {auc:.4f}\\n\")\n\neval_classif(y_test, y_pred_logit, y_proba_logit, \"Régression Logistique\")\neval_classif(y_test, y_pred_knn, y_proba_knn, \"KNN\")"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "",
    "text": "Cette étude présente une approche méthodologique combinant analyse en composantes principales (ACP), sélection de variables, et modélisation par k plus proches voisins (KNN) ainsi que régression logistique pour la classification de données médicales. L’objectif principal était d’identifier les variables les plus discriminantes et d’évaluer la performance prédictive des modèles. Les résultats montrent que la réduction de dimension facilite l’interprétation, tandis que les deux modèles de classification offrent des performances satisfaisantes, avec un bon compromis entre précision et interprétabilité. Cette méthode robuste peut être étendue à d’autres jeux de données similaires.\nCe projet est un exercice personnel visant à mettre en pratique les notions acquises.\nMots-clés : Analyse en composantes principales, Sélection de variables, K plus proches voisins, Régression logistique, Classification, Données médicales.\n\nThis study presents a methodological approach combining principal component analysis (PCA), variable selection, k-nearest neighbours (KNN) modelling and logistic regression for the classification of medical data. The main objective was to identify the most discriminating variables and to assess the predictive performance of the models. The results show that dimension reduction facilitates interpretation, while both classification models offer satisfactory performance, with a good compromise between accuracy and interpretability. This robust method can be extended to other similar datasets.\nThis project is a personal exercise to practice the concepts I have acquired.\nKeywords: Principal component analysis, Variable selection, K-nearest neighbours, Logistic regression, Classification, Medical data."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#exploration-des-données",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#exploration-des-données",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "Exploration des données",
    "text": "Exploration des données\nL’exploration des données comprend :\n- Le résumé statistique descriptif des variables\n- La distribution de la variable cible\n- L’étude des corrélations entre variables\nCes étapes permettent de mieux comprendre la structure et les caractéristiques de la base avant modélisation."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#description-des-variables",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#description-des-variables",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "Description des variables",
    "text": "Description des variables\nLes variables de cette base sont construites à partir de l’analyse de noyaux de cellules détectés dans des images médicales.\n\nid (int) : Identifiant unique de l’observation (patient).\ndiagnosis (catégorielle) Variable cible binaire :\n\nM : Malignant (maligne)\nB : Benign (bénigne)\n\nCaractéristiques mesurées\n\nPour chaque noyau de cellule, 10 mesures statistiques ont été calculées, puis la moyenne, l’écart-type (erreur standard, noté se), et la valeur extrême (worst) ont été rapportés :\n\n\n\nMesures de base :\n\n\n\nCes mesures sont disponibles en 3 versions chacune : .mean, .se, .worst\n\n\n\n\n\n\n\nVariable de base\nSignification\n\n\n\n\nradius\nRayon moyen du noyau\n\n\ntexture\nÉcart-type des valeurs de niveaux de gris\n\n\nperimeter\nPérimètre du noyau\n\n\narea\nSurface du noyau\n\n\nsmoothness\nRégularité des contours (valeurs faibles = plus lisses)\n\n\ncompactness\nCompacité = (périmètre² / surface) - 1.0\n\n\nconcavity\nGravité des concavités dans les contours\n\n\nconcave points\nNombre de points concaves sur les contours\n\n\nsymmetry\nSymétrie de la cellule\n\n\nfractal dimension\nMesure de la “rugosité” des contours\n\n\n\nChaque mesure est donc déclinée en :\n\n*_mean\n*_se\n*_worst\n\nPar exemple :\n\nradius_mean, radius_se, radius_worst\ntexture_mean, texture_se, texture_worst\n…\nfractal_dimension_mean, fractal_dimension_se, fractal_dimension_worst\n\nCe qui donne au total 30 variables quantitatives."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#résumé-des-types-de-variables",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#résumé-des-types-de-variables",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "Résumé des types de variables",
    "text": "Résumé des types de variables\n\n\n\n\n\n\n\n\nType de variable\nNom\nNombre\n\n\n\n\nIdentifiant\nid\n1\n\n\nCible binaire\ndiagnosis\n1\n\n\nVariables numériques (×10 mesures ×3 stats)\n*_mean, *_se, *_worst\n30"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#remarques",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#remarques",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "Remarques",
    "text": "Remarques\n\nOn se focalisera uniquement sur les moyennes\nAucune valeur manquante n’est présente dans le jeu de données.\nLes variables sont toutes numériques à l’exception de diagnosis.\nUn prétraitement est souvent nécessaire (standardisation, sélection de variables, etc.) avant d’entraîner un modèle."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#analyse-exploratoire",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#analyse-exploratoire",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "Analyse exploratoire",
    "text": "Analyse exploratoire\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nQuelques statistiques descriptives\n\n\ndf = pd.read_csv('data.csv')\ndf.head()\n\n         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n0    842302         M  ...                  0.11890          NaN\n1    842517         M  ...                  0.08902          NaN\n2  84300903         M  ...                  0.08758          NaN\n3  84348301         M  ...                  0.17300          NaN\n4  84358402         M  ...                  0.07678          NaN\n\n[5 rows x 33 columns]\n\n\n\nprint('\\n')\ndf = df.loc[:, (df.columns.str.contains('mean')) | (df.columns == 'diagnosis')]\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 11 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   diagnosis               569 non-null    object \n 1   radius_mean             569 non-null    float64\n 2   texture_mean            569 non-null    float64\n 3   perimeter_mean          569 non-null    float64\n 4   area_mean               569 non-null    float64\n 5   smoothness_mean         569 non-null    float64\n 6   compactness_mean        569 non-null    float64\n 7   concavity_mean          569 non-null    float64\n 8   concave points_mean     569 non-null    float64\n 9   symmetry_mean           569 non-null    float64\n 10  fractal_dimension_mean  569 non-null    float64\ndtypes: float64(10), object(1)\nmemory usage: 49.0+ KB\n\n\n\ndf.isna().sum()\n\ndiagnosis                 0\nradius_mean               0\ntexture_mean              0\nperimeter_mean            0\narea_mean                 0\nsmoothness_mean           0\ncompactness_mean          0\nconcavity_mean            0\nconcave points_mean       0\nsymmetry_mean             0\nfractal_dimension_mean    0\ndtype: int64\n\n\n\nprint('\\n')\n# on saute la première colonne (id) et la derniere columns (unnamed)\ndf.iloc[:, 1:df.shape[1]].describe()\n\n       radius_mean  texture_mean  ...  symmetry_mean  fractal_dimension_mean\ncount   569.000000    569.000000  ...     569.000000              569.000000\nmean     14.127292     19.289649  ...       0.181162                0.062798\nstd       3.524049      4.301036  ...       0.027414                0.007060\nmin       6.981000      9.710000  ...       0.106000                0.049960\n25%      11.700000     16.170000  ...       0.161900                0.057700\n50%      13.370000     18.840000  ...       0.179200                0.061540\n75%      15.780000     21.800000  ...       0.195700                0.066120\nmax      28.110000     39.280000  ...       0.304000                0.097440\n\n[8 rows x 10 columns]\n\n\n      On peut également visualiser ce résumé statistique :\n\ndef plot_multiple_histograms(rows=4, cols=3, figsize=(12, 12)):\n    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n    axes = axes.flatten() \n    #total_plots = rows * cols\n    columns = df.columns\n\n    for i, col in enumerate(columns):\n        if i == 0: # faire un barplot pour la variable quatégorielle\n          sns.barplot(\n            x=df[col].value_counts().index,\n            y=df[col].value_counts().values, \n            ax=axes[i]\n          )\n          axes[i].set_title(str(col).capitalize(), fontsize=8)\n        sns.histplot(data=df[col], ax=axes[i])\n        axes[i].set_title(str(col).capitalize(), fontsize=8)\n        axes[i].set_xlabel(str(col), fontsize=6)\n        axes[i].set_ylabel(\"Fréquence\", fontsize=6)\n        axes[i].tick_params(axis='both', labelsize=6)\n\n    for j in range(len(columns), len(axes)):\n        fig.delaxes(axes[j])\n    plt.tight_layout()\n        \n    plt.show()\n      \n\n\n\nCode\nplot_multiple_histograms()\n\n\n\n\n\nFigure 1: Distribution des variables\n\n\n\n\n      L’objectif est de comparer deux modèles de classification — Régression Logistique et K-Nearest Neighbors (KNN) — pour prédire si une tumeur est bénigne (\\(B\\)) ou maligne (\\(M\\)). Le jeu de données est déséquilibré, avec une majorité de cas bénins (~63 %).\nAfin de traiter ce déséquilibre, les actions suivantes ont été envisagées :\n\nStratification lors de la séparation train/test.\nUtilisation de métriques robustes au déséquilibre (F1-score, AUC ROC).\nTechniques correctives potentielles : class_weight='balanced'.\nCorrélogramme des variables quantitatives\n\n\n\nCode\ncorr_matrix = df.iloc[:, 1:(df.shape[1])].corr()\nplt.figure(figsize=(12, 12))\n# mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\nsns.heatmap(\n  corr_matrix, \n  annot=True, \n  fmt=\".2f\", \n  center=0,\n  linewidths=0.5,\n  cbar_kws={\"shrink\": .7},  # taille barre couleur\n  xticklabels=True,\n  yticklabels=True\n)\nplt.xticks(rotation=35, ha='right', fontsize=10)\n\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0.5, 0, 'radius_mean'), Text(1.5, 0, 'texture_mean'), Text(2.5, 0, 'perimeter_mean'), Text(3.5, 0, 'area_mean'), Text(4.5, 0, 'smoothness_mean'), Text(5.5, 0, 'compactness_mean'), Text(6.5, 0, 'concavity_mean'), Text(7.5, 0, 'concave points_mean'), Text(8.5, 0, 'symmetry_mean'), Text(9.5, 0, 'fractal_dimension_mean')])\n\n\nCode\nplt.yticks(rotation=0, fontsize=10)\n\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0, 0.5, 'radius_mean'), Text(0, 1.5, 'texture_mean'), Text(0, 2.5, 'perimeter_mean'), Text(0, 3.5, 'area_mean'), Text(0, 4.5, 'smoothness_mean'), Text(0, 5.5, 'compactness_mean'), Text(0, 6.5, 'concavity_mean'), Text(0, 7.5, 'concave points_mean'), Text(0, 8.5, 'symmetry_mean'), Text(0, 9.5, 'fractal_dimension_mean')])\n\n\nCode\nplt.subplots_adjust(bottom=0.4, left=0.3)\nplt.title(\"Corrélogramme avec p-values\", fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nCorrélogramme des variables quantitatives\n\n\n\n\nOn a des variables qui sont fortement corrélées et d’autres non. Toutefois nous ne pouvons pas nous prononcer sur la justification statistique du lien entres elles. Pour cela nous affichons le même graphique avec les pvalues pour les associations significatives et des classes pour celles qui ne le sont pas.\n\nfrom scipy.stats import pearsonr\ncols = df.select_dtypes(include=np.number).columns  # seulement les colonnes numériques\nn = len(cols)\n\ncorr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)\npval_matrix = pd.DataFrame(np.ones((n, n)), columns=cols, index=cols)\n\nfor i in range(n):\n    for j in range(n):\n        if i &lt;= j:  # éviter les doublons\n            r, p = pearsonr(df[cols[i]], df[cols[j]])\n            corr_matrix.iloc[i, j] = r\n            corr_matrix.iloc[j, i] = r\n            pval_matrix.iloc[i, j] = p\n            pval_matrix.iloc[j, i] = p\n            \n            \n# masquer les p-values non significatives\nmask_significant = pval_matrix &lt; 0.05\nannot = corr_matrix.round(2).astype(str) + \"\\np=\" + pval_matrix.round(3).astype(str)\nannot[~mask_significant] = \"\"\n\nplt.figure(figsize=(12, 12))\n\nsns.heatmap(\n  corr_matrix, \n  fmt=\"\",\n  annot=annot,\n  annot_kws={\"size\": 8},\n  center=0,\n  linewidths=0.5,\n  mask=~mask_significant,\n  cbar_kws={\"shrink\": .7},\n  xticklabels=True,\n  yticklabels=True\n)\n\nplt.xticks(rotation=35, ha='right', fontsize=9)\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0.5, 0, 'radius_mean'), Text(1.5, 0, 'texture_mean'), Text(2.5, 0, 'perimeter_mean'), Text(3.5, 0, 'area_mean'), Text(4.5, 0, 'smoothness_mean'), Text(5.5, 0, 'compactness_mean'), Text(6.5, 0, 'concavity_mean'), Text(7.5, 0, 'concave points_mean'), Text(8.5, 0, 'symmetry_mean'), Text(9.5, 0, 'fractal_dimension_mean')])\n\nplt.yticks(rotation=0, fontsize=9)\n\n(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]), [Text(0, 0.5, 'radius_mean'), Text(0, 1.5, 'texture_mean'), Text(0, 2.5, 'perimeter_mean'), Text(0, 3.5, 'area_mean'), Text(0, 4.5, 'smoothness_mean'), Text(0, 5.5, 'compactness_mean'), Text(0, 6.5, 'concavity_mean'), Text(0, 7.5, 'concave points_mean'), Text(0, 8.5, 'symmetry_mean'), Text(0, 9.5, 'fractal_dimension_mean')])\n\nplt.subplots_adjust(bottom=0.5, left=0.4)\nplt.title(\"Corrélogramme : corrélations significatives uniquement (p &lt; 0.05)\", fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\nCorrélogramme des variables quantitatives statistiquement significatives\n\n\n\n\n      Sauf quelques unes ne sont pas significativement corrélées entre-elles mais le sont avec d’autres variables"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#analyse-en-composantes-principales",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#analyse-en-composantes-principales",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "Analyse en composantes principales",
    "text": "Analyse en composantes principales\n      Pour cette partie spéciquement ainsi que pour la partie statistique du modèle logistique, nous utiliserons le langage R au lieu de python car il est plus facile à prendre en main (avis personnel). Mais pour la partie machine learning nous utiliserons le langage Python.\nL’ ACP permettra d’éliminer les variables corrélées entre elles en ne gardant que les plus contributives à la formation des axes que nous choisirons (pour plus de détails visitez ma publication Reduction de dimensionnalité, clustering non supervisé).\n\ndf_r &lt;- py$df\ndf_r$diagnosis &lt;- as.factor(df_r$diagnosis)\nstr(df_r)\n\n'data.frame':   569 obs. of  11 variables:\n $ diagnosis             : Factor w/ 2 levels \"B\",\"M\": 2 2 2 2 2 2 2 2 2 2 ...\n $ radius_mean           : num  18 20.6 19.7 11.4 20.3 ...\n $ texture_mean          : num  10.4 17.8 21.2 20.4 14.3 ...\n $ perimeter_mean        : num  122.8 132.9 130 77.6 135.1 ...\n $ area_mean             : num  1001 1326 1203 386 1297 ...\n $ smoothness_mean       : num  0.1184 0.0847 0.1096 0.1425 0.1003 ...\n $ compactness_mean      : num  0.2776 0.0786 0.1599 0.2839 0.1328 ...\n $ concavity_mean        : num  0.3001 0.0869 0.1974 0.2414 0.198 ...\n $ concave points_mean   : num  0.1471 0.0702 0.1279 0.1052 0.1043 ...\n $ symmetry_mean         : num  0.242 0.181 0.207 0.26 0.181 ...\n $ fractal_dimension_mean: num  0.0787 0.0567 0.06 0.0974 0.0588 ...\n - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=569, step=1)\n\n\n\n\nCode\nlibrary(FactoMineR) #install.packages(\"FactoMineR\")\nlibrary(factoextra) #install.packages(\"factoextra\")\nlibrary(cluster) #install.packages(\"cluster\")\n\nacp_model &lt;- PCA(df_r, quali.sup = \"diagnosis\", scale.unit = TRUE, graph = FALSE)\n\n\n\n\n\nValeurs proppres : Choix des dimensions d’analyse\n\n\n\n\n\nCode\nfviz_eig(acp_model, geom = 'line') +\n  labs(title = \"Pourcentages des variances expliquées par les composantes principales\",\n       y = \"Pourcentage d'inertie\", x = \"Composantes principales\")\n\n\n\n\n\nDiagramme des variances expliquées par les composantes principales\n\n\n\n\n      On observe le coude à partie de la troisième dimension. Mais en se basant sur le critère du taux d’inertie on a environ 80% de l’information conténue dans les données. Par conséquent notre analyse sera axée sur les deux premiers axes.\n\n\n\nAnalyses des variables\n\n\n\n\n\nCode\nfviz_pca_var(acp_model, col.var = \"contrib\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE) +\n  labs(title = \"Cercle de corrélation des variables\",\n       y = \"Dimension 2\", x = \"Dimension 1\") +\n  theme_light()\n\n\n\n\n\nCartes de la representation des variables sur les dimensions 1 et 2\n\n\n\n\n      On voit que :"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#les-kmeans",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#les-kmeans",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "Les Kmeans",
    "text": "Les Kmeans\n\n\nCode\nacp_model &lt;- PCA(\n  df_r[, c(\"diagnosis\", \"perimeter_mean\", \"smoothness_mean\", \"fractal_dimension_mean\", \"concave points_mean\")],\n  quali.sup = \"diagnosis\", \n  scale.unit = TRUE,\n  graph = FALSE\n)\n#| label: inertie-val\n#| code-fold: true\nfviz_eig(acp_model, geom = 'line') +\n  labs(title = \"Pourcentages des variances expliquées par les composantes principales\",\n       y = \"Pourcentage d'inertie\", x = \"Composantes principales\")\n\n\n\n\n\n      Pareil, on choisit les deux premières dimensions (+ de 80% de la variance expliquées).\nPour faire les Kmeans, nous prendrons 2 comme nombre de clusters.\n\ndataTocluster &lt;- scale(acp_model$ind$coord[,1:2])\nresKmeans &lt;- kmeans(dataTocluster, 2, nstart = 50)\nstr(resKmeans)\n\nList of 9\n $ cluster     : Named int [1:569] 2 2 2 1 2 1 2 1 2 1 ...\n  ..- attr(*, \"names\")= chr [1:569] \"1\" \"2\" \"3\" \"4\" ...\n $ centers     : num [1:2, 1:2] -0.48 1.118 0.268 -0.624\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"1\" \"2\"\n  .. ..$ : chr [1:2] \"Dim.1\" \"Dim.2\"\n $ totss       : num 1136\n $ withinss    : num [1:2] 480 255\n $ tot.withinss: num 735\n $ betweenss   : num 401\n $ size        : int [1:2] 398 171\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\n\n\nCode\ntable(resKmeans$cluster)\n\n\n\n  1   2 \n398 171 \n\n\n\n# attribuer les noms au clusters\ntable(Cluster = resKmeans$cluster, Diagnosis = df_r$diagnosis)\n\n       Diagnosis\nCluster   B   M\n      1 349  49\n      2   8 163\n\n\n      On voit clairement que :\n\nCluster 1 contient surtout des cas M (163 M vs 8 B) → Cluster 1 ≈ “Maligne”\nCluster 2 contient surtout des cas B (349 B vs 49 M) → Cluster 2 ≈ “Bénigne”\n\n\n\n\nReprésentations graphiques\n\n\n\n\n\nCode\ndataTocluster &lt;- as.data.frame(dataTocluster)\ndataTocluster &lt;- dataTocluster %&gt;% \n  mutate (classe = factor(\n    ifelse(resKmeans$cluster == 1, \"Maligne\", \"Bénigne\"),\n    levels = c(\"Bénigne\", \"Maligne\")\n    ), diagnosis = df_r$diagnosis)\n\nggplot(dataTocluster, aes (x = Dim.1, y = Dim.2, color = classe)) +\n  geom_point() +\n  labs(\n    title = \"Classification des patients (Axe 1 - Axe 2)\",\n    x = \"Dimension 1\",\n    y = \"Dimension 2\",\n    color = \"Classe/Cluster\"\n  )+\n  theme_light()\n\n\n\n\n\nFigure 2: Classification des patients par la méthode des K-moyennes\n\n\n\n\nInterpretation :\n      On observe que les patients sont pratiquement linéairement séparables dans le plan défini par les deux premières composantes principales.\nCela signifie que la projection sur ces deux dimensions met bien en évidence une séparation claire entre les deux groupes (bénins et malins), ce qui est cohérent avec la qualité du clustering obtenu par k-means.\nCette bonne séparation visuelle corrobore la pertinence des variables sélectionnées et confirme que les composantes principales résument efficacement la variance utile à la distinction des diagnostics. Nous pouvons passer à présent aux différents modèles de prédiction (regression logistique et k plus plus proches voisins)"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#logit-vs-knn",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#logit-vs-knn",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "Logit vs KNN",
    "text": "Logit vs KNN\n\n\nCode\n# Importation des librairies nécessaires\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom imblearn.over_sampling import SMOTE\n\n\ndf = df.loc[:, [\"diagnosis\", \"perimeter_mean\", \"smoothness_mean\", \"fractal_dimension_mean\", \"concave points_mean\"]]\n\n# Variables explicatives et cible\nX = df.drop(columns=['diagnosis'])\ny = df['diagnosis']\n\n# Séparation train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Mise à l’échelle des variables\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# --- Modèle 1 : Régression Logistique ---\nlogit = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\nlogit.fit(X_train_scaled, y_train)\n\n\nLogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n\n\nCode\ny_pred_logit = logit.predict(X_test_scaled)\ny_proba_logit = logit.predict_proba(X_test_scaled)[:,1]\n\n# --- Modèle 2 : KNN ---\n\nsmote = SMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_res, y_train_res)\n\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\nCode\ny_pred_knn = knn.predict(X_test_scaled)\ny_proba_knn = knn.predict_proba(X_test_scaled)[:,1]\n\n\n\nExactitude de l’entraînement des modèles\n\n\nfrom sklearn.metrics import accuracy_score\n\ny_train_pred_logit = logit.predict(X_train_scaled)\ntrain_accuracy_logit = accuracy_score(y_train_pred_logit, y_train)\n\ny_train_pred_knn = knn.predict(X_train_scaled)\ntrain_accuracy_knn = accuracy_score(y_train_pred_knn, y_train)\n\nprint(f\"Train Accuracy Logit: {train_accuracy_logit:.4f}\")\n\nTrain Accuracy Logit: 0.9121\n\nprint(f\"Train Accuracy KNN: {train_accuracy_knn:.4f}\")\n\nTrain Accuracy KNN: 0.9322\n\n\n\nExactitude du test des modèles\n\n\nacc_logit = accuracy_score(y_test, y_pred_logit)\nacc_knn = accuracy_score(y_test, y_pred_knn)\n\nprint(f\"Accuracy Régression Logistique : {acc_logit:.4f}\")\n\nAccuracy Régression Logistique : 0.9357\n\nprint(f\"Accuracy KNN : {acc_knn:.4f}\")\n\nAccuracy KNN : 0.9006\n\n\n\nINTERPRETATION\n\n      L’exactitude du modèle logistique diminue faibelement du train au test (environ 0.004 soit 0, 4% ) alors que celle du KNN diminue un peu plus (environ 0.019 soit 1,9 %).\nEn revanche, l’exactitude du KNN diminue un peu plus, d’environ 1,9 %. Cette baisse plus marquée suggère que le modèle KNN s’adapte davantage aux spécificités du jeu d’entraînement (il est plus sensible aux données d’entraînement), ce qui peut traduire un léger surapprentissage.\nEn résumé :\n\nLe modèle logistique est plus stable et robuste, avec une capacité de généralisation meilleure.\nLe KNN, bien qu’ayant une meilleure précision sur le test, présente un risque un peu plus important de surajustement, ce qui peut poser problème sur des données très différentes de l’entraînement.\nLa valeur de la fonction de perte (log-loss)\n\nLa log-loss est une mesure utile pour la régression logistique, qui reflète la qualité probabiliste des prédictions.\n\nLa regression logistique :\n\n\nfrom sklearn.metrics import log_loss\n\ntrain_loss_logit = log_loss(y_train, logit.predict_proba(X_train_scaled))\ntest_loss_logit = log_loss(y_test, logit.predict_proba(X_test_scaled))\n\nprint(f\"Log-loss train (logit): {train_loss_logit:.4f}\")\n\nLog-loss train (logit): 0.1983\n\nprint(f\"Log-loss test (logit): {test_loss_logit:.4f}\")\n\nLog-loss test (logit): 0.1895\n\n\n\nLog-loss train (0.1983) et log-loss test (0.1895) sont très proches, et la perte est même légèrement meilleure (plus faible) sur le test.\nCela indique que le modèle ne souffre pas de surapprentissage significatif — il généralise bien sur les données nouvelles.\n\nLe KNN :\n\n\nLe KNN n’optimise pas explicitement une fonction de perte pendant l’entraînement — c’est un algorithme instance-based, il mémorise les exemples.\nOn ne peut donc pas parler de “loss” au sens d’une fonction de coût minimisée.\nMais on peut évaluer la performance via la classification error \\((1 - accuracy)\\) ou d’autres métriques.\n\n\nMatrices de confusion\n\n\n\n\nCode\ncm_logit = confusion_matrix(y_test, y_pred_logit)\ncm_knn = confusion_matrix(y_test, y_pred_knn)\n\n# Fonction pour tracer la matrice de confusion\ndef plot_cm(cm, title):\n    plt.figure(figsize=(5,4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['Benin (0)', 'Malin (1)'],\n                yticklabels=['Benin (0)', 'Malin (1)'])\n    plt.ylabel('Real labels')\n    plt.xlabel('Predictions')\n    plt.title(title)\n    plt.show()\n\nplot_cm(cm_logit, \"Confusion matrix - Logistic regression\")\nplot_cm(cm_knn, \"Confusion matrix - KNN\")\n\n\n\n\n\n\n\n\n(a) Matrice de confusion - Régression Logistique\n\n\n\n\n\n\n\n(b) Matrice de confusion - KNN\n\n\n\n\nFigure 3: Comparaison des ML (Logit vs KNN - Matrices de confusion)\n\n\n\n      On peut voir que le KNN a une moins bonne prédiction sur la base de la matrice de confusion car on a :\n\n100 Vrais négatifs (vrais benin) contre 101 pour le modèle logit\n57 Vrais positifs (vrais malin) contre 59 pour le modèle logit\n7 Faux positifs contre (faux malin) 6 pour le modèle logit\n\n\n\nPrécision, recall, score F1, AUC\n\n\n\n\nCode\ndef eval_classif(y_true, y_pred, y_proba, model_name=\"Modèle\"):\n    print(f\"== Results for {model_name} ==\")\n    print(\"\\nClassification report : :\")\n    print(classification_report(y_true, y_pred, digits=4))\n    auc = roc_auc_score(y_true, y_proba)\n    print(f\"AUC ROC : {auc:.4f}\\n\")\n\neval_classif(y_test, y_pred_logit, y_proba_logit, \"Régression Logistique\")\n\n\n== Results for Régression Logistique ==\n\nClassification report : :\n              precision    recall  f1-score   support\n\n           B     0.9528    0.9439    0.9484       107\n           M     0.9077    0.9219    0.9147        64\n\n    accuracy                         0.9357       171\n   macro avg     0.9303    0.9329    0.9315       171\nweighted avg     0.9359    0.9357    0.9358       171\n\nAUC ROC : 0.9816\n\n\nCode\neval_classif(y_test, y_pred_knn, y_proba_knn, \"KNN\")\n\n\n== Results for KNN ==\n\nClassification report : :\n              precision    recall  f1-score   support\n\n           B     0.9327    0.9065    0.9194       107\n           M     0.8507    0.8906    0.8702        64\n\n    accuracy                         0.9006       171\n   macro avg     0.8917    0.8986    0.8948       171\nweighted avg     0.9020    0.9006    0.9010       171\n\nAUC ROC : 0.9572\n\n\n\nRésultats pour la Régression Logistique\nExactitude globale (accuracy) : 93,03 %, ce qui signifie que le modèle classe correctement environ 93 patients sur 100.\nPrécision par classe :\n\nPour la classe bénigne (\\(B\\)), la précision est de 95,28 %, indiquant une bonne détection des cas bénins.\nPour la classe maligne (\\(M\\)), la précision est de 90,77 %, également satisfaisante. Mais cet écart de précision pourraît être attribué au déquilibre des reponses (Benin et Malin) et cela malgré la stratification et la pondération. Toutefois les métriques telles que F1-Scrore et AUC sont moins sensibles au déséquilibre des classes de la variable reponse (ici la catégorie de tumeur).\n\nRappel (sensibilité) :\n\nClasse bénigne : 94,39 %, montrant une bonne capacité à détecter les vrais positifs bénins.\n\nClasse maligne : 92,19 %, un peu moins élevée. L’explication donnée en ammont est également valide ici.\n\nF1-score : 94,84 % pour la classe bénigne et 91,47 % pour la classe maligne, indiquant un bon équilibre global entre précision et rappel.\nAUC ROC : 0,98116, proche de 1, ce qui montre une excellente capacité de discrimination.\n\n\n\nRésultats pour le KNN (K plus proches voisins)\nExactitude globale (accuracy) : 91,26 %, moins bonne que celle la régression logistique.\nPrécision par classe :\n\nPour la classe bénigne (\\(B\\)), la précision est de 93,46 %, moins bonne que celle la régression logistique.\nPour la classe maligne (\\(M\\)), la précision est de 89, 06%, légèrement inférieure à la régression logistique.\n\nRappel (sensibilité) :\n\nClasse bénigne : 93,46 %, un peu plus faible que le rappel du modèle logit.\nClasse maligne : 89, 06%, meilleure que celui de la régression logistique.\n\nF1-score : 93,46 % pour la classe bénigne et 89,06 % pour la classe maligne. Ces scores sont tous deux inférieurs à celui de la régression logistique.\nAUC ROC : 0,9632, très élevé, mais un peu inférieur à celui de la régression logistique.\n\n\nConclusion : \nLes deux modèles présentent des performances globalement comparables pour différencier les patients bénins et malins :\n\nLa regression logistique obtient une meilleure précision globale ainsi qu’un meilleur équilibre du F1-score, notamment pour la classe bénigne, témoignant d’une bonne capacité à limiter les erreurs dans cette catégorie.\nLa régression logistique offre un rappel plus élevé pour la classe bénigne et une meilleure AUC ROC, indiquant une excellente capacité de discrimination globale et une qualité probabiliste supérieure des prédictions.\n\nLe modèle logistique se révèle également plus stable et robuste, avec une meilleure capacité de généralisation. En effet, bien que le KNN affiche une précision considérable sur le test, il présente un risque accru de surajustement, ce qui peut nuire à ses performances sur des données très différentes de l’entraînement.\n\nConcernant la fonction de perte\n\nLa log-loss, métrique essentielle pour la régression logistique, reflète la qualité probabiliste des prédictions.\n\nLes valeurs observées (log-loss de 0.1983 au train et 0.1895 au test) sont très proches, voire légèrement meilleures sur le test.\nCette stabilité confirme que le modèle logistique ne souffre pas de surapprentissage significatif et qu’il généralise efficacement sur des données nouvelles.\n\n      La régression logistique est globalement préférable dans ce cas, car elle offre une performance plus équilibrée, une meilleure capacité discriminante (AUC) et une stabilité importante, ce qui la rend plus fiable pour un usage clinique ou opérationnel."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#annexe-1-hypothèses-et-interprétations-des-tests-statistiques",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#annexe-1-hypothèses-et-interprétations-des-tests-statistiques",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "Annexe 1 : Hypothèses et interprétations des tests statistiques",
    "text": "Annexe 1 : Hypothèses et interprétations des tests statistiques\n\nTest de Shapiro-Wilk\nCe test permet de vérifier la normalité d’une distribution.\n\nHypothèses : \\[\n\\begin{cases}\nH_0 : \\text{Les données suivent une loi normale} \\\\\nH_1 : \\text{Les données ne suivent pas une loi normale}\n\\end{cases}\n\\]\nInterprétation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les données peuvent être considérées comme normales.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) les données ne sont pas normales.\n\n\n\n\n\nTest de Levene\nCe test permet de vérifier l’homogénéité des variances entre les groupes.\n\nHypothèses : \\[\n\\begin{cases}\nH_0 : \\text{Les variances des groupes sont égales} \\\\\nH_1 : \\text{Les variances des groupes sont différentes}\n\\end{cases}\n\\]\nInterprétation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les variances sont homogènes.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) les variances sont différentes.\n\n\n\n\n\nTest d’ANOVA classique\nCe test compare les moyennes de plusieurs groupes. Il nécessite que les données soient normales et que les variances soient homogènes.\n\nHypothèses : \\[\n\\begin{cases}\nH_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\nH_1 : \\exists \\, i \\ne j \\text{ tel que } \\mu_i \\ne \\mu_j\n\\end{cases}\n\\]\nInterprétation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les moyennes sont statistiquement égales.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) au moins une moyenne est différente.\n\n\n\n\n\nTest d’ANOVA de Welch\nCe test est une version robuste de l’ANOVA utilisée lorsque l’homogénéité des variances n’est pas respectée, mais que les données restent normales.\n\nHypothèses : \\[\n\\begin{cases}\nH_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\nH_1 : \\exists \\, i \\ne j \\text{ tel que } \\mu_i \\ne \\mu_j\n\\end{cases}\n\\]\nInterprétation de la \\(p\\)-valeur : identique à celle du test ANOVA classique.\n\n\n\n\nTest de Kruskal-Wallis\nTest non paramétrique utilisé en cas de non-normalité ou lorsque les données sont ordinales.\n\nHypothèses : \\[\n\\begin{cases}\nH_0 : \\text{Les distributions des groupes sont identiques} \\\\\nH_1 : \\text{Au moins une distribution est différente}\n\\end{cases}\n\\]\nInterprétation de la \\(p\\)-valeur :\n\nSi \\(p &gt; 0.05\\) : on ne rejette pas \\({H_0}\\) \\(\\Rightarrow\\) les distributions sont considérées comme similaires.\nSi \\(p \\leq 0.05\\) : on rejette \\({H_0}\\) \\(\\Rightarrow\\) au moins une des distributions diffère significativement.\n\n\n\nRemarque : Tous ces tests renvoient une \\(p\\)-valeur qui est comparée au seuil de signification habituellement fixé à 5% (\\(\\alpha = 0.05\\))."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#annexe-2",
    "href": "INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/work.html#annexe-2",
    "title": "Optimisation de la classification médicale par intégration de techniques statistiques et machine learning",
    "section": "Annexe 2:",
    "text": "Annexe 2:\nLe rapport de corrélation \\(\\eta^2\\) est une mesure de l’effet qui quantifie la proportion de la variance expliquée par un facteur.\nIl est défini par la formule suivante :\n\\[\n\\eta^2 = \\frac{\\sum_{i=1}^{k} n_i (\\bar{y}_i - \\bar{y})^2}{\\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y})^2}\n\\]\noù :\n\n\\(k\\) est le nombre de groupes,\n\\(n_i\\) est la taille du groupe \\(i\\),\n\\(\\bar{y}_i\\) est la moyenne du groupe \\(i\\),\n\\(\\bar{y}\\) est la moyenne globale,\n\\(y_{ij}\\) est l’observation \\(j\\) du groupe \\(i\\).\n\nCette mesure permet d’évaluer l’ampleur de la différence entre les groupes, en indiquant la proportion de la variance totale attribuable à la variation entre groupes."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html",
    "title": "KedjeBoost – Votre resto, en mode turbo !",
    "section": "",
    "text": "KedjenouXpress est une application de gestion de ventes conçue pour simplifier les opérations commerciales dans la restauration. Elle a été développée en 2023 pour remplacer les méthodes manuelles et offrir une solution numérique rapide, intuitive et efficace.\nL’interface utilisateur bénéficie d’un design moderne et agréable, grâce à l’utilisation de styles personnalisés en CSS pour améliorer l’expérience visuelle et l’ergonomie.\nSkills : Java, JavFx, MySql, Modélisation UML, POO\n\n\n\nLes outils et technologies suivants ont été utilisés pour développer le logiciel :\n\nJava (version 8.2) : choisi pour sa stabilité et sa compatibilité avec le développement d’applications de bureau.\nNetBeans : utilisé comme environnement de développement intégré (IDE) pour sa simplicité et sa bonne intégration avec Java.\nWAMP : utilisé pour héberger localement la base de données et assurer la communication entre l’application et les données, avec MySQL comme système de gestion de base de données relationnelle.\n\n\n\n\n\n\nJava (JDK 8) : Télécharger Java JDK (version 8)\n\nNetBeans IDE : Télécharger NetBeans\n\nWAMP Server : Télécharger WAMP\n\n\n\n\n\nWAMP Server (Windows, Apache, MySQL, PHP) est un environnement de développement web local. Avant de l’installer, il est important de s’assurer que certaines dépendances logicielles sont présentes sur votre système.\n\n1. Dépendances à installer\n\nWAMP Server nécessite plusieurs versions du Microsoft Visual C++ Redistributable, indispensables au bon fonctionnement d’Apache, MySQL et PHP. Ces bibliothèques sont parfois installées automatiquement, mais il est recommandé de les vérifier avant.\n\n\n\nVersions requises les plus courantes\n\n\n\n\nVisual C++ 2008 (x86 et x64)\nVisual C++ 2010 (x86 et x64)\nVisual C++ 2012 (x86 et x64)\nVisual C++ 2013 (x86 et x64)\nVisual C++ 2015-2022 (x86 et x64)\n\n🔗 Télécharger toutes les versions nécessaires ici :\nhttps://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist\n\n2. Téléchargement de WAMP\nAccédez au site officiel : https://www.wampserver.com\nTéléchargez la version correspondant à votre système (32 bits ou 64 bits).\nEnregistrez le fichier .exe sur votre ordinateur.\n3. Installation\nLancez le fichier téléchargé (clic droit &gt; Exécuter en tant qu’administrateur).\nSuivez les instructions de l’assistant d’installation :\n\nAcceptez le contrat de licence.\nChoisissez le répertoire d’installation (par défaut : C:\\wamp64\\).\nSélectionnez votre navigateur par défaut si demandé.\nChoisissez également votre éditeur de texte préféré (Notepad++ par exemple).\n\nTerminez l’installation.\n4. Démarrage de WAMP\nOuvrez WAMP Server via le menu Démarrer.\nUne icône apparaît dans la barre des tâches :\n\n🟢 Vert : tous les services fonctionnent.\n🟡 Orange : un ou plusieurs services sont arrêtés.\n🔴 Rouge : aucun service ne fonctionne.\n\n5. Vérification\nCliquez sur l’icône WAMP &gt; “Localhost” : une page de bienvenue doit s’afficher.\nVous pouvez accéder à phpMyAdmin pour créer et gérer vos bases de données MySQL.\n\n\n\n\n\n      L’architecture adoptée pour cette application repose sur le modèle MVC (Modèle-Vue-Contrôleur), un patron de conception logiciel qui vise à séparer clairement les responsabilités dans une application, particulièrement celles disposant d’une interface utilisateur graphique.\nCe découpage permet d’obtenir un code plus lisible, maintenable, et facilement évolutif. Cela facilite également le travail collaboratif en isolant les tâches : un développeur peut travailler sur la logique métier (modèle) pendant qu’un autre se concentre sur l’interface (vue).\nLe modèle MVC est composé de trois éléments fondamentaux :\n\n1. Modèle (Model)\n\n      Le modèle contient les données de l’application ainsi que la logique métier. Il est responsable de la création, la mise à jour et la validation des données. Il ne s’occupe jamais de l’affichage.\nExemple\npublic class Personne {\n    private String nom;\n    private int age;\n\n    // Constructeur\n    public Personne(String nom, int age) {\n        this.nom = nom;\n        this.age = age;\n    }\n\n    // Accesseurs (getters)\n    public String getNom() {\n        return nom;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    // Modificateurs (setters)\n    public void setNom(String nom) {\n        this.nom = nom;\n    }\n\n    public void setAge(int age) {\n        if (age &gt;= 0) {\n            this.age = age;\n        } else {\n            System.out.println(\"L'âge doit être positif.\");\n        }\n    }\n\n    // Méthode d'affichage (optionnelle)\n    public void afficherInfos() {\n        System.out.println(\"Nom : \" + nom + \", Âge : \" + age);\n    }\n}\n\n2. Vue (View)\n\n      La vue est responsable de l’affichage des données à l’utilisateur. Elle ne contient aucune logique métier. Son objectif est uniquement de présenter visuellement les informations issues du modèle et de transmettre les actions de l’utilisateur au contrôleur.\nExemple FXML (JavaFX) d’une vue simple avec un bouton :\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n\n&lt;?import javafx.scene.control.*?&gt;\n&lt;?import javafx.scene.layout.*?&gt;\n\n&lt;AnchorPane xmlns:fx=\"http://javafx.com/fxml\" fx:controller=\"monappli.controllers.PersonneController\"&gt;\n    &lt;children&gt;\n        &lt;Button text=\"Afficher les infos\" layoutX=\"100\" layoutY=\"80\" onAction=\"#afficherInfos\"/&gt;\n    &lt;/children&gt;\n&lt;/AnchorPane&gt;\nCe fichier FXML définit une fenêtre avec un bouton qui, lorsqu’il est cliqué, déclenche une méthode du contrôleur (nommée afficherInfos).\n\n3. Contrôleur (Controller)\n\n      Le contrôleur fait le lien entre la vue et le modèle. Il :\n\nintercepte les actions de l’utilisateur (clics, saisies, etc.),\nmet à jour le modèle en fonction des actions,\ndemande à la vue de s’actualiser.\n\nExemple de classe contrôleur JavaFX associée à la vue précédente :\npackage monappli.controllers;\n\nimport javafx.event.ActionEvent;\nimport javafx.fxml.FXML;\nimport monappli.models.Personne;\n\npublic class PersonneController {\n    private Personne personne;\n\n    public PersonneController() {\n        // Création d'un objet Personne par défaut\n        this.personne = new Personne(\"Jean\", 28);\n    }\n\n    @FXML\n    public void afficherInfos(ActionEvent event) {\n        personne.afficherInfos();\n    }\n}\n\n\n\n\n      Pour la gestion des données, le projet s’appuie sur MySQL, un système de gestion de base de données relationnelle (SGBDR) largement utilisé dans les applications professionnelles. L’interaction entre l’application Java et la base de données a été rendue possible grâce au package mysql-connector-java, téléchargé puis intégré comme dépendance dans le projet.\nCela a permis de stocker localement les données, d’assurer leur persistence et d’y accéder efficacement via des requêtes SQL.\n\n\nRequêtes SQL utilisées\n\n\nPlusieurs types de requêtes ont été implémentés dans le cadre de ce projet, notamment :\n\nINSERT INTO : pour l’insertion de nouvelles données (ex. : ajout d’un produit ou d’un utilisateur) ;\nSELECT : pour la récupération et l’affichage des données (liste des ventes, employés, inventaire, etc.) ;\nUPDATE : pour la mise à jour d’enregistrements (ex. : modifier un produit) ;\nDELETE : pour la suppression de données obsolètes ou incorrectes.\n\n\n\nOptimisation par jointures\n\n\n      Des jointures (JOIN) ont également été utilisées pour relier plusieurs tables par exemple, les ventes, les utilisateurs et les produits, afin de produire des rapports détaillés, et d’optimiser les filtres et recherches complexes.\n\nRemarque : Cette architecture relationnelle a été choisie pour sa fiabilité, sa performance et sa compatibilité avec les outils de développement Java."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#technologies-utilisées",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#technologies-utilisées",
    "title": "KedjeBoost – Votre resto, en mode turbo !",
    "section": "",
    "text": "Les outils et technologies suivants ont été utilisés pour développer le logiciel :\n\nJava (version 8.2) : choisi pour sa stabilité et sa compatibilité avec le développement d’applications de bureau.\nNetBeans : utilisé comme environnement de développement intégré (IDE) pour sa simplicité et sa bonne intégration avec Java.\nWAMP : utilisé pour héberger localement la base de données et assurer la communication entre l’application et les données, avec MySQL comme système de gestion de base de données relationnelle."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#liens-de-téléchargement",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#liens-de-téléchargement",
    "title": "KedjeBoost – Votre resto, en mode turbo !",
    "section": "",
    "text": "Java (JDK 8) : Télécharger Java JDK (version 8)\n\nNetBeans IDE : Télécharger NetBeans\n\nWAMP Server : Télécharger WAMP"
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#installation-de-wamp-server",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#installation-de-wamp-server",
    "title": "KedjeBoost – Votre resto, en mode turbo !",
    "section": "",
    "text": "WAMP Server (Windows, Apache, MySQL, PHP) est un environnement de développement web local. Avant de l’installer, il est important de s’assurer que certaines dépendances logicielles sont présentes sur votre système.\n\n1. Dépendances à installer\n\nWAMP Server nécessite plusieurs versions du Microsoft Visual C++ Redistributable, indispensables au bon fonctionnement d’Apache, MySQL et PHP. Ces bibliothèques sont parfois installées automatiquement, mais il est recommandé de les vérifier avant.\n\n\n\nVersions requises les plus courantes\n\n\n\n\nVisual C++ 2008 (x86 et x64)\nVisual C++ 2010 (x86 et x64)\nVisual C++ 2012 (x86 et x64)\nVisual C++ 2013 (x86 et x64)\nVisual C++ 2015-2022 (x86 et x64)\n\n🔗 Télécharger toutes les versions nécessaires ici :\nhttps://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist\n\n2. Téléchargement de WAMP\nAccédez au site officiel : https://www.wampserver.com\nTéléchargez la version correspondant à votre système (32 bits ou 64 bits).\nEnregistrez le fichier .exe sur votre ordinateur.\n3. Installation\nLancez le fichier téléchargé (clic droit &gt; Exécuter en tant qu’administrateur).\nSuivez les instructions de l’assistant d’installation :\n\nAcceptez le contrat de licence.\nChoisissez le répertoire d’installation (par défaut : C:\\wamp64\\).\nSélectionnez votre navigateur par défaut si demandé.\nChoisissez également votre éditeur de texte préféré (Notepad++ par exemple).\n\nTerminez l’installation.\n4. Démarrage de WAMP\nOuvrez WAMP Server via le menu Démarrer.\nUne icône apparaît dans la barre des tâches :\n\n🟢 Vert : tous les services fonctionnent.\n🟡 Orange : un ou plusieurs services sont arrêtés.\n🔴 Rouge : aucun service ne fonctionne.\n\n5. Vérification\nCliquez sur l’icône WAMP &gt; “Localhost” : une page de bienvenue doit s’afficher.\nVous pouvez accéder à phpMyAdmin pour créer et gérer vos bases de données MySQL."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#architecture-de-lapplication-modèle-mvc",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#architecture-de-lapplication-modèle-mvc",
    "title": "KedjeBoost – Votre resto, en mode turbo !",
    "section": "",
    "text": "L’architecture adoptée pour cette application repose sur le modèle MVC (Modèle-Vue-Contrôleur), un patron de conception logiciel qui vise à séparer clairement les responsabilités dans une application, particulièrement celles disposant d’une interface utilisateur graphique.\nCe découpage permet d’obtenir un code plus lisible, maintenable, et facilement évolutif. Cela facilite également le travail collaboratif en isolant les tâches : un développeur peut travailler sur la logique métier (modèle) pendant qu’un autre se concentre sur l’interface (vue).\nLe modèle MVC est composé de trois éléments fondamentaux :\n\n1. Modèle (Model)\n\n      Le modèle contient les données de l’application ainsi que la logique métier. Il est responsable de la création, la mise à jour et la validation des données. Il ne s’occupe jamais de l’affichage.\nExemple\npublic class Personne {\n    private String nom;\n    private int age;\n\n    // Constructeur\n    public Personne(String nom, int age) {\n        this.nom = nom;\n        this.age = age;\n    }\n\n    // Accesseurs (getters)\n    public String getNom() {\n        return nom;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    // Modificateurs (setters)\n    public void setNom(String nom) {\n        this.nom = nom;\n    }\n\n    public void setAge(int age) {\n        if (age &gt;= 0) {\n            this.age = age;\n        } else {\n            System.out.println(\"L'âge doit être positif.\");\n        }\n    }\n\n    // Méthode d'affichage (optionnelle)\n    public void afficherInfos() {\n        System.out.println(\"Nom : \" + nom + \", Âge : \" + age);\n    }\n}\n\n2. Vue (View)\n\n      La vue est responsable de l’affichage des données à l’utilisateur. Elle ne contient aucune logique métier. Son objectif est uniquement de présenter visuellement les informations issues du modèle et de transmettre les actions de l’utilisateur au contrôleur.\nExemple FXML (JavaFX) d’une vue simple avec un bouton :\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n\n&lt;?import javafx.scene.control.*?&gt;\n&lt;?import javafx.scene.layout.*?&gt;\n\n&lt;AnchorPane xmlns:fx=\"http://javafx.com/fxml\" fx:controller=\"monappli.controllers.PersonneController\"&gt;\n    &lt;children&gt;\n        &lt;Button text=\"Afficher les infos\" layoutX=\"100\" layoutY=\"80\" onAction=\"#afficherInfos\"/&gt;\n    &lt;/children&gt;\n&lt;/AnchorPane&gt;\nCe fichier FXML définit une fenêtre avec un bouton qui, lorsqu’il est cliqué, déclenche une méthode du contrôleur (nommée afficherInfos).\n\n3. Contrôleur (Controller)\n\n      Le contrôleur fait le lien entre la vue et le modèle. Il :\n\nintercepte les actions de l’utilisateur (clics, saisies, etc.),\nmet à jour le modèle en fonction des actions,\ndemande à la vue de s’actualiser.\n\nExemple de classe contrôleur JavaFX associée à la vue précédente :\npackage monappli.controllers;\n\nimport javafx.event.ActionEvent;\nimport javafx.fxml.FXML;\nimport monappli.models.Personne;\n\npublic class PersonneController {\n    private Personne personne;\n\n    public PersonneController() {\n        // Création d'un objet Personne par défaut\n        this.personne = new Personne(\"Jean\", 28);\n    }\n\n    @FXML\n    public void afficherInfos(ActionEvent event) {\n        personne.afficherInfos();\n    }\n}"
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#gestion-de-la-base-de-données",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#gestion-de-la-base-de-données",
    "title": "KedjeBoost – Votre resto, en mode turbo !",
    "section": "",
    "text": "Pour la gestion des données, le projet s’appuie sur MySQL, un système de gestion de base de données relationnelle (SGBDR) largement utilisé dans les applications professionnelles. L’interaction entre l’application Java et la base de données a été rendue possible grâce au package mysql-connector-java, téléchargé puis intégré comme dépendance dans le projet.\nCela a permis de stocker localement les données, d’assurer leur persistence et d’y accéder efficacement via des requêtes SQL.\n\n\nRequêtes SQL utilisées\n\n\nPlusieurs types de requêtes ont été implémentés dans le cadre de ce projet, notamment :\n\nINSERT INTO : pour l’insertion de nouvelles données (ex. : ajout d’un produit ou d’un utilisateur) ;\nSELECT : pour la récupération et l’affichage des données (liste des ventes, employés, inventaire, etc.) ;\nUPDATE : pour la mise à jour d’enregistrements (ex. : modifier un produit) ;\nDELETE : pour la suppression de données obsolètes ou incorrectes.\n\n\n\nOptimisation par jointures\n\n\n      Des jointures (JOIN) ont également été utilisées pour relier plusieurs tables par exemple, les ventes, les utilisateurs et les produits, afin de produire des rapports détaillés, et d’optimiser les filtres et recherches complexes.\n\nRemarque : Cette architecture relationnelle a été choisie pour sa fiabilité, sa performance et sa compatibilité avec les outils de développement Java."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#demarrage-de-lapplication",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#demarrage-de-lapplication",
    "title": "KedjeBoost – Votre resto, en mode turbo !",
    "section": "Demarrage de l’application",
    "text": "Demarrage de l’application\n      Après la compilation et la transformation en application exécutable, les dépendances nécessaires notamment Java 8.2 pour le bon fonctionnement des packages requis, ainsi que le serveur WAMP pour la gestion des bases de données doivent être installées sur la machine réceptrice. Une fois ces étapes terminées, un raccourci de l’application est créé et placé sur le bureau.\n\n\n\n\n\nIcone de l’ application\n\n\n\n\nPour démarrer l’application, il est recommandé de faire un clic droit sur l’icône de l'application, puis de sélectionner Exécuter en tant qu'administrateur. Cela ouvrira la première page, qui correspond à la page d’accueil."
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#écran-daccueil",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#écran-daccueil",
    "title": "KedjeBoost – Votre resto, en mode turbo !",
    "section": "Écran d’accueil",
    "text": "Écran d’accueil\n      Au démarrage de l’application, une interface d’accueil a été développée. Elle permet à l’utilisateur de se connecter soit en mode administrateur, soit en mode utilisateur. Chaque mode requiert la saisie d’un identifiant et d’un mot de passe.\nUn bouton \"Quitter\" est également disponible en bas de l’écran. En cas de tentative de fermeture de l’application, une boîte de confirmation s’affiche afin d’éviter toute fermeture accidentelle.\nVidéo de démonstration"
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#partie-administrateur",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#partie-administrateur",
    "title": "KedjeBoost – Votre resto, en mode turbo !",
    "section": "Partie Administrateur",
    "text": "Partie Administrateur\n      La section Administrateur de l’application est constituée de six interfaces principales, chacune ayant un rôle spécifique dans la gestion et le suivi des activités. Ces interfaces sont les suivantes (vous pouvez visualiser les vidéos):\n1. Inventaires : permet l’enregistrement et la gestion des produits disponibles en stock.\n\n2. Employés : destinée à l’enregistrement et au suivi des employés (vendeurs, caissiers, etc.).\n\n3. Ventes : affiche la liste des produits vendus et permet de suivre les transactions réalisées.\n\n4. Tableau de bord (Dashboard) : fournit une vue récapitulative des ventes et autres indicateurs clés pour une vision globale de l’activité.\n\n5. Rapports : génère et permet d’imprimer des rapports de vente, facilitant ainsi l’analyse et le suivi.\n\n6. Paramètres : offre des options de personnalisation de l'application - changement du nom de l’entreprise, de l’adresse, des informations de contact, etc.\n\n\n\n\n\nInterface Admin- Paramètre"
  },
  {
    "objectID": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#utilisateurs",
    "href": "INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html#utilisateurs",
    "title": "KedjeBoost – Votre resto, en mode turbo !",
    "section": "Utilisateurs",
    "text": "Utilisateurs\n      Cette interface a été conçue pour être utilisée par les vendeurs ou caissiers opérant dans l’entreprise. Elle se compose de quatre sous-interfaces, chacune ayant un rôle bien défini :\n1. Interface principale\nElle permet :\n\nde vendre un produit en stock,\nd’imprimer la facture de la commande une fois validée,\nd’enregistrer le mode de paiement : en espèces (cash), via des services de monnaie électronique (Orange Money, Moov Money) ou encore d’enregistrer une commande différée, à régler ultérieurement.\n\n2. Interface \"Voir Commande\nCette interface permet de :\n\nrechercher une commande en attente,\nvalider ou annuler une commande lancée précédemment.\n\n3. Interface \"Gérer les rapports de vente\nElle offre la possibilité de :\n\nsélectionner une date ou une plage de dates,\nimprimer les ventes réalisées par le caissier connecté,\nafficher le total des ventes, à la fois en montant (prix) et en quantité pour la période sélectionnée.\n\n\n4. Interface État des serveurs\nCette dernière interface donne une vue d’ensemble sur :\n\nles quantités et totaux vendus,\nrépartis par serveur (vendeur),\nafin de faciliter le suivi des performances individuelles.\n\nNB : Cette interface est également accessible depuis la section Administrateur, ce qui permet à la direction de superviser les activités des vendeurs en temps réel."
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html",
    "title": "ShiFuMi IA – Reconnaissance de gestes",
    "section": "",
    "text": "Ce projet vise à créer une intelligence artificielle capable de jouer au jeu ShiFuMi (pierre-papier-ciseaux) contre un humain. L’objectif est de reconnaître automatiquement les gestes d’une main via une webcam, et de répondre en temps réel. Ce prototype est développé seul, en utilisant les outils suivants :\n\nCNN pré-entraîné (MobileNetV2) pour classifier les images de mains,\nTensorFlow/Keras pour l’entraînement,\nYOLOv8 (en préparation) pour la détection en direct via webcam,"
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#pourquoi-entraîner-yolov8",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#pourquoi-entraîner-yolov8",
    "title": "ShiFuMi IA – Reconnaissance de gestes",
    "section": "🤔 Pourquoi entraîner YOLOv8 ?",
    "text": "🤔 Pourquoi entraîner YOLOv8 ?\n      L’objectif principal de l’entraînement de ce modèle est de détecter les mains dans des images issues d’une webcam ou d’une vidéo. Cette détection constitue une première étape essentielle avant de transmettre la région d’intérêt (ROI), c’est-à-dire la main détectéeà un modèle CNN pour effectuer la classification du geste (pierre, feuille, ciseaux).\nCette stratégie en deux étapes permet de :\n\nRéduire le bruit visuel autour de la main (fond, visage, objets parasites)\nAugmenter la précision du classifieur CNN en se concentrant uniquement sur la main"
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#structure-des-données-yolo",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#structure-des-données-yolo",
    "title": "ShiFuMi IA – Reconnaissance de gestes",
    "section": "📚 Structure des données YOLO",
    "text": "📚 Structure des données YOLO\nLe format d’attente de YOLO est spécifique : chaque image d’entraînement doit être accompagnée d’un fichier d’annotation .txt contenant les informations de localisation des objets (ici, la main).\nL’organisation des données se présente généralement ainsi :\ndatasets/yolo_hand/\n├── images/\n│   ├── train/\n│   │   ├── img_001.jpg\n│   │   ├── img_002.jpg\n│   └── val/\n│       ├── img_101.jpg\n│       ├── img_102.jpg\n├── labels/\n│   ├── train/\n│   │   ├── img_001.txt\n│   │   ├── img_002.txt\n│   └── val/\n│       ├── img_101.txt\n│       ├── img_102.txt\n      Chaque fichier .txt contient une ou plusieurs lignes correspondant aux objets détectés dans l’image, selon le format suivant :\n&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\n\nToutes les valeurs sont normalisées entre 0 et 1 relativement à la taille de l’image.\nclass_id correspond ici à la main, donc souvent 0 dans le cadre d’un problème mono-classe."
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entraînement-avec-ultralytics-exemple",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entraînement-avec-ultralytics-exemple",
    "title": "ShiFuMi IA – Reconnaissance de gestes",
    "section": "🚀 Entraînement avec Ultralytics (Exemple)",
    "text": "🚀 Entraînement avec Ultralytics (Exemple)\nPour entraîner le modèle YOLOv8, on utilise la commande suivante :\nyolo task=detect mode=train \\\n  model=yolov8n.pt \\\n  data=config.yaml \\\n  epochs=50 \\\n  imgsz=640\nOù le fichier config.yaml contient :\npath: datasets/yolo_hand\ntrain: images/train\nval: images/val\n\nnames:\n  0: main\n\n\n\n\n\n\nPourquoi le notebook n’est pas publié ?\n\n\n\nL’outil de dévéloppement du site ne supporte pas la bibliothèque tensorflow, du coup cela rend impossible le déploiement du site car l’exécution des code du notebook ne passe pas. Toutefois, une fois les notebooks et scripts entièrement mis au propres, je les mettrai à disposition sur un dépot public github. Neanmoins étant donné que les données ne m’appartiennent pas et que je n’ai plus en ma possession certains liens directs vers celles-ci, elle ne seront pas publiées. Parcontre la structure des repertoires sera donnée."
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entraînement-du-modèle-yolo-epochs33100",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entraînement-du-modèle-yolo-epochs33100",
    "title": "ShiFuMi IA – Reconnaissance de gestes",
    "section": "Entraînement du modèle YOLO (Epochs=33/100)",
    "text": "Entraînement du modèle YOLO (Epochs=33/100)\n      Etant donné que le modèle était entrainé local et du fait qu’il mettait du temps, j’ai volontairement stoppé l’entraînement à la 33-ième itération. Car à ce stade les résultats de la détections des mains étaient satisfaisants (MAP2 environ égale à 0,94). Voici un extrait du résultat :"
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entraînement-du-modèle-cnn",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#entraînement-du-modèle-cnn",
    "title": "ShiFuMi IA – Reconnaissance de gestes",
    "section": "Entraînement du modèle CNN",
    "text": "Entraînement du modèle CNN\n      Une fois le modèle YOLO prêt à detecter les mains dans une image, le boxe (pour dire le cadre/rectangle contenant la main) est envoyé au modèle CNN afin qu’il puisse classer la main parmis les trois gestes : paper pour papier, rock pour pierre et scissors pour ciseau.\nLa vidéo ci-après illustre le résultat."
  },
  {
    "objectID": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#footnotes",
    "href": "INFO_MINI_PROJETS/shifumi-cnn-yolov8.html#footnotes",
    "title": "ShiFuMi IA – Reconnaissance de gestes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLe fine-tuning est une technique d’apprentissage supervisé qui consiste à prendre un modèle préentraîné sur un grand jeu de données général (comme ImageNet) et à le réentraîner sur un jeu de données spécifique à un problème particulier.↩︎\nMean Average Precision : C’est la métrique principale utilisée pour évaluer les performances d’un modèle de détection comme YOLO. Précision (Precision)↩︎"
  },
  {
    "objectID": "Projects/index.html",
    "href": "Projects/index.html",
    "title": "Bienvenue sur la page de mes publications",
    "section": "",
    "text": "Bienvenue sur mon espace dédié à la valorisation de projets en science des données, statistiques et développement d’outils interactifs.\nÀ travers ces publications, je partage mes travaux allant de la modélisation statistique à l’intelligence artificielle, en passant par l’analyse visuelle, la programmation, la Business Intelligence et des applications concrètes dans des domaines variés : santé, finances, éducation, entreprise, ou encore comportement du consommateur.\nChaque projet illustre ma volonté de combiner :\n\nune compréhension rigoureuse des données,\ndes méthodes modernes de modélisation et de visualisation,\net un souci de transparence, reproductibilité et accessibilité.\n\n🛠️ Outils principaux : Python, R, Quarto, Streamlit, Shiny, FastAPI, SQL, Django REST, etc."
  },
  {
    "objectID": "Projects/index.html#à-propos-de-cette-page",
    "href": "Projects/index.html#à-propos-de-cette-page",
    "title": "Bienvenue sur la page de mes publications",
    "section": "",
    "text": "Bienvenue sur mon espace dédié à la valorisation de projets en science des données, statistiques et développement d’outils interactifs.\nÀ travers ces publications, je partage mes travaux allant de la modélisation statistique à l’intelligence artificielle, en passant par l’analyse visuelle, la programmation, la Business Intelligence et des applications concrètes dans des domaines variés : santé, finances, éducation, entreprise, ou encore comportement du consommateur.\nChaque projet illustre ma volonté de combiner :\n\nune compréhension rigoureuse des données,\ndes méthodes modernes de modélisation et de visualisation,\net un souci de transparence, reproductibilité et accessibilité.\n\n🛠️ Outils principaux : Python, R, Quarto, Streamlit, Shiny, FastAPI, SQL, Django REST, etc."
  },
  {
    "objectID": "Projects/index.html#mes-catégories-de-projets",
    "href": "Projects/index.html#mes-catégories-de-projets",
    "title": "Bienvenue sur la page de mes publications",
    "section": "📂 Mes catégories de projets",
    "text": "📂 Mes catégories de projets\nLa liste ci-dessous regroupe mes publications par grandes thématiques. Cliquez sur une vignette pour explorer les projets associés.\nCliquez sur la catégorie de publications que vous souhaitez consulter"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-national-basketball-association",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-national-basketball-association",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "La National Basketball Association ?",
    "text": "La National Basketball Association ?\n\nLigue professionnelle de basketball la plus compétitive au monde\n30 équipes (Est et Ouest), 82 matchs de saison régulière\nDonnées riches et variées sur l’ensemble de la ligue :\n\nsuivi très détaillé des performances\ndes archives complètes depuis plus de 75 ans\n\n\n\nProblématique : Prédire la durée de carrière des joueurs nouvellement draftés"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#objectifs",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#objectifs",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Objectifs",
    "text": "Objectifs\n\nObjectif 1 : identifier des questions décrivant le jeu de données\n\nAnalyse exploratoire des données afin de répondre à 10 interrogations\nQuestions explorant différentes dimensions du basketball (équipes, les joueurs, les matchs, les play-offs ou encore la draft)\n\n\n\n\nObjectif 2 : prédire la durée de carrière des joueurs NBA\n\nSpécification et ajustement d’un modèle d’apprentissage automatique\n\n\n\n\n\nObjectif 3 : développer et déployer une application\n\nCréation d’ interfaces interactives pour afficher les réponses aux questions et d’autres informations sur la NBA"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#présentation-du-jeu-de-données-source-kaggle",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#présentation-du-jeu-de-données-source-kaggle",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Présentation du jeu de données (Source Kaggle)",
    "text": "Présentation du jeu de données (Source Kaggle)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravail de préparation des données : \n\nIntégration manuelle des vainqueurs NBA manquants;\nharmonisation des noms de franchises :\n\n\nPhiladelphia Warriors  ⇒  San Francisco Warriors  ⇒  Golden State Warriors"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#pourquoi-avons-nous-utilisé-des-classes",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#pourquoi-avons-nous-utilisé-des-classes",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Pourquoi avons-nous utilisé des classes ?",
    "text": "Pourquoi avons-nous utilisé des classes ?"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#présentation-de-la-classe-reponse",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#présentation-de-la-classe-reponse",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Présentation de la classe Reponse",
    "text": "Présentation de la classe Reponse\n\n\n\nclass Reponse:\n    def __init__(self, data: dict[pd.DataFrame]):\n        # Réalisons les tests nécessaire sur l'objet data\n\n        if (\n            not isinstance(data, dict)\n        ):\n            raise TypeError(\"L'argument data doit être un dictionnaire.\")\n\n        if (\n            any(not isinstance(data[key], pd.DataFrame) for key in data.keys())\n        ):\n            raise TypeError(\"Toutes les valeurs des clés doivents être des \"\n                            \"pandas.DataFrame.\")\n\n        # Testons qu'on a bien la clé common_player_info dans le dictionnaire\n        if (\"draft_history\" not in data.keys()):\n            raise KeyError(\"La clé 'draft_history' ne fait pas parti du dictionnaire\")\n        if (\"common_player_info\" not in data.keys()):\n            raise KeyError(\"La clé 'common_player_info' ne fait pas parti du \"\n                           \"dictionnaire\")\n        if (\"game\" not in data.keys()):\n            raise KeyError(\"La clé 'game' ne fait pas parti du dictionnaire\")\n\n        self.data = copy.deepcopy(data)\n\n\n\n\nEntrée :\n\nDictionnaire de tables\n\nVérification des entrées :\n\nLe type des entrées est vérifié\n\nPrésence des tables d’intérêt dans le dictionnaire\n\nExceptions levées : TypError, KeyError"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-méthodes-de-la-classe-reponse",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-méthodes-de-la-classe-reponse",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Les méthodes de la classe Reponse",
    "text": "Les méthodes de la classe Reponse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ⇐  Les méthodes indexées :\n\n(1) retourne une table contenant le nombre de victoires ou de défaites pour chaque équipe entre les saisons données.\n(2) retourne une table listant les équipes ayant remporté au moins le nombre de titres requis.\n(3) retourne un dictionnaire contenant deux tables, un pour chaque conférence (Est et Ouest)."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-méthode-equip_victoires_defaites_saison-et-ses-usages",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-méthode-equip_victoires_defaites_saison-et-ses-usages",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "La méthode equip_victoires_defaites_saison et ses usages",
    "text": "La méthode equip_victoires_defaites_saison et ses usages\n\ndef equip_victoires_defaites_saison(self, annee_debut: int, annee_fin: int,\n                                        season_type: str = 'Regular Season',\n                                        defaite: bool = False) -&gt; pd.DataFrame:\n\nEntrées : période – type de saison – l’issue du match\nTraitement pour l’obtention du nombre de victoires ou de défaites :\n\n# Détermination des équipes en fonction du résultat souhaité\n        game_chosen_season['Equipes'] = np.where(\n            ((game_chosen_season['wl_home'] == \"W\") & (not defaite)) |\n            ((game_chosen_season['wl_home'] == \"L\") & defaite),\n            game_chosen_season['team_name_home'],\n            game_chosen_season['team_name_away']\n        )\n\n        # Agrégation des résultats\n        results = game_chosen_season.groupby([\"season_years\", \"Equipes\"]).aggregate({\n            'wl_home': 'count',\n            'Equipes': 'first',\n            'season_years': 'first'\n        }).reset_index(drop=True)"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-méthode-equipe_remporte_au_moins_n_fois_le_titre",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-méthode-equipe_remporte_au_moins_n_fois_le_titre",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "La méthode equipe_remporte_au_moins_N_fois_le_titre",
    "text": "La méthode equipe_remporte_au_moins_N_fois_le_titre\n\ndef equipe_remporte_au_moins_N_fois_le_titre(self, nb_victoire_min: int = 3,\n                                                 debut_periode: int = 1946,\n                                                 fin_periode: int = 2023\n                                                 ) -&gt; pd.DataFrame\n(1) nba_champions_manquant = {\n\"1957-1958\": \"Atlanta Hawks\", \"1958-1959\": \"Boston Celtics\", \"1960-1961\": \"Boston Celtics\", \n\"1964-1965\": \"Boston Celtics\", \"1968-1969\": \"Boston Celtics\", \"1993-1994\": \"Houston Rockets\", \n\"1995-1996\": \"Chicago Bulls\", \"1999-2000\": \"Los Angeles Lakers\", \"2001-2002\": \"Los Angeles Lakers\", \"2005-2006\": \"Miami Heat\"\n}\n\nSélection des données\nIdentification des vainqueurs de chaque saison\nAlimentation des résultats (1)\nEdition de la table avec le nombre de titres gagnés sur la période par équipe\nRenvoi de la table avec les équipes avec au moins 3 titres NBA"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-méthode-classement_conferences",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#la-méthode-classement_conferences",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "La méthode classement_conferences",
    "text": "La méthode classement_conferences\n\ndef classement_conferences(self, season: str = '2022-2023',\n                               end: str = None) -&gt; dict[pd.DataFrame]:\n  . . .       \n\n  classement = {\"Conférence Est\": classement_est,\n              \"Conférence Ouest\": classement_ouest}\n\nSélection des données (saison régulière)\nIdentification du nombre de victoire par équipe\nClassement selon la conférence\nRenvoie les deux tables correspondantes aux conférences Est et Ouest"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-équipes-ayant-remporté-au-moins-n-titres-nba-entre-deux-périodes-données",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-équipes-ayant-remporté-au-moins-n-titres-nba-entre-deux-périodes-données",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Les équipes ayant remporté au moins N titres NBA, entre deux périodes données",
    "text": "Les équipes ayant remporté au moins N titres NBA, entre deux périodes données\n\nclass Reponse:\n    def __init__(self, data: dict[pd.DataFrame]):\n      ...\n    ...\n    def equipe_remporte_au_moins_N_fois_le_titre(self, nb_victoire_min: int = 3,\n                                                 debut_periode: int = 1946,\n                                                 fin_periode: int = 2023\n                                                 ) -&gt; pd.DataFrame\n\n\n\n\n\n\n\nParamètres de la méthode equipe_remporte_au_moins_N_fois_le_titre"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-équipes-ayant-remporté-au-moins-3-titres-nba",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#les-équipes-ayant-remporté-au-moins-3-titres-nba",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Les équipes ayant remporté au moins 3 titres NBA",
    "text": "Les équipes ayant remporté au moins 3 titres NBA\n\n\n\n\nEquipes ayant remporté au moins 3 titres NBA\n\n\nEquipe\nNombre.de.titre.NBA\n\n\n\n\nBoston Celtics\n17\n\n\nLos Angeles Lakers\n17\n\n\nGolden State Warriors\n7\n\n\nChicago Bulls\n6\n\n\nPhiladelphia 76ers\n6\n\n\nSan Antonio Spurs\n5\n\n\nMiami Heat\n3\n\n\nDetroit Pistons\n3\n\n\nHouston Rockets\n3"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#classement-des-conférences-à-la-fin-dune-saison-donnée",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#classement-des-conférences-à-la-fin-dune-saison-donnée",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Classement des conférences à la fin d’une saison donnée",
    "text": "Classement des conférences à la fin d’une saison donnée\n\nclass Reponse:\n    def __init__(self, data: dict[pd.DataFrame]):\n      ...\n    ...\n    def classement_conferences(self, season: str = '2022-2023',\n                               end: str = None) -&gt; dict[pd.DataFrame]:\n\n\n\n\n\n\n\nParamètres de la méthode classement_conferences"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#classement-des-conférences-à-la-fin-de-la-saison-2022-2023",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#classement-des-conférences-à-la-fin-de-la-saison-2022-2023",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Classement des conférences à la fin de la saison 2022-2023",
    "text": "Classement des conférences à la fin de la saison 2022-2023\n\n\n\n\n\n\nEquipe conférence Est\nVictoires\nPoints\nEquipe conférence Ouest\nVictoires\nPoints\n\n\n\n\nMilwaukee Bucks\n58\n9589\nDenver Nuggets\n53\n9495\n\n\nBoston Celtics\n57\n9671\nMemphis Grizzlies\n51\n9587\n\n\nPhiladelphia 76ers\n54\n9448\nSacramento Kings\n48\n9898\n\n\nCleveland Cavaliers\n51\n9205\nPhoenix Suns\n45\n9319\n\n\nNew York Knicks\n47\n9514\nGolden State Warriors\n44\n9753\n\n\nBrooklyn Nets\n45\n9295\nLA Clippers\n44\n9314\n\n\nMiami Heat\n44\n8977\nLos Angeles Lakers\n43\n9608\n\n\nAtlanta Hawks\n41\n9711\nMinnesota Timberwolves\n42\n9494\n\n\nToronto Raptors\n41\n9254\nNew Orleans Pelicans\n42\n9378\n\n\nChicago Bulls\n40\n9276\nOklahoma City Thunder\n40\n9633\n\n\nIndiana Pacers\n35\n9535\nDallas Mavericks\n38\n9366\n\n\nWashington Wizards\n35\n9279\nUtah Jazz\n37\n9600\n\n\nOrlando Magic\n34\n9136\nPortland Trail Blazers\n33\n9299\n\n\nCharlotte Hornets\n27\n9098\nSan Antonio Spurs\n22\n9269\n\n\nDetroit Pistons\n17\n9045\nHouston Rockets\n22\n9081"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#er-choix-de-la-draft-et-caractéristiques-physiques-des-joueurs",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#er-choix-de-la-draft-et-caractéristiques-physiques-des-joueurs",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "1er choix de la draft et caractéristiques physiques des joueurs",
    "text": "1er choix de la draft et caractéristiques physiques des joueurs\n\n\n\n\n\n\n1er choix de la draf NBA (2019-2023) \n\n\nSaison\nNom\nEquipe\nPays\n\n\n\n\n2019\nZion Williamson\nNew Orleans\nUSA\n\n\n2020\nAnthony Edwards\nMinnesota\nUSA\n\n\n2021\nCade Cunningham\nDetroit\nUSA\n\n\n2022\nPaolo Banchero\nOrlando\nUSA\n\n\n2023\nVictor Wembanyama\nSan Antonio\nFrance\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoids et tailles médians par position \n\n\nposition\nTaille.cm\nPoids.Kg\n\n\n\n\nPivot\n210.82\n108.86\n\n\nPivot/Ailier fort\n210.82\n113.40\n\n\nAilier\n200.66\n98.88\n\n\nAilier fort/Pivot\n208.28\n108.86\n\n\nAilier/Meneur\n200.66\n99.79\n\n\nArrière/Meneur\n190.50\n86.18\n\n\nArrière/Ailier\n198.12\n95.25"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#quel-modèle-avons-nous-choisis",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#quel-modèle-avons-nous-choisis",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Quel modèle avons-nous choisis ?",
    "text": "Quel modèle avons-nous choisis ?\n\nParmi les modèles de machine, nous avons choisi un modèle d’apprentissage supervisé et ce fut celui de la regression\n\n\n\n\n\n\nApprentissage supervisé - Modèle de regression"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#modèle-de-regression-linéaire",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#modèle-de-regression-linéaire",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Modèle de regression linéaire",
    "text": "Modèle de regression linéaire\nPRINCIPE : Prédire une variable quantitative à l’aide d’une ou plusieurs variables explicatives (quantitatives ou qualitatives)\n\\[\\begin{equation}\n  y = \\beta X + \\epsilon\n\\end{equation}\\]\noù \\(\\beta\\) est le coefficient associé aux variables explicatives \\(X\\) et \\(\\epsilon\\) le terme d’erreur.\n\nLe modèle de regression linéaire permet de prédire une variable quantitative à l’aide de variables explicatves appélée features. Son équation est la suivante : y la variable à prédire = beta x + epsilon ou x est l’ensemble des features et epsilon les termes d’erreur"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#choix-des-variables-explicatives",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#choix-des-variables-explicatives",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Choix des variables explicatives",
    "text": "Choix des variables explicatives\n\nNotre regard s’est d’abord tourné vers les variables age à la draft, poste occupé sur le terrain, taille et poids du joueurs.\n\n\n\n\n\n\nVariables préalables"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#choix-des-variables-explicatives-1",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#choix-des-variables-explicatives-1",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Choix des variables explicatives",
    "text": "Choix des variables explicatives\nVariables retenues\n\nLe poste occupé sur le terrain ayant un lien qvec la taille et le poids du joueur, nous avons seulement gardé le poste occupé sur le terrain en plus de l’âge à la draft. Cela a permis d’éviter à un problème de multicolinéarité et donc d’éviter un mauvais ajustement du modèle.\n\n\n\n\n\n\nVariables réténues pour l’ajustement du modèle"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#entrainement-du-modèle",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#entrainement-du-modèle",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Entrainement du modèle",
    "text": "Entrainement du modèle\n\nEntraînement et évaluation croisée\nNous lançons cinq entraînements successifs du modèle, chacun sur 80 % des données, en réservant à chaque fois un pli différent pour la validation (données de tests).\n\n\n\n\n\n\nValidation croisée K-Fold (k = 5)"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#entrainement-du-modèle-1",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#entrainement-du-modèle-1",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Entrainement du modèle",
    "text": "Entrainement du modèle\n\n\n\n\n\nRésumé de l’apprentissage automatique\n\n\n\n\n\n\nAprès avoir validé la robustesse du modèle via la CV, nous le ré-entraînons une dernière fois sur 100 % des observations disponibles, sans rien réserver comme jeu de test.\n\nPourquoi ? Pour exploiter au maximum l’information disponible et obtenir un modèle final plus performant.\nComment s’assurer de sa fiabilité ? Nous nous appuyons entièrement sur la moyenne (4,59) et l’écart-type (± 0,12) des RMSE issus de la CV comme mesure de sa capacité de généralisation.\n\nPerformance confirmée\n\nGrâce à cette validation croisée, nous avons vu que le modèle généralise bien : les scores ne varient que de ± 0,12 autour de 4,59.\n\nInterprétations et prédictions\n\nForts de cette robustesse, nous pouvons passer sereinement à l’analyse de l’importance des variables, à l’interprétation des effets et en dernier lieu à la production de prédictions fiables sur de nouvelles données."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#methode-fit-de-la-classe-linearregression",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#methode-fit-de-la-classe-linearregression",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Methode fit de la classe LinearRegression",
    "text": "Methode fit de la classe LinearRegression\ndef fit(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"\n        Estime les coefficients de régression par OLS.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            Matrice des prédicteurs.\n        y : np.ndarray\n            Vecteur cible.\n\n        Returns\n        -------\n        np.ndarray\n            Coefficients estimés.\n        \"\"\"\n        if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n            raise TypeError(\"X et Y doivent-être de type np.ndarray\")\n\n        cond_number = np.linalg.cond(X.T @ X)\n        if cond_number &gt; 1e10:\n            warnings.warn(\n                \"Matrice X.T @ X mal conditionnée (cond &gt; 1e10).\"\n                \"Risque de multicolinéarité.\"\n            )\n\n        X_X_inv = np.linalg.pinv(X.T @ X)\n        Beta = X_X_inv @ X.T @ y\n        return Betas\n\nPour repondre à la problématique posée, nous avons écrit une classe linearRegression qui entraine le modèle, le valide, et prédit la durée de carrière des joueurs. Avant d’ajuster le modèle, nous nous assurons que la matrice est bien conditionnée. (Sur le code on peut voir qu’un nombre de condition &gt; 1e10 signifie un problème de multicolinéarité)"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#résultats-de-lentraînement",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#résultats-de-lentraînement",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Résultats de l’entraînement",
    "text": "Résultats de l’entraînement\n\n\n\n\n\n\nRésultats de l’entraînement du modèle \n\n\nVariable\nEstimation\nBorne.inférieure\nBorne.supérieure\n\n\n\n\nintercept\n13.0807938172894\n11.2139999315576\n14.9475877030211\n\n\nage_at_draft\n-0.304520510475786\n-0.385970108084788\n-0.223070912866783\n\n\nPivot-Ailier fort\n4.03469135857516\n2.58019430274557\n5.48918841440475\n\n\nAilier\n-0.723448794163931\n-1.24222365227521\n-0.204673936052656\n\n\nAilier fort-Pivot\n3.00118640834299\n1.80323628965945\n4.19913652702653\n\n\nAilier-Arrière\n1.31879081528329\n-0.203041070946694\n2.84062270151327\n\n\nArrière\n-0.779850850158738\n-1.30098176750811\n-0.258719932809363\n\n\nArrière-Ailier\n1.88960963320118\n0.805602797554438\n2.97361646884792\n\n\n\n\n\n\n\n\n\n\n\n\nAge à la draft + 1  ⇒  diminution moyenne de la durée de carrière de 0.30 ans\nUn pivot/ailier a plus de chance de durer à la NBA que les joueurs occupant les autres postes\nUn arrière a moins de chance de durer à la NBA que les joueurs occupant les autres postes\n\n\n\nPrédiction : \\(exp = 13,08 - 0.305*Age_{draft} + \\beta_j*Poste_j\\)\nAge_a_la_draft = 18 ans et Poste = Pivot\n\nDurée de carrière : 7.6 ans\nIntervalle de confiance [4.3, 10.9]\n\n\n\n\n\n\n\n\nPar exemple pour un joueur qui s’est présenté à 18 ans la draft, et occupant le poste de pivot, le modèle prédit une durée de carrière de 7,6 ans avec un IC allant de 4,3 à 10,9 ans]"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#synthèse-de-létude",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#synthèse-de-létude",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Synthèse de l’étude",
    "text": "Synthèse de l’étude\n\nCe projet entre pleinement dans le domaine de l’informatique appliquée aux données.\nSuivi du processus : nettoyage, exploration et modélisation\nRéponses rigoureuses aux questions posées\nConstruction d’un modèle supervisé pour prédire la durée de carrière des joueurs"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#avantages-et-difficultés",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#avantages-et-difficultés",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "Avantages et difficultés",
    "text": "Avantages et difficultés\n\n\n\n\n\n\n\n\nAvantages\n\n\n\nAcquisition de compétences transversales : manipulation de données, machine learning, visualisation interactive.\nIntervalle de confiance des prédictions\nImplémentation manuelle du modèle supervisé\nModularisation du code facilitant la maintenance et la réutilisation.\nPrise d’initiatives (création d’une interface, traitement des noms d’équipes changeants, gestion des données manquantes)\n\n\n\n\n\n\n\n\n\n\n\n\nDifficultés\n\n\n\nErreur de prédiction plus ou moins élevée (4,6 ans)\nDes attentes initiales manquaient de clarté\nQuelques difficultés à identifier et corriger les incohérences dans les données historiques"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#end",
    "href": "projet-traitement-donnees/report_writing/Presentation/presentation-ptd.html#end",
    "title": "Prédiction de la durée de carrière des joueurs NBA",
    "section": "END",
    "text": "END\n\n\n\n\n\n\n\n\n\n\n\n\nSoutenance du projet traitement de données 1A"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "",
    "text": "La NBA (National Basketball Association) est la ligue professionnelle de basketball la plus compétitive au monde. Depuis plus de 75 ans, elle réunit les meilleurs talents, entraîneurs et infrastructures. Aujourd’hui, la ligue est composée de 30 équipes réparties entre deux conférences (Est et Ouest), disputant chacune 82 matchs de saison régulière. Les huit meilleures équipes de chaque conférence accèdent ensuite aux séries éliminatoires (play-offs), structurées en quatre tours successifs en format au meilleur des sept matchs (Guan, Wang, and Yuan 2022; Teramoto and Cross 2010).\n      Malgré son prestige, la carrière moyenne d’un joueur en NBA reste relativement courte. Plusieurs études montrent que cette longévité varie selon l’origine des joueurs et leur parcours : par exemple, les joueurs étrangers sans passage par une université américaine semblent avoir une carrière plus brève que ceux issus du système universitaire des USA (Groothuis and Hill 2018).\n      Du point de vue des données, la NBA génère une volumétrie importante d’informations sur les joueurs : caractéristiques physiques, parcours, statistiques de jeu, durée de carrière, etc. Ces données sont une opportunité pour mener une analyse exploratoire orientée science des données, dans le but d’identifier des patterns significatifs.\nL’objectif de cette étude est donc d’utiliser les données disponibles pour prédire la durée de carrière d’un joueur NBA à partir d’attributs personnels (âge, position, parcours académique, etc.) à l’aide de modèles de machine learning.\n\n\n\n      Dans le prolongement de l’introduction, cette section vise à expliciter la méthodologie adoptée pour répondre aux différentes questions posées dans le cadre de ce projet, qu’il s’agisse des questions obligatoires ou de celles laissées au choix de l’équipe.\n      Nous commencerons par une présentation claire et synthétique de la classe que nous avons développée pour structurer notre l’apprentissage automatique. Cette classe, que nous avons construite manuellement, intègre des fonctionnalités avancées telles que la gestion des variables catégorielles, la détection de la multicolinéarité, la validation croisée, ainsi que la régularisation de type Ridge. Elle se distingue notamment de l’implémentation classique LinearRegression de la bibliothèque scikit-learn, en y ajoutant des méthodes personnalisées adaptées à nos besoins spécifiques.\n      Ensuite, nous détaillerons les principales étapes du processus de modélisation, en expliquant comment les méthodes de cette classe nous permettent de mettre en œuvre une régression linéaire multiple de manière complète et contrôlée. Nous décrirons le rôle de chaque méthode dans le traitement des données, l’entraînement du modèle, l’évaluation des performances et l’interprétation des résultats, notamment à travers le calcul d’intervalles de confiance et d’indicateurs d’erreur comme le RMSE.\n      Par ailleurs, une application web interactive a été développée avec Shiny afin de rendre notre travail accessible à travers une interface utilisateur conviviale. Nous présenterons cette application en détail : son objectif, son fonctionnement, et ses principales fonctionnalités. Nous expliquerons également les choix techniques qui ont guidé son développement, ainsi que la manière dont elle permet de visualiser et d’interagir avec les résultats issus de notre modèle de régression.\n      Enfin, nous proposerons un retour critique sur notre expérience de projet. Cette partie mettra en lumière les défis rencontrés tout au long du processus, les solutions que nous avons mises en œuvre, ainsi que la manière dont nous avons organisé et coordonné notre travail en équipe. Nous partagerons les enseignements tirés de cette collaboration, tant sur le plan technique que méthodologique et humain."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#introduction",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#introduction",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "",
    "text": "La NBA (National Basketball Association) est la ligue professionnelle de basketball la plus compétitive au monde. Depuis plus de 75 ans, elle réunit les meilleurs talents, entraîneurs et infrastructures. Aujourd’hui, la ligue est composée de 30 équipes réparties entre deux conférences (Est et Ouest), disputant chacune 82 matchs de saison régulière. Les huit meilleures équipes de chaque conférence accèdent ensuite aux séries éliminatoires (play-offs), structurées en quatre tours successifs en format au meilleur des sept matchs (Guan, Wang, and Yuan 2022; Teramoto and Cross 2010).\n      Malgré son prestige, la carrière moyenne d’un joueur en NBA reste relativement courte. Plusieurs études montrent que cette longévité varie selon l’origine des joueurs et leur parcours : par exemple, les joueurs étrangers sans passage par une université américaine semblent avoir une carrière plus brève que ceux issus du système universitaire des USA (Groothuis and Hill 2018).\n      Du point de vue des données, la NBA génère une volumétrie importante d’informations sur les joueurs : caractéristiques physiques, parcours, statistiques de jeu, durée de carrière, etc. Ces données sont une opportunité pour mener une analyse exploratoire orientée science des données, dans le but d’identifier des patterns significatifs.\nL’objectif de cette étude est donc d’utiliser les données disponibles pour prédire la durée de carrière d’un joueur NBA à partir d’attributs personnels (âge, position, parcours académique, etc.) à l’aide de modèles de machine learning.\n\n\n\n      Dans le prolongement de l’introduction, cette section vise à expliciter la méthodologie adoptée pour répondre aux différentes questions posées dans le cadre de ce projet, qu’il s’agisse des questions obligatoires ou de celles laissées au choix de l’équipe.\n      Nous commencerons par une présentation claire et synthétique de la classe que nous avons développée pour structurer notre l’apprentissage automatique. Cette classe, que nous avons construite manuellement, intègre des fonctionnalités avancées telles que la gestion des variables catégorielles, la détection de la multicolinéarité, la validation croisée, ainsi que la régularisation de type Ridge. Elle se distingue notamment de l’implémentation classique LinearRegression de la bibliothèque scikit-learn, en y ajoutant des méthodes personnalisées adaptées à nos besoins spécifiques.\n      Ensuite, nous détaillerons les principales étapes du processus de modélisation, en expliquant comment les méthodes de cette classe nous permettent de mettre en œuvre une régression linéaire multiple de manière complète et contrôlée. Nous décrirons le rôle de chaque méthode dans le traitement des données, l’entraînement du modèle, l’évaluation des performances et l’interprétation des résultats, notamment à travers le calcul d’intervalles de confiance et d’indicateurs d’erreur comme le RMSE.\n      Par ailleurs, une application web interactive a été développée avec Shiny afin de rendre notre travail accessible à travers une interface utilisateur conviviale. Nous présenterons cette application en détail : son objectif, son fonctionnement, et ses principales fonctionnalités. Nous expliquerons également les choix techniques qui ont guidé son développement, ainsi que la manière dont elle permet de visualiser et d’interagir avec les résultats issus de notre modèle de régression.\n      Enfin, nous proposerons un retour critique sur notre expérience de projet. Cette partie mettra en lumière les défis rencontrés tout au long du processus, les solutions que nous avons mises en œuvre, ainsi que la manière dont nous avons organisé et coordonné notre travail en équipe. Nous partagerons les enseignements tirés de cette collaboration, tant sur le plan technique que méthodologique et humain."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#cadre-du-projet",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#cadre-du-projet",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "Cadre du projet",
    "text": "Cadre du projet\n      Ce projet s’inscrit dans le cadre des travaux de fin d’année des étudiants de première année à l’École Nationale de la Statistique et de l’Analyse de l’Information (ENSAI).\nIl constitue le volet informatique appliqué aux données du programme académique.\nL’objectif est de mobiliser les compétences acquises en programmation, modélisation et analyse pour résoudre une problématique concrète à partir de données réelles.\n➡️ Les slides de la présentation sont disponibles à l’adresse suivante : Accéder aux slides"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#aspects-techniques-du-projet",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#aspects-techniques-du-projet",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "Aspects techniques du projet",
    "text": "Aspects techniques du projet\n\nPrésentation du jeu de données\n      L’étude repose sur un ensemble de données relatives au basketball, plus précisément à la ligue américaine : la NBA. Ces données nous ont été fournies sous forme de plusieurs fichiers CSV, que nous avons convertis en tables pour les besoins du traitement. Chaque fichier contient des informations spécifiques sur les matchs, les équipes ou les joueurs de la ligue. Les données sur les confrontations entre équipes couvrent la période allant de 1946 à 2023.\n      La table common_player_info fournit des caractéristiques de certains joueurs telles que leur nom, prénom, date de naissance, année de draft, statut de “greatest” ou non, durée de carrière, taille, poids, etc. La table draft_combine_history contient quant à elle des informations intéressantes sur la draft des joueurs : université d’origine, équipe de destination, rang de draft, etc. Enfin, la table game recense les matchs de la NBA de la saison 1946-1947 à la saison 2022-2023**. On y trouve notamment des variables essentielles pour filtrer les données et mener nos analyses : saison, date de match, issue de la rencontre (victoire ou défaite), et nombre de points marqués par chaque équipe lors des confrontations. C’est sur ces trois tables principalement que nous réaliserons notre traitement.\n\n\nExploration et harmonisation des données\n      Lors de la phase exploratoire des données, nous avons constaté que les informations de la table game pouvaient évoluer d’une saison à l’autre. En effet, il n’est pas rare qu’une franchise change de nom, notamment lorsqu’elle déménage dans une nouvelle ville. Par exemple, les Seattle SuperSonics sont devenus les Oklahoma City Thunder après leur relocalisation, et les New Jersey Nets ont été renommés Brooklyn Nets après leur installation à Brooklyn.\nAfin d’éviter de biaiser nos résultats en considérant à tort que deux noms différents correspondent à deux équipes distinctes, nous avons effectué des recherches sur les 30 franchises de la NBA, accompagnés par l’IA, pour identifier celles ayant changé de nom entre 1946 et 2023.\n      À l’aide de manipulations de tables avec pandas et de dictionnaires, nous avons harmonisé les noms afin d’attribuer à chaque franchise son nom actuel. Cette étape nous permet ainsi de travailler avec un ensemble cohérent de 30 modalités représentant les franchises existantes.\n      De plus, dans la table game, les confrontations des playoffs sont absentes pour les saisons suivantes : 1958-1959, 1960-1961, 1964-1965, 1968-1969, 1993-1994, 1995-1996, 1999-2000, 2001-2002 et 2005-2006. Il est donc impossible de déterminer le vainqueur du titre pour ces saisons à partir des seules données disponibles. Nous avons donc recherché et récupéré sur Internet les vainqueurs du titre NBA pour ces années. Ainsi, lorsque l’analyse porte sur le nombre de titres NBA remportés par chaque franchise ou sur l’identification du champion lors d’une saison précise, ces données externes sont intégrées à l’analyse.\n\n\nRéponses aux questions\n      L’un des objectifs principaux de ce projet était de mener une première analyse exploratoire des données afin de répondre à un certain nombre d’interrogations. Parmi celles-ci, deux questions nous ont été imposées.\n\nPrésentation des questions posées\n      Les deux questions obligatoires définies dans le cadre du projet sont les suivantes :\n\nQ1. Quelles sont les équipes ayant remporté au moins 3 titres NBA, en précisant le nombre de titres pour chacune d’elles ?\nQ2. Quel était le classement des conférences Ouest et Est à la fin de la saison régulière 2022-2023 ?\n\n      Les questions ci-dessous sont celles que nous avons jugées les plus intéressantes à explorer et auxquelles nous avons choisi d’apporter des réponses :\n\nQ3. Quelle est la taille et le poids médians des joueurs selon leur poste ?\nQ4. Quelles sont les universités ayant formé le plus de joueurs évoluant en NBA ?\nQ5. En dehors des États-Unis, quels sont les pays d’origine les plus représentés parmi les joueurs NBA ?\nQ6. Quelle est l’évolution du nombre de rencontres sur périodes données en NBA ?\nQ7. Qui sont les numéros 1 de la draft lors des 5 dernières saisons ?\nQ8. Quelles sont les équipes ayant obtenu le plus de victoires et de défaites durant les 5 dernières saisons régulières ?\nQ9. Quels sont les champions NBA au cours des 8 dernières saisons ?\nQ10. Quelles sont les équipes ayant remporté le titre NBA deux années consécutives ?\n\n\n\nPrésentation des réponses apportées\n      Les réponses aux questions sont toutes dans le notebook jupyter (reponses_aux_questions.ipynb). Dans le présent document, nous présentons celles concernant les questions obligatoires, ainsi qu’une question supplémentaire que nous jugeons particulièrement pertinente.\n\nLes équipes ayant remporté au moins 3 titres NBA\n\n      Lorsqu’on observe le nombre de titres remportés par chaque franchise en NBA, on constate une grande variabilité. Par ailleurs, en se concentrant sur les équipes ayant remporté au moins 3 titres, plusieurs franchises emblématiques ressortent, ayant marqué l’histoire du basketball en NBA (cf. tableau d’après). On observe que les Boston Celtics et les Los Angeles Lakers dominent largement le palmarès, avec chacun 17 titres sur la période 1946-2023. Ils sont suivis par les Golden State Warriors, qui en comptent 7.\n\n\n\nRécapitulatif du nombre de titre gagné de chaque franchise\n\n\nEquipe\nNombre de titre NBA\n\n\n\n\nBoston Celtics\n17\n\n\nLos Angeles Lakers\n17\n\n\nGolden State Warriors\n7\n\n\nChicago Bulls\n6\n\n\nPhiladelphia 76ers\n6\n\n\nSan Antonio Spurs\n5\n\n\nMiami Heat\n3\n\n\nDetroit Pistons\n3\n\n\nHouston Rockets\n3\n\n\n\n\n\n\nLe classement des conférences Ouest et Est à la fin de la saison régulière 2022-2023\n\n      Pour déterminer les meilleures équipes de chaque conférence, nous avons analysé leur nombre de victoires durant la saison régulière 2022-2023. Le classement inclut également le total de points marqués par chaque franchise. En cas d’égalité, ce total permet de départager les équipes. Ainsi, les vainqueurs des conférences Est et Ouest sont respectivement les Milwaukee Bucks et les Denver Nuggets (cf. le tableau ci-dessous).\n\n\n\nClassement des conférences Ouest et Est à la fin de la saison régulière 2022-2023\n\n\nEquipe conférence Est\nVictoires\nPoints\nEquipe conférence Ouest\nVictoires\nPoints\n\n\n\n\nMilwaukee Bucks\n58\n9589\nDenver Nuggets\n53\n9495\n\n\nBoston Celtics\n57\n9671\nMemphis Grizzlies\n51\n9587\n\n\nPhiladelphia 76ers\n54\n9448\nSacramento Kings\n48\n9898\n\n\nCleveland Cavaliers\n51\n9205\nPhoenix Suns\n45\n9319\n\n\nNew York Knicks\n47\n9514\nGolden State Warriors\n44\n9753\n\n\nBrooklyn Nets\n45\n9295\nLA Clippers\n44\n9314\n\n\nMiami Heat\n44\n8977\nLos Angeles Lakers\n43\n9608\n\n\nAtlanta Hawks\n41\n9711\nMinnesota Timberwolves\n42\n9494\n\n\nToronto Raptors\n41\n9254\nNew Orleans Pelicans\n42\n9378\n\n\nChicago Bulls\n40\n9276\nOklahoma City Thunder\n40\n9633\n\n\nIndiana Pacers\n35\n9535\nDallas Mavericks\n38\n9366\n\n\nWashington Wizards\n35\n9279\nUtah Jazz\n37\n9600\n\n\nOrlando Magic\n34\n9136\nPortland Trail Blazers\n33\n9299\n\n\nCharlotte Hornets\n27\n9098\nSan Antonio Spurs\n22\n9269\n\n\nDetroit Pistons\n17\n9045\nHouston Rockets\n22\n9081\n\n\n\n\n\n\nLes équipes ayant remporté le titre NBA deux années consécutives\n\n      Remporter le titre NBA deux années de suite est un exploit rare, compte tenu de la forte compétitivité entre les franchises. Parmi celles qui y sont parvenues, 8 équipes ont réussi à conserver leur titre deux saisons consécutives : Los Angeles Lakers, Syracuse Nationals, Boston Celtics, Detroit Pistons, Chicago Bulls, Houston Rockets, Miami Heat, Golden State Warriors. Il est à noter que les Boston Celtics détiennent un record historique avec 8 titres consécutifs entre les saisons 1958-1959 et 1965-1966.\n\n\nPrésentation des methodes utilisées\n      Pour répondre aux différentes questions, nous avons créé une classe Reponse dans laquelle chaque question fait l’objet d’une méthode dédiée. Ces méthodes intègrent des paramètres afin de permettre aux utilisateurs d’afficher les résultats tout en pouvant appliquer des filtres personnalisés.\n      Concernant la première question obligatoire, nous avons défini une méthode prenant en paramètres nb_victoire_min (le nombre minimal de titres à remporter), debut_periode et fin_periode (années délimitant la période d’étude). Ces deux derniers paramètres permettent de filtrer les données sur la période souhaitée. Ensuite, nous récupérons les vainqueurs de chaque saison et comptons les titres remportés par chaque franchise à l’aide de manipulations avec pandas. Enfin, nous ne conservons que celles ayant remporté un nombre de titres supérieur ou égal à celui indiqué.\n      Pour la deuxième question obligatoire, une autre méthode a été créée avec les paramètres season (saison d’intérêt) et end (date de fin pour limiter les matchs pris en compte). Si le paramètre end est fourni, le classement est calculé à cette date précise ; sinon, le classement à la fin de la saison régulière est affiché. Ces paramètres permettent de filtrer les confrontations selon la saison choisie et, le cas échéant, selon la date limite.\n      Pour répondre à la dixième question relative aux équipes ayant remporté au moins 2 années consécutives le titre, une méthode prenant les paramètres N, debut_periode et fin_periode a été conçue. Elle permet d’identifier les équipes ayant remporté le titre au moins N années consécutives dans une période définie. Après filtrage des données, les vainqueurs sont extraits, et un traitement via la manipulation de dictionnaires permet de repérer les franchises ayant remporté le titre plusieurs années de suite.\n      De façon générale, les autres méthodes de la classe Reponse suivent une logique similaire : elles exploitent des paramètres pour affiner les résultats et répondre plus précisément aux besoins de l’utilisateur, contrairement à des méthodes fixes sans personnalisation. Ces traitements sont rendus possibles grâce aux fonctionnalités offertes par le package pandas.\n\n\n\n\n\nDiagramme de classes\n\n\n\n\n\n\n\nPrésentation de la problématique considérée et de la réponse apportée\n      Comme énoncé et justifié dans l’introduction, notre problématique est la suivante : Prédire la durée de carrière des joueurs en NBA.\n\nComment nous sommes parvenus à répondre à cette problématique ?\n      Pour répondre à la problématique, nous avons choisi d’utiliser un modèle de régression linéaire multiple. Ce modèle permet de prédire la durée de carrière des joueurs à partir de leurs caractéristiques individuelles.\nNous avons identifié la table common_player_info comme base pertinente pour l’entraînement. Cette table contient plusieurs informations sur les joueurs. Les variables explicatives que nous avons sélectionnées sont : la date de naissance, l’année de draft, le poste occupé sur le terrain, la taille, le poids. La durée de carrière est notre variable cible.\nPlutôt que d’utiliser directement le modèle de régression linéaire de scikit-learn (LinearRegression), nous avons choisi d’implémenter notre propre classe, nommée LinearRegression. Cette classe permet :\n\nd’entraîner (ajuster) un modèle;\nde réaliser des prédictions;\nd’évaluer les performances du modèle à l’aide de k-fold cross-validation 1.\n\nNous avons également :\n\nintégré une méthode de régression Ridge pour traiter d’éventuels problèmes de multicolinéarité;\ncalculé les intervalles de confiance pour les prédictions.\n\nUn diagramme de flux détaillant le fonctionnement de notre classe est présenté en annexe.\nFinalement, pour des problèmes de multicolinéarité entre certaines variables explicatives et des ajustements peu satisfaisants, nous avons choisi d’inclure l’âge à la draft ( année de draft - année de naissance) et la position (poste sur le terrain), ces deux variables s’étant révélées plus pertinentes que la taille ou le poids.\nNotre modèle final avait donc pour formule :\n\\[\\begin{equation}\n  \\text{Durée de carrière} = \\beta_0 + \\beta_1 \\cdot \\text{Âge à la draft} + \\sum_{j=1}^{k} \\beta_{j+1} \\cdot \\text{Position}_j + \\varepsilon\n(\\#eq:label)\n\\end{equation}\\]\noù \\(\\beta_0\\) est l’ordonnée à l’origine (intercept), \\(\\beta_1\\) est le coefficient associé à l’âge à la draft, \\(\\text{Position}_j\\) représente les variables indicatrices pour les différentes positions, \\(\\beta_{j+1}\\) sont les coefficients qui leur sont associés et \\(\\varepsilon\\) est le terme d’erreur. Les variables omises, telles que la taille et le poids, influencent directement la position d’un joueur. Par exemple, un joueur plus grand est souvent appelé à jouer au poste de pivot. De même, un joueur possédant une grande taille, un poids important et une bonne rapidité physique sera généralement affecté aux postes d’ailier fort et de pivot simultanément. En gardant ces variables, il y’avait une incohérence dans les résultats certainement dûe à la multicolinéarité entre elles et la position. La reponse à la question 3 confirme le lien entre le poste occupé sur le terrain, la taille et le poids (cf. figure en annexes).\nLe notebook apprentissage_automatique.ipynb permet de retrouver les résultats dans la section d’après.\n\n\nReponse apportée à la problématique\n\nRésultats de l’entraînement et interprétations\nNous avons réentraîné notre modèle sur l’ensemble des données disponibles, sans réserver de jeu de test.\nPourquoi ? Pour exploiter toute l’information disponible et améliorer la précision du modèle final.\nMais comment garantir sa fiabilité ?\nNous nous basons entièrement sur les résultats issus de la validation croisée : la moyenne des erreurs quadratiques moyennes (RMSE) était de 4,59, avec une faible variabilité (écart-type de 0,12).\nCela indique une bonne capacité de généralisation du modèle.\n\n\nAnalyse des effets\nLe modèle confirme des tendances observées dans la réalité : - Plus l’âge à la draft est élevé, plus la durée de carrière est courte. - Les joueurs occupant le poste de pivot ou ailier fort ont en moyenne une carrière plus longue, grâce à leur polyvalence.\nSur la figure des resultats de l’entrainement du modèle (graphique a)), nous constatons que les performances de notre classe LinearRegression sont comparables à celles du module linear_model de Scikit-learn.\n\n\nExemple de prédiction\nPour un joueur de 18 ans, sélectionné au poste de pivot, le modèle prédit : - Durée de carrière estimée : 7,6 ans - Intervalle de confiance : [4,3 ; 10,9]\n\n\nLimites\nMême si l’intervalle de confiance renforce la crédibilité des résultats, il convient de noter que la RMSE reste élevée (environ 4,6 ans), ce qui signifie que les erreurs de prédiction peuvent être importantes dans certains cas.\n\n\n\n\n\n\n\n(a) Resultat de notre modèle VS celui de Sci-kit Learn\n\n\n\n\n\n\n\n(b) Moyennes des erreurs quadratiques moyennes\n\n\n\n\nFigure 1: Résultats de l’entrainement du modèle\n\n\n\n\n\n\nLien entre les classe créées et l’application\n      Dans un souci de modularité, nous avons structuré notre application autour de trois interfaces principales, chacune encapsulée dans une classe distincte.\n\nLa première interface est la page d’accueil, qui ne fait pas directement appel aux classes LinearRegression ou Reponse. Elle sert principalement d’introduction à l’application mais est basée sur une classe.\nLa seconde interface, Reponses aux questions, repose sur la classe Reponse (détaillé en annexe) pour répondre aux requêtes des utilisateurs. Elle permet également d’appliquer des filtres sur les résultats, comme l’affichage des médianes, moyennes selon le choix de l’utilisateur, ou encore un filtrage par date.\nEnfin, la page Machine Learning présente les résultats d’entraînement du modèle ajusté ainsi que sa validation. Elle offre aussi la possibilité à l’utilisateur de faire des prédictions en renseignant les caractéristiques (features) d’un joueur de la NBA. Cette page utilise une classe CareerPrediction, laquelle dépend elle-même de la classe LinearRegression.\n\n\n\nArchitecture du projet\n      La figure ci-dessous présente l’architecture générale du projet. Chaque dossier a été conçu pour répondre à une préoccupation spécifique. Pour que ces dossiers soient reconnus comme des paquets Python, des fichiers __init__.py y ont été ajoutés avec la définition des modules. Un fichier setup.py permet de gérer les dépendances internes et de faciliter les interactions entre les différents modules.\nEnfin, tous les packages nécessaires à l’exécution des codes ainsi que leurs versions sont listés dans le fichier requirements.txt.\n\n\n\n\n\nArchitecture du projet\n\n\n\n\n\n\nPackages et bibliothèques utilisés\n      Le projet repose sur plusieurs bibliothèques Python essentielles, parmi lesquelles :\n\nPandas : utilisée pour la manipulation de données tabulaires, elle facilite le traitement et la transformation des données structurées.\nMatplotlib : permet de créer des visualisations détaillées et personnalisées sous forme de graphiques.\nNumPy : utile pour les opérations mathématiques et la gestion efficace de tableaux multidimensionnels.\nScikit-learn : utilisée pour les tâches de machine learning. Elle nous a permis de comparer nos implémentations personnalisées avec les modèles standards de la bibliothèque).\nShiny : a été utilisée pour développer l’interface utilisateur de l’application.\n\nCes outils ont été essentiels pour mener à bien les étapes d’analyse, de modélisation et de visualisation du projet.\n\n\nCouverture des tests\n      La quasi-totalité des classes et fonctions a été testée, y compris celles liées aux pages de l’application web développée.\nAu total, 214 tests ont été réalisés, assurant une couverture de plus de 94% du code.\n\n\n\n\n\nTests effectués"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#utilisation-de-lapplication",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#utilisation-de-lapplication",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "Utilisation de l’application",
    "text": "Utilisation de l’application\n\nPage d’Accueil\nLa page Accueil présente des statistiques générales sur la NBA :\n\nNombre total de joueurs,\nNombre d’équipes,\nNombre d’universités représentées.\n\nL’utilisateur peut également explorer avec des filtres :\n\nLe nombre de joueurs Greatest 2,\nL’évolution du nombre de matchs par saison,\nLa répartition des positions de joueurs entre deux années définies.\n\nCes éléments sont illustrés en figure ci-dessous.\n\n\n\n\n\nPage d’accueuil de l’application\n\n\n\n\n\n\nPage de reponses aux questions\nCette page permet à l’utilisateur de :\n\nVisualiser les résultats des analyses,\nInteragir avec la base via des champs de saisie dynamiques.\n\nExemple : pour la question “Classer les universités selon le nombre de joueurs formés”, l’utilisateur peut :\n\nDéfinir une période,\nSélectionner le Top N universités.\n\nLes réponses sont regroupées par catégories (Joueurs, Équipes, etc.).\nLa figure ci-dessous illustre ce qui a été dis en ammont.\n\n\n\n\n\nPage de reponses aux questions de l’application\n\n\n\n\n\n\nPage des résultats du modèle d’apprentissage supervisé et de prédiction de la durée de carrière : Machine Learning\n      Sur cette page nous affichons les résultats du modèle à la suite de son entraînement puis la validation du modèle. On voit que l’erreur quadratique moyenne 3 tourne autour de 4,6 en moyenne (le graphique à droite sur la juste en bas). Ensuite pour permettre à l’utilisateur de prédire la durée de carrière d’un joueur lambda, des champs de saisies sont affichés. Il pourra donc entrer la date de naissance du joueur, la date de sa draft et choisir sa position. Même s’il ne connait pas les date exacte, ce n’est pas grave, c’est l’année qui importe. Ensuite en cliquant sur le bouton Predire il obtient la prédiction avec un intervalle de confiance.\nL’application a été déployée sur un server posit (version gratuite) et responsive donc s’adapte à différentes tailles d’écran. Pour acceder à l’application cliquez ici.\n\n\n\n\n\nPage des résultats du modèle d’apprentissage supervisé et de prédiction de la durée de carrière"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#retour-dexpérience",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#retour-dexpérience",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "Retour d’expérience",
    "text": "Retour d’expérience\n\nRéponse aux attentes\n          Au cours du projet, nous avons été confrontés à plusieurs défis. En effet, au départ, les attentes concernant les questions et le rendu n’étaient pas totalement claires. Toutefois, grâce aux explications de notre tutrice Manon EVAIN, les choses se sont rapidement clarifiées. Elle nous a notamment encouragé à structurer notre code en classes, à les tester rigoureusement, et surtout à implémenter nous-mêmes le modèle d’apprentissage automatique.\nCes recommandations, combinées à nos efforts, nous ont poussés à aller au-delà des exigences initiales : nous avons ainsi modularisé l’ensemble du projet, afin d’en faciliter la maintenabilité et la réutilisabilité.\nConcernant les réponses aux questions, il a fallu faire preuve de minutie et de rigueur, notamment pour identifier les observations manquantes susceptibles de fausser les résultats par rapport aux données officielles disponibles en ligne. À la fin du projet, nous avons aussi dû gérer un problème de changement des noms d’équipes dans le temps. Pour y remédier rapidement, nous avons développé une fonction spécifique, évitant ainsi des erreurs dans nos analyses.\nEnfin, la mise en œuvre manuelle du modèle d’apprentissage automatique a été particulièrement bénéfique. Elle nous a permis non seulement de renforcer nos compétences en statistiques, mais aussi de mieux comprendre leur implémentation informatique.\n\n\nTravail en équipe\nDès le début, nous avons défini les différentes questions, puis nous les avons réparties équitablement, chacun répondant en moyenne à trois questions. L’utilisation de GitHub a grandement facilité le travail collaboratif, notamment pour surmonter les contraintes de distance géographique.\nPar ailleurs, lors de nos séances de travail en autonomie, nous avons veillé à partager nos méthodes respectives, afin que chacun puisse comprendre et valider les approches adoptées par les autres. Cette coopération constante nous a permis de rester alignés tout au long du projet.\n\n\nSatisfaction du travail final\n      Nous sommes satisfaits du travail accompli, bien que certains aspects puissent être améliorés. Le projet a demandé un fort investissement, mais a suscité un réel intérêt. Nous avons exploré de nouvelles compétences, notamment en développant une interface graphique. Cela nous a poussés à sortir de notre zone de confort, tout en veillant à la qualité du rendu. En somme, ce fut une belle opportunité d’apprentissage, tant sur le plan technique qu’en équipe."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#conclusion",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#conclusion",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "Conclusion",
    "text": "Conclusion\n      Ce projet s’inscrit pleinement dans le domaine de l’informatique appliquée au traitement de données. À travers l’analyse des données issues de la NBA, nous avons mis en oeuvre une chaîne complète de traitement : du nettoyage des données à leur exploration statistique, jusqu’à la mise en place d’un modèle de machine learning supervisé.\nNous avons ainsi développé une solution permettant de prédire la durée de carrière des joueurs à partir de variables explicatives pertinentes, en mobilisant des techniques issues de la régression linéaire et en appliquant des méthodes de validation croisée. Ce travail nous a permis de mieux comprendre les enjeux liés à la préparation des données (feature engineering, gestion des valeurs manquantes, encodage des variables catégorielles), étape cruciale en apprentissage automatique. Ces étapes nous ont également permis d’apporter des réponses correctes aux différentes questions posées.\nPar ailleurs, le projet a donné lieu à la création d’une application interactive en Python (framework Shiny for Python), offrant une visualisation dynamique des données et des résultats, et illustrant l’intérêt d’outils informatiques modernes pour valoriser les analyses.\nCe projet nous a permis de consolider nos compétences en programmation, en traitement de données et en machine learning, tout en approfondissant notre capacité à travailler en équipe sur un projet structuré, dans une logique proche des pratiques professionnelles en data science."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#liste-des-sigles-et-abréviations",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#liste-des-sigles-et-abréviations",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "Liste des sigles et abréviations",
    "text": "Liste des sigles et abréviations\n\nCSV : Comma-Separated Values\nFormat de fichier texte utilisé pour stocker des données tabulaires avec des virgules comme séparateurs.\nIA : Intelligence Artificielle\nEnsemble de techniques visant à simuler l’intelligence humaine par des machines.\nIMC : Indice de Masse Corporelle\nIndicateur utilisé pour évaluer la corpulence d’une personne à partir de son poids et de sa taille.\nJPG : Joint Photographic Experts Group\nFormat de fichier image compressé couramment utilisé.\nNBA : National Basketball Association\nLigue professionnelle de basketball nord-américaine.\nOLS : Ordinary Least Squares (Moindres Carrés Ordinaires)\nMéthode d’estimation en régression linéaire consistant à minimiser la somme des carrés des erreurs.\nPNG : Portable Network Graphics\nFormat d’image sans perte de qualité, adapté au web.\nRMSE : Root Mean Squared Error (Erreur Quadratique Moyenne Racine)\nMesure statistique de l’écart type des résidus de prédiction.\nUSA : États-Unis d’Amérique\nPays dont les franchises de la NBA sont majoritairement issues."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#sec:ml",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#sec:ml",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "Annexe 1 : Algorithme utilisé pour le machine learning Modèle de regression linéaire",
    "text": "Annexe 1 : Algorithme utilisé pour le machine learning Modèle de regression linéaire\n\n\n\n\n\nDiagramme de flux des méthodes de la classe LinearRegression"
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#sec:m2",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#sec:m2",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "Annexe 2 : Description des classes",
    "text": "Annexe 2 : Description des classes\nClasse Reponse\n      La méthode init() avant d’initialiser la classe Reponse doit :\n\nVérifier que l’argument data est bien un dictionnaire.\nVérifier que toutes les valeurs sont des objets de type pandas.DataFrame.\n\n      La méthode equip_victoires_defaites_saison() retourne une table contenant le nombre de victoires ou de défaites pour chaque équipe entre les saisons données.\n      La méthode prop_joueurs_en_nba_selon_universite_de_formation() retourne une table contenant le Top N des universités selon le nombre de leurs étudiants ayant évolué en NBA. Si graph = True, la méthode renvoie également un diagramme à barres.\n      La méthode stat_sur_taille_et_poids_par_poste() retourne une table contenant la statistique demandée sur la taille et le poids des joueurs selon leur poste.\n      La méthode equipe_remporte_au_moins_N_fois_le_titre() retourne une table listant les équipes ayant remporté au moins le nombre de titres requis.\n      La méthode premiers_choix_draft_N_derniere_saison() retourne une table contenant les premiers choix de draft sur les N dernières saisons.\n      La méthode vainqueur_titre_NBA_saisons() retourne une table contenant les vainqueurs du titre NBA sur la période choisie ou sur les saisons d’une période définie.\n      La méthode equipe_qui_remporte_N_fois_daffile_le_titre() retourne une table listant les équipes ayant remporté au moins N titres consécutifs.\n      La méthode nombre_victoires_ou_defaites_equipe_les_N_dernieres_saisons() retourne une table listant les équipes ayant obtenu le plus de victoires ou de défaites pour chaque saison d’une période donnée. Elle renvoie le nombre de défaites si defaite = True.\n      La méthode top_N_nb_joueurs_par_pays() retourne une table indiquant la répartition des joueurs NBA selon leur pays d’origine. Si graph = True, elle retourne également un diagramme à barres.\n      La méthode classement_conferences() retourne un dictionnaire contenant deux tables, un pour chaque conférence (Est et Ouest). Pour chaque table, on retrouve le classement des équipes selon leur nombre de victoires obtenues en saison régulière.\nClasse HomeFunction\n      La méthode init() initialise la classe avec un dictionnaire de DataFrames contenant les données à visualiser dans l’interface.\n      La méthode create_line_chart_nb_match() retourne un graphique représentant l’évolution du nombre de matchs joués par saison, sur une période donnée.\n      La méthode return_data_home() retourne les données principales affichées sur la page d’accueil dans l’interface.\n      La méthode create_dunut_chart_of_position_distribution() retourne un graphique en forme de donut chart illustrant la répartition des joueurs selon leur poste, pour une période donnée.\n      La méthode nombre_univ() retourne un entier représentant le nombre total d’universités ayant formé des joueurs NBA présent dans la base de données.\n      La méthode return_greatest_players() retourne une table des joueurs les plus marquants (statut de greatest) sur une période donnée, ainsi que le nombre de joueurs inclus.\n      La méthode return_nb_players() retourne un entier représentant le nombre total de joueurs dans la base.\n      La méthode return_nb_teams() retourne un entier représentant le nombre total d’équipes présentes dans les données.\nClasse ReponseFunction\n      La méthode init() initialise la classe avec un dictionnaire de DataFrames contenant les données nécessaires à l’affichage.\n      La méthode prop_university() retourne un diagramme représentant les universités ayant formé le plus de joueurs NBA, selon une période et un top N définis.\n      La méthode statistique_taille_poids() retourne une table contenant une statistique (moyenne, médiane, etc.) sur la taille ou le poids des joueurs NBA, utilisée pour une représentation tabulaire dans l’interface.\n      La méthode au_moins_N_fois_le_titre() retourne une table listant les équipes ayant remporté au moins un certain nombre de titres NBA sur une période donnée. Les données sont destinées à un affichage dans l’interface.\n      La méthode remporter_titre() retourne une table avec les équipes ayant remporté le titre NBA au cours de la période spécifiée. Affichage sous de tableau dans l’interface.\n      La méthode nombre_victoires_defaites() retourne une table indiquant l’équipe avec le plus de nombre de victoires ou de défaites sur la saison régulière, pour une période donnée.\n      La méthode numero_1_draft() retourne une table listant les joueurs sélectionnés en premier lors de la draft pour les nb dernières années. Idéal pour un tableau historique.\n      La méthode distribution_pays() retourne à la fois une table et un diagramme illustrant la répartition des joueurs NBA par pays d’origine, selon un Top N défini.\n      La méthode recup_min_max_date() retourne deux chaînes de caractères représentant la date de début et de fin d’une saison. Elle est utilisée pour ajuster dynamiquement les filtres temporels dans l’interface.\n      La méthode display_conference() retourne le classement des équipes dans les conférences Est et Ouest pour une saison donnée.\nClasse LinearRegression\n      La méthode init() : vérifie types, colonnes, seuil et intercept ; nettoie df ; crée X et y\n      La méthode create_x_y(): dichotomise les variables catégorielles ; ajoute un intercept si demandé\n      La méthode get_dummies() : transforme une variable catégorielle en colonnes indicatrices\n      La méthode fit() : estimation OLS par pseudoinverse ; avertit si multicolinéarité\n      La méthode fit_ridge() : estimation Ridge avec paramètre alpha ; intercept non pénalisé\n      La méthode compute_confidence_interval() : calcule les intervalles de confiance des coefficients\n      La méthode compute_rmse() : calcule la racine de l’erreur quadratique moyenne\n      La méthode predict() : génère les prédictions via X %*% Beta\n      La méthode perfom_linear_reg() : pipeline complet (fit, predict, IC, RMSE) en OLS ou Ridge\n      La méthode create_k_fold(I) : génère aléatoirement k sous-échantillons pour validation croisée\n      La méthode k_fold() : exécute la validation croisée k-fold et renvoie les ***RMSE}\nClasse CareerPrediction\n      La méthode init(data, use_ridge=FALSE, x_vars=list(), alpha=1.0, k=5) vérifie que data est un data.frame, initialise les attributs (use_ridge, alpha, k, etc.), clone les données et appelle prepare_data().\n      La méthode prepare_data() convertit draft_year et extrait birth_year de birthdate, remplace “undrafted” par 0, filtre les joueurs dont la carrière s’achève \\(\\leq\\) 2022 et garde ceux dont age_at_draft &gt; 0. Elle définit x_vars = c(“age_at_draft”, “position”) et instancie un modèle LinearRegression.\n      La méthode run_regression() reconstruit le modèle LinearRegression avec y_var = “season_exp” et x_vars = c(“age_at_draft”,“position”). Elle Lance perfom_linear_reg(use_ridge, alpha), renvoie les résultats (coefficients, IC, RMSE).\n      La méthode plot_k_fold() exécute k_fold(k) pour obtenir les RMSE} par fold, puis crée un graphique matplotlib*** et retourne l’objet figure.\n      La méthode predict_career_duration(birthdate, draft_year, position, coef_df) calcule age-at-draft, construit le vecteur de features (dont les dummy variables de position), récupère l’intercept et les coefficients estimés dans coef_df. Elle agrège contributions de chaque variable pour produire duree_predite et son intervalle_confiance.\nClasse ExportFiles\n      La méthode init() initialise l’exporteur (aucun paramètre requis)\n      La méthode normalize_path(path) normalise un chemin en remplaçant les backslashes par des slashs.\n      La méthode export_to_csv_format(table, path) vérifie que table est un DataFrame ou Series et que path est une chaîne, normalise le chemin et exporte la table en ***CSV} (sans index)\n      La méthode export_to_xlsx_format(table, path) effectue les mêmes contrôles de type que pour le CSV, convertit une Series en DataFrame si nécessaire et exporte au format Excel .xlsx\n      La méthode export_to_jpg(img, path) vérifie que img est une matplotlib.figure.Figure et que path est une chaîne, normalise le chemin et enregistre la figure au format JPG\n      La méthode export_to_png(img, path) effectue mêmes contrôles que pour JPG normalise le chemin et enregistre la figure au format PNG\n\nAnnexes 3 : Lien visuel éventuel entre le poste occupé sur le terrain, la taille et le poids des joueurs\n\n\n\n\n\nTaille et poids médians par poste en NBA\n\n\n\n\n      Sur cette figure, on peut voir que les ailiers forts/pivots, pivots et les pivots/ailiers forts sont les plus lourds et les plus grands en médiane. Ce qui traduirait une corrélation intrinsèque entre le poste, la taille et le poids des joueurs."
  },
  {
    "objectID": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#footnotes",
    "href": "projet-traitement-donnees/report_writing/synthese-des-travaux.html#footnotes",
    "title": "Prédiction de la durée de carrière des joueurs NBA sur la base de leurs caractéristiques et parcours individuels",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLa validation croisée est une technique permettant d’évaluer la performance d’un modèle en le testant sur plusieurs sous-ensembles des données. Elle permet de mieux estimer la généralisation du modèle (Run, Cévaër, and Dubé 2023).↩︎\nUn joueur Greatest est un joueur faisant partie de la sélection des 75 meilleurs joueurs de l’histoire de la NBA, désignée à l’occasion du 75e anniversaire de la ligue.↩︎\nL’erreur quadratique moyenne (Root Mean Squared Error ou RMSE) est définie comme la racine carrée de la moyenne des carrés des écarts entre les valeurs prédites \\(\\hat{y}_i\\) et les valeurs réelles \\(y_i\\) : \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2}\n\\] Plus la RMSE est faible, plus le modèle est précis.↩︎"
  },
  {
    "objectID": "pub.html",
    "href": "pub.html",
    "title": "DJAMAL TOE WEBSITE",
    "section": "",
    "text": "Modélisation Statistique & Machine Learning\n\n\n\nPython\n\n\nR\n\n\nStatistiques\n\n\nModélisation statistiques\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nClustering\n\n\nEDA\n\n\nReduction de dimensionalité\n\n\n\nÉtudes appliquées aux domaines de la santé, de la finance et du sport à travers des techniques statistiques et de machine learning interprétables : régression, clustering, réduction de dimension.\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgrammation & Projets Interactifs\n\n\n\nJava\n\n\nPython\n\n\nMySql\n\n\nAPI vocales\n\n\nJavaFX\n\n\nProgrammation orientée objet\n\n\n\nDéveloppement d’applications interactives (bureaux, vocales, connectées aux bases de données) en Java et Python, avec une attention à la modularité et à l’expérience utilisateur.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyse exploratoire & Visualisation\n\n\n\nR\n\n\nPython\n\n\nLeaflet\n\n\nsf\n\n\ntmap\n\n\nGeopandas\n\n\nFolium\n\n\nggplot2\n\n\nmatplotlib\n\n\nseaborn\n\n\n\nReprésentation visuelle de données complexes via des cartes et graphiques interactifs, notamment en santé publique et socio-économie, avec R et Python.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/programming/index.html",
    "href": "publications/programming/index.html",
    "title": "Programmation & Projets Interactifs",
    "section": "",
    "text": "Cette page rassemble mes projets interactifs ou orientés développement logiciel.\nOn y retrouve des outils pratiques comme : - des assistants vocaux intelligents, - des applications Java de gestion, - ou encore des interfaces interactives.\nCliquez sur la publication que vous souhaitez consulter"
  },
  {
    "objectID": "publications/programming/index.html#programmation-projets-interactifs",
    "href": "publications/programming/index.html#programmation-projets-interactifs",
    "title": "Programmation & Projets Interactifs",
    "section": "",
    "text": "Cette page rassemble mes projets interactifs ou orientés développement logiciel.\nOn y retrouve des outils pratiques comme : - des assistants vocaux intelligents, - des applications Java de gestion, - ou encore des interfaces interactives.\nCliquez sur la publication que vous souhaitez consulter"
  },
  {
    "objectID": "publications/stat_ml/index.html",
    "href": "publications/stat_ml/index.html",
    "title": "Modélisation Statistique & Machine Learning",
    "section": "",
    "text": "Cette page regroupe mes projets axés sur la modélisation statistique, le machine learning, l’intelligence artificielle et les méthodes de classification et de prédiction.\nOn y retrouve des approches variées allant :\n\ndes régressions statistiques classiques (Poisson, binomial negatif, logistique),\nà des méthodes non supervisées (ACP, K-means, GMM),\nen passant par des réseaux de neurones convolutifs pour le traitement d’images,\nou encore des modèles hybrides pour la détection d’anomalies ou la prédiction.\n\nLes projets couvrent des domaines appliqués tels que la santé, le sport, la finance ou encore la surveillance épidémiologique (Cliquez sur la publication que vous souhaitez consulter)."
  },
  {
    "objectID": "publications/stat_ml/index.html#projets-de-modélisation-machine-learning",
    "href": "publications/stat_ml/index.html#projets-de-modélisation-machine-learning",
    "title": "Modélisation Statistique & Machine Learning",
    "section": "",
    "text": "Cette page regroupe mes projets axés sur la modélisation statistique, le machine learning, l’intelligence artificielle et les méthodes de classification et de prédiction.\nOn y retrouve des approches variées allant :\n\ndes régressions statistiques classiques (Poisson, binomial negatif, logistique),\nà des méthodes non supervisées (ACP, K-means, GMM),\nen passant par des réseaux de neurones convolutifs pour le traitement d’images,\nou encore des modèles hybrides pour la détection d’anomalies ou la prédiction.\n\nLes projets couvrent des domaines appliqués tels que la santé, le sport, la finance ou encore la surveillance épidémiologique (Cliquez sur la publication que vous souhaitez consulter)."
  },
  {
    "objectID": "publications/theorie/index.html",
    "href": "publications/theorie/index.html",
    "title": "Formations & Théorie",
    "section": "",
    "text": "Cette page rassemble des ressources pédagogiques axées sur la modélisation, le machine learning, les algorithmes, ou encore les bonnes pratiques de communication scientifique (Cliquez sur la publication que vous souhaitez consulter).\nVous y trouverez :\n\ndes explications théoriques avec mise en œuvre pas à pas,\ndes présentations dynamiques ou interactives,"
  },
  {
    "objectID": "publications/theorie/index.html#formations-théorie",
    "href": "publications/theorie/index.html#formations-théorie",
    "title": "Formations & Théorie",
    "section": "",
    "text": "Cette page rassemble des ressources pédagogiques axées sur la modélisation, le machine learning, les algorithmes, ou encore les bonnes pratiques de communication scientifique (Cliquez sur la publication que vous souhaitez consulter).\nVous y trouverez :\n\ndes explications théoriques avec mise en œuvre pas à pas,\ndes présentations dynamiques ou interactives,"
  },
  {
    "objectID": "publications/visualisation/index.html",
    "href": "publications/visualisation/index.html",
    "title": "Analyse exploratoire & Visualisation",
    "section": "",
    "text": "Cette page présente des projets axés sur l’exploration de données, les visualisations interactives, et l’usage d’outils cartographiques ou graphiques pour mieux comprendre les phénomènes observés (Cliquez sur la publication que vous souhaitez consulter)."
  },
  {
    "objectID": "publications/visualisation/index.html#analyse-exploratoire-visualisation",
    "href": "publications/visualisation/index.html#analyse-exploratoire-visualisation",
    "title": "Analyse exploratoire & Visualisation",
    "section": "",
    "text": "Cette page présente des projets axés sur l’exploration de données, les visualisations interactives, et l’usage d’outils cartographiques ou graphiques pour mieux comprendre les phénomènes observés (Cliquez sur la publication que vous souhaitez consulter)."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications - Djamal TOE",
    "section": "",
    "text": "Bienvenue sur ma page dédiée à mes publications en statistiques, data science et informatique. Vous y trouverez mes articles, projets, analyses et supports pédagogiques classés par thème.\n\n\n\n\nDiagnostic tumeurs cérébrales avec un réseau de neurones\nUtilisation d’EfficientNetB5 pour classer les IRM de tumeurs cérébrales en trois catégories à l’aide de réseaux de neurones convolutifs.\nRéduction de dimensionnalité et clustering\nApplication de l’Analyse en Composantes Principales (ACP) et du K-means pour segmenter les pays selon des indicateurs socio-économiques.\nClassification médicale par ACP, KNN, logit\nComparaison de méthodes de classification (ACP, KNN, logit) pour détecter les tumeurs mammaires bénignes ou malignes.\nPrédire la durée de carrière NBA\nRégression supervisée pour prédire la longévité de la carrière des joueurs NBA à partir de leurs premières performances.\nDétection d’anomalies transactionnelles avec GMM\nUtilisation des mélanges de Gaussiennes (GMM) pour identifier les comportements suspects dans des transactions financières.\nPrédiction du diabète chez les femmes\nRégression logistique sur le dataset Pima Indians pour estimer le risque de diabète en fonction de variables cliniques.\nImpact d’une intervention contre le paludisme\nModélisation par régression de Poisson pour mesurer l’effet d’une intervention sur l’évolution hebdomadaire des cas de paludisme.\nClassification de gestes avec CNN & YOLOv8\nDétection et reconnaissance de gestes “Pierre-Feuille-Ciseaux” en temps réel via des modèles CNN et YOLOv8.\n\n\n\n\n\n\nAssistant vocal en Python\nCréation d’un assistant vocal intelligent combinant reconnaissance vocale, traduction et synthèse vocale.\nKedjeBoost – App Desktop Java/MySQL\nDéveloppement d’une application JavaFX de gestion de boutique avec base de données MySQL et génération de rapports Excel et pdf.\n\n\n\n\n\n\nCartes thématiques avec R\nCréation de cartes interactives avec tmap, leaflet et sf pour visualiser des données géographiques de santé publique.\n\n\n\n\n\n\nRégression linéaire et descente de gradient\nExplication théorique de la régression linéaire et implémentation de l’algorithme de descente de gradient pas à pas.\nPrésentations dynamiques avec R/Quarto\nTutoriel pour réaliser des présentations interactives en Quarto, Reveal.js et xaringan.\n\n\n\n\n\n\nFace Attendance System : MTCNN, FaceNet & Streamlit\nSystème de pointage par reconnaissance faciale avec détection en temps réel, encodage et interface web (publication prochaine).\n\n\n\n🔙 Retour à l’accueil"
  },
  {
    "objectID": "publications.html#modélisation-statistique-machine-learning",
    "href": "publications.html#modélisation-statistique-machine-learning",
    "title": "Publications - Djamal TOE",
    "section": "",
    "text": "Diagnostic tumeurs cérébrales avec un réseau de neurones\nUtilisation d’EfficientNetB5 pour classer les IRM de tumeurs cérébrales en trois catégories à l’aide de réseaux de neurones convolutifs.\nRéduction de dimensionnalité et clustering\nApplication de l’Analyse en Composantes Principales (ACP) et du K-means pour segmenter les pays selon des indicateurs socio-économiques.\nClassification médicale par ACP, KNN, logit\nComparaison de méthodes de classification (ACP, KNN, logit) pour détecter les tumeurs mammaires bénignes ou malignes.\nPrédire la durée de carrière NBA\nRégression supervisée pour prédire la longévité de la carrière des joueurs NBA à partir de leurs premières performances.\nDétection d’anomalies transactionnelles avec GMM\nUtilisation des mélanges de Gaussiennes (GMM) pour identifier les comportements suspects dans des transactions financières.\nPrédiction du diabète chez les femmes\nRégression logistique sur le dataset Pima Indians pour estimer le risque de diabète en fonction de variables cliniques.\nImpact d’une intervention contre le paludisme\nModélisation par régression de Poisson pour mesurer l’effet d’une intervention sur l’évolution hebdomadaire des cas de paludisme.\nClassification de gestes avec CNN & YOLOv8\nDétection et reconnaissance de gestes “Pierre-Feuille-Ciseaux” en temps réel via des modèles CNN et YOLOv8."
  },
  {
    "objectID": "publications.html#programmation-projets-interactifs",
    "href": "publications.html#programmation-projets-interactifs",
    "title": "Publications - Djamal TOE",
    "section": "",
    "text": "Assistant vocal en Python\nCréation d’un assistant vocal intelligent combinant reconnaissance vocale, traduction et synthèse vocale.\nKedjeBoost – App Desktop Java/MySQL\nDéveloppement d’une application JavaFX de gestion de boutique avec base de données MySQL et génération de rapports Excel et pdf."
  },
  {
    "objectID": "publications.html#analyse-exploratoire-visualisation",
    "href": "publications.html#analyse-exploratoire-visualisation",
    "title": "Publications - Djamal TOE",
    "section": "",
    "text": "Cartes thématiques avec R\nCréation de cartes interactives avec tmap, leaflet et sf pour visualiser des données géographiques de santé publique."
  },
  {
    "objectID": "publications.html#formations-théorie",
    "href": "publications.html#formations-théorie",
    "title": "Publications - Djamal TOE",
    "section": "",
    "text": "Régression linéaire et descente de gradient\nExplication théorique de la régression linéaire et implémentation de l’algorithme de descente de gradient pas à pas.\nPrésentations dynamiques avec R/Quarto\nTutoriel pour réaliser des présentations interactives en Quarto, Reveal.js et xaringan."
  },
  {
    "objectID": "publications.html#publications-à-venir",
    "href": "publications.html#publications-à-venir",
    "title": "Publications - Djamal TOE",
    "section": "",
    "text": "Face Attendance System : MTCNN, FaceNet & Streamlit\nSystème de pointage par reconnaissance faciale avec détection en temps réel, encodage et interface web (publication prochaine).\n\n\n\n🔙 Retour à l’accueil"
  }
]