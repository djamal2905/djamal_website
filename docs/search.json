[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "",
    "text": "Bienvenue sur mon site personnel.\nJe suis Djamal Y. TOE, statisticien passionné par les analyses avancées, la visualisation de données, et la résolution de problèmes complexes à travers des approches quantitatives. Ce site présente mes projets, mes recherches, et mes contributions dans le domaine des statistiques et de la science des données. Je suis titulaire d’une licence professionnelle en statistiques-informatique et actuellement élève ingénieur en Data Science à l’Ecole Nationale de la statistique et de l’Analyse de l’Information à Bruz Rennes, France."
  },
  {
    "objectID": "about.html#à-propos-de-moi",
    "href": "about.html#à-propos-de-moi",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "À propos de moi",
    "text": "À propos de moi\nJe combine mes compétences en statistiques, programmation et analyse de données pour transformer des ensembles de données en informations exploitables. Mon objectif est d’améliorer la prise de décision grâce à des modèles et des méthodes robustes. Ayant effectuer des stages en entreprises, j’ai appris beaucoup de choses notamment en bio-statistiques et sur les modélisations qui y sont utilisées. J’ai également des connaissance en cartographie (avec R)."
  },
  {
    "objectID": "about.html#expérience",
    "href": "about.html#expérience",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "Expérience",
    "text": "Expérience\n\nStagiaire au Centre de Méthodologie et de Gestion de données au Centre MURAZ sis Bobo-Dioulasso, Burkina Faso & l’Institut National de recherche en Science de la Santé sis Bobo-Dioulasso, Burkina Faso\n\ndurée : 10 mois Fin Juillet 2023 - Fin Avril 2024\nTravail effectué :\n\nAnalyse exploratoire de données\nTests statistiques et Modélisations\nSystème d’information géographique\nRedaction automatique de rapports\nTravail en equipe sur le projet d’aide des personnes agées (MAAKOROBA)\nGestion des analyses de routines du service (ACP, AFC, ACM, Regressions)"
  },
  {
    "objectID": "about.html#projets-personnels",
    "href": "about.html#projets-personnels",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "Projets personnels",
    "text": "Projets personnels\n\n1. Analyses Factorielles et Visualisations\n\nAnalyses factorielles (ACP, AFC, ACM, AFM, AFD) pour comprendre les structures complexes des données.\nVisualisation interactive des résultats pour une meilleure interprétation.\n\n\n\n2. Modèles de Régression et Prévision\n\nRégression linéaire, logistique, et mixte\nPrévisions à l’aide de modèles de séries temporelles (pas trop avancé)\n\n\n\n3. Applications Statistiques\n\nDéveloppement d’outils interactifs pour l’analyse de données (Shiny, Quarto)\nRapports automatisés (Rmarkdown, Bookdown)\n\n\n\n4. Applications Bureau et Web\n\nDévéloppement de logiciel bureau pour la gestion des caisses\nDévéloppement de sites web avec python&Django (pas trop avancé)\n\n\n\n5. Computer vision\nJe débute dans la vision par ordinateur avec :\n\nLa SVM (Support Vector Machine)\nLe KNN (K- Nearest Neighbour)\nL’ACP (L’Analyse en Composante Principale)\nLes reseaux de neurones convolutionnels (en cours d’apprentissage)\n\n\n\n6. Langages de programmtion et outils statistiques\n\nPython, Java, C++ & C\nR, Stata, SPSS (Moyen)\nHtml, Css\nOffice et Suites\nSystème de Gestion de données :\n\nMySql\nOracle SQL"
  },
  {
    "objectID": "about.html#dernières-publications",
    "href": "about.html#dernières-publications",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "Dernières Publications",
    "text": "Dernières Publications\n\nExploration des techniques d’analyse factorielle\nConseils pour réaliser des présentations avec R et Rstudio\nTutoriel sur la réalisation de cartes de proportions et de cartes choroplèthes avec R\nModélisation des données de comptages : Evaluation de l’impact d’une intervention sur le nombre de cas de paludisme\nCrée ton assistant virtuel avec commandes vocalesen python\nFormation en python"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html",
    "href": "ANALYSES_FACTORIELLES/TP03.html",
    "title": "Djamaldbz - Méthodes d’Analyse factorielle TP02",
    "section": "",
    "text": "packages_ &lt;- c(\"ggplot2\", \"dplyr\",\"readxl\",\"cowplot\")\n\nfor (pkg in packages_) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#quelques-définitions",
    "href": "ANALYSES_FACTORIELLES/TP03.html#quelques-définitions",
    "title": "Djamaldbz - Méthodes d’Analyse factorielle TP02",
    "section": "Quelques définitions",
    "text": "Quelques définitions\n      Le calcul de l’empreinte écologique et de la biocapacité nous aide à répondre à la question de recherche fondamentale : Quelle est la demande des êtres humains envers les surfaces biologiquement productives (empreinte écologique) par rapport à la quantité que la planète (ou la surface productive d’une région) peut régénérer sur ces surfaces (biocapacité) ?\n\nHectare global (gha) : C’est l’unité choisie pour exprimer toutes les quantités d’intérêt concernant la consommation/émission de carbone. Une unité de surface correspondant à la productivité moyenne d’un hectare de terres mondiales. Un hectare de terres agricoles vaudra plus d’hectares globaux qu’un hectare de désert.\nEmpreinte écologique (en gha par personne) : Le nombre de gha requis pour produire les besoins et absorber les déchets d’un pays.\nBiocapacité (en gha) : La capacité d’un pays à produire ce dont il a besoin et à absorber ses déchets (réserve écologique).\nJour de dépassement : Jour de l’année où la demande d’un pays dépasse sa biocapacité annuelle."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#chargement-des-données",
    "href": "ANALYSES_FACTORIELLES/TP03.html#chargement-des-données",
    "title": "Djamaldbz - Méthodes d’Analyse factorielle TP02",
    "section": "Chargement des données",
    "text": "Chargement des données\n\n##-- Installer et Charger les packages requis\n###--- vecteurs des packages\npackages &lt;- c(\"factoextra\", \"corrr\", \"FactoMineR\", \"dplyr\",\"kableExtra\",\"corrplot\",\n              \"explor\")\n\n###--- Boucle pour installer et charger les packages\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}\n\n##-- charger la base de données via le lien web\nlink.to.data &lt;- \"https://marieetienne.github.io/datasets/overshootday_overview.csv\"\ndf &lt;- read.csv(link.to.data)"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#analyse-exploratoire-des-données",
    "href": "ANALYSES_FACTORIELLES/TP03.html#analyse-exploratoire-des-données",
    "title": "Djamaldbz - Méthodes d’Analyse factorielle TP02",
    "section": "Analyse exploratoire des données",
    "text": "Analyse exploratoire des données\n\nnrow(df); ncol(df) ;dim(df)\n\n[1] 182\n\n\n[1] 13\n\n\n[1] 182  13\n\n\nLes données sont composées de 182 lignes et de 13 colonnes."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#résumé-statitique-des-variables",
    "href": "ANALYSES_FACTORIELLES/TP03.html#résumé-statitique-des-variables",
    "title": "Djamaldbz - Méthodes d’Analyse factorielle TP02",
    "section": "Résumé statitique des variables",
    "text": "Résumé statitique des variables\nOn utilise la commande summary(df) tout simplement, mais pour une question d’exthétique on utilise ce code.\n\n##-- summary pour les variable numériques\nsummary.df.num &lt;- sapply(df[sapply(df, is.numeric)], function(x) {\n  c(\n    min = min(x, na.rm = TRUE),\n    Q1 = quantile(x, 0.25, na.rm = TRUE),\n    Q3 = quantile(x, 0.75, na.rm = TRUE),\n    med = quantile(x, 0.5, na.rm = TRUE),\n    mean = mean(x, na.rm = TRUE),\n    max = max(x, na.rm = TRUE),\n    count = sum(!is.na(x)),\n    sd = sd(x, na.rm = TRUE),\n    `NA's` = round(sum(is.na(x)),0)\n  )\n})\nsummary.df.num &lt;- as.data.frame(summary.df.num)\n\nEnsuite nous affichons ce resumé dans un tableau :\n\n\n\nTableau 1 : Résumé statistique des variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlife_expectancy\nhdi\nper_capita_gdp\npop\ntotal_prod\ntotal_cons\nbiocapacity\nnumber_of_countries_required\nnumber_of_earths_required\novershoot_day\n\n\n\n\nmin\n52,525000\n0,3850000\n732,836\n0,06200\n0,371747\n0,5540298\n0,1041268\n0,0180633\n0,3668548\n41,0000\n\n\nQ1.25%\n65,747000\n0,5945000\n4888,255\n2,64100\n1,156834\n1,2195240\n0,6633750\n0,8273357\n0,8075166\n143,0000\n\n\nQ3.75%\n76,400695\n0,8350000\n31670,000\n32,91550\n3,828778\n3,8418335\n2,6656718\n2,7330613\n2,5438978\n365,0000\n\n\nmed.50%\n71,900000\n0,7310000\n13548,200\n10,01950\n1,924223\n2,3197815\n1,3622344\n1,7280656\n1,5360601\n239,0000\n\n\nmean\n71,180320\n0,7177193\n21139,464\n43,47636\n2,879469\n2,9624675\n3,5569055\n2,9127705\n1,9616192\n239,7802\n\n\nmax\n84,445610\n0,9620000\n120505,000\n1480,63200\n13,394536\n13,1263342\n85,6461100\n55,1061868\n8,6916969\n365,0000\n\n\ncount\n175,000000\n171,0000000\n163,000\n182,00000\n182,000000\n181,0000000\n181,0000000\n181,0000000\n181,0000000\n182,0000\n\n\nsd\n7,615465\n0,1533110\n22330,819\n156,03751\n2,515235\n2,1957327\n10,0256869\n5,1916277\n1,4539202\n109,5507\n\n\nNA’s\n7,000000\n11,0000000\n19,000\n0,00000\n0,000000\n1,0000000\n1,0000000\n1,0000000\n1,0000000\n0,0000\n\n\n\nNote: aby Djamal Y. TOE\n\n\n  Nous constatons que ceraines variables ont des données manquantes, nous pouvons décider de soit les supprimer, soit les prédire avec des méthodes d’imputation en fonction de leurs importances. Mais pour le moment nous allons juste les supprimer.\n\ndf &lt;- na.omit(df)\nnrow(df)\n\n[1] 162\n\n\nAinsi nous passons de 182 à 162 lignes."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#contruction-de-lanalyse-en-composante-principale",
    "href": "ANALYSES_FACTORIELLES/TP03.html#contruction-de-lanalyse-en-composante-principale",
    "title": "Djamaldbz - Méthodes d’Analyse factorielle TP02",
    "section": "Contruction de l’Analyse en composante principale",
    "text": "Contruction de l’Analyse en composante principale\n\nLe poids pour les pays : Les tailles respectives des populations de chaques pays car cela garantit que l’analyse est représentative des différences globales, en tenant compte de l’impact démographique des pays.\nMétrique : Normalisation des données car les variables ne sont pas toutes sur la même échelle. Cela permet d’éviter que les variables avec de grosses valeurs (grandes échelles) dominent l’analyse.\nvariables sup :\n\nQuali sup : region, income_group\nQuanti sup : pop\n\n\n\nRéalisation de l’ACP\n\nVérifions la corrélations entre les variables quantitatives\n\n\nnumeric.vars &lt;- as.data.frame(df[sapply(df, is.numeric)])\nM &lt;- round(cor(numeric.vars),2) #- Calculer la matrice de corrélation\n\n##-- créer un objet qui contient une palette de couleur pour le gradiant dans le plot\ncol &lt;- colorRampPalette(c(\"#BB4444\", \"#EE9988\", \"#FFFFFF\", \"#77AADD\", \"#4477AA\"))\n\n##-- dessiner le graphique\ncorrplot(M, method=\"color\", col=col(200),  \n         type=\"upper\", order=\"hclust\", \n         addCoef.col = \"black\", #- ajout des coefficients correlation\n         tl.col=\"black\", tl.srt=45, tl.cex = 1\n         , #- couleur, rotation et police de texte des libellés \n         ##-- ne pas afficher les coefficients de corrélations sur la diagonale (ils valent tous 1)\n         diag=FALSE \n         ) \n\n\n\n\nFigure 1 : Matrice de corrélations\n\n\n\n\n    On voit qu’il y’ a quand même des variables qui sont &lt;&gt; (pour l’affirmer avec plus d’assurance il serait judicieux de faire un test billatéral de corrélation de Pearson avec la commande cor.test(method = “pearson”, alternative = “two.sided”)).\n\nCréation du modèle de l’ACP\n\n\ndata.pca &lt;- df[,-1] #- sélectionner toute les variables sauf la variable pays\nrownames(data.pca) &lt;- df[,1] #- renommer les lignes avec les noms des pays (individus)\npoids &lt;- df$pop\npca.model &lt;- PCA(data.pca, scale.unit = TRUE,\n                 quali.sup = c(\"region\", \"income_group\"),\n                 graph = FALSE,\n                 row.w = data.pca$pop,\n                 quanti.sup = 6)\n\n##- explor(pca.model) pour une interface interactive\n\n\n\nRecupération des valeurs propres et des variances\n\neigen.values &lt;- pca.model$eig\nknitr::kable(eigen.values[1:3,2:3], caption = capTab(\"Inerties expliquées par les 3 premiers axes\"))\n\n\nTableau 2 : Inerties expliquées par les 3 premiers axes\n\n\n\npercentage of variance\ncumulative percentage of variance\n\n\n\n\ncomp 1\n69,758338\n69,75834\n\n\ncomp 2\n16,485374\n86,24371\n\n\ncomp 3\n4,865912\n91,10962\n\n\n\n\n\nOn remarque que les axes 1,2 et 3 représentent respectivement 69,76, 16,49 et 4,09, donc au total 91,11\nOn pourrait aussi visualiser le graphique des valeurs propres :\n\nplt.eig &lt;- fviz_eig(pca.model, title = \"Valeurs propres avec Singapore\")\n\n\n\nQualité de representation des plans / sur les plans\n\nQualité de representation des plans\n\n  Le premier plan a un taux d’inertie supérieur à 86 %, il capte une grande partie de l’information présente dans les données ce qui signifie qu’il à une bonne qualité de representation alors que le second (1-3) en capte environ 74,63 % donc a une faible qualité de représenatation comparé au premier. En depit de ce fait, les deux plans ont quand même qualité de représentation si mous fions au critère du taux d’inertie.\n\nQualité de representation sur les plans\n\n(1-2)\n\n\nLES VARIABLES\n\ngraph.cos2.var &lt;- fviz_pca_var(pca.model,col.var=\"cos2\", gradient.cols=c(\"#F1C40F\",\"#2ECC71\",\"#8E44AD\"), repel=TRUE, ggtheme = theme_light())\n\ngraph.cos2.var\n\n\n\n\nFigure 3 : Qualités de representation des variables\n\n\n\n\nConcernant les variables, on constate qu’elles toutes sont bien representées avec des cosinus carrés qui ont une valeur minimale environ 0,8 à part les variables biocapacity, life_expectancy, number_of_countries_required qui ont un cosinus carrés qui vaut environ 0,7.\nLES INDIVIDUS\n\nthreshold &lt;- 0.85\ndata.ind.cos2 &lt;- pca.model$ind$cos2\n\ndim1 &lt;- data.ind.cos2[,\"Dim.1\"]\ndim1 &lt;- dim1[dim1 &gt;= threshold]\ncountries.dim1 &lt;- names(dim1)\nnames(dim1) &lt;-  NULL\n\ndim2 &lt;- data.ind.cos2[,\"Dim.2\"]\ndim2 &lt;- dim2[dim2 &gt;= 0.6]\ncountries.dim2 &lt;- names(dim2)\nnames(dim2) &lt;-  NULL\n\n##-- crétion des dataframes \ndim1.df &lt;- data.frame(\n  Country = countries.dim1,\n  `Cos carré` = dim1\n) %&gt;% arrange(desc(dim1))\n\n\ndim2.df &lt;- data.frame(\n  Country = countries.dim2,\n  `Cos carré` = dim2\n) %&gt;% arrange(desc(dim2))\n\n\n##-- création des tableaux kableExtra\ndim1.tbl &lt;- kableExtra::kbl(dim1.df, caption = capTab(\"Individus ayant un cosinus carré supérieur ou égal à 0,85 sur l'axe 1\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;% add_footnote(label = \"Source des données :  https://marieetienne.github.io/datasets/overshootday_overview.csv\")\n\ndim2.tbl &lt;- kableExtra::kbl(dim2.df, caption = capTab(\"Individus ayant un cosinus carré supérieur ou égal à 0,6 sur l'axe 2\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))  %&gt;% add_footnote(label = \"Source des données :  https://marieetienne.github.io/datasets/overshootday_overview.csv\")\n\n\n\n\ndim1.tbl\n\n\n\nTableau 3 : Individus ayant un cosinus carré supérieur ou égal à 0,85 sur l'axe 1\n\n\nCountry\nCos.carré\n\n\n\n\nRwanda\n0,9810454\n\n\nNepal\n0,9772128\n\n\nHaiti\n0,9763067\n\n\nPakistan\n0,9745537\n\n\nSao Tome and Principe\n0,9617076\n\n\nIndia\n0,9558559\n\n\nKenya\n0,9448689\n\n\nTogo\n0,9425662\n\n\nMalawi\n0,9425394\n\n\nTanzania, United Republic of\n0,9419010\n\n\nEthiopia\n0,9390190\n\n\nGambia\n0,9377395\n\n\nPoland\n0,9272053\n\n\nYemen\n0,9228666\n\n\nCzech Republic\n0,9218239\n\n\nAustria\n0,9047663\n\n\nGuatemala\n0,9044971\n\n\nMyanmar\n0,8964483\n\n\nBurundi\n0,8916493\n\n\nCambodia\n0,8911437\n\n\nDenmark\n0,8896874\n\n\nUnited States of America\n0,8875048\n\n\nSlovenia\n0,8840522\n\n\nMalaysia\n0,8840507\n\n\nBenin\n0,8831711\n\n\nSudan\n0,8713134\n\n\nSenegal\n0,8705626\n\n\nTimor-Leste\n0,8640862\n\n\nBelgium\n0,8604596\n\n\nAngola\n0,8591836\n\n\nGhana\n0,8579621\n\n\nSierra Leone\n0,8542504\n\n\nSlovakia\n0,8524619\n\n\n\na Source des données : https://marieetienne.github.io/datasets/overshootday_overview.csv\n\n\n\n\n\n\n\n\n\n\n\n\ndim2.tbl \n\n\n\nTableau 4 : Individus ayant un cosinus carré supérieur ou égal à 0,6 sur l'axe 2\n\n\nCountry\nCos.carré\n\n\n\n\nNamibia\n0,7502317\n\n\nParaguay\n0,6807204\n\n\nBrazil\n0,6672407\n\n\nBolivia\n0,6609662\n\n\nBarbados\n0,6458843\n\n\n\na Source des données : https://marieetienne.github.io/datasets/overshootday_overview.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\nAXE 1 : On voit que les pays (individus) comme le Togo, le Yemen, les USA, le Rwanda sont tres bien representés. RMRQ : Il y en a d’autres\nAXE 2 : Il n’y a que 6 pays qui sont bien représentés sur cet axe. Il s’agit de la Namibie, le Paraguay, le Brésil, la Bolivie et Barbados.\n\nREMARQUE :  Pour le plan formé des axes 1 et 3, on peut procéder la même que celle en amont\n\n\nCaractérisation des axes\n\ngraph.contrib.var &lt;- fviz_pca_var(pca.model,col.var=\"contrib\", gradient.cols=c(\"#F1C40F\",\"#2ECC71\",\"#8E44AD\"), repel=TRUE, ggtheme = theme_light())\n\ngraph.contrib.var\n\n\n\n\nFigure 4 : Cercle de corrélation des variables et leur contribution à la formation des axes\n\n\n\n\n\n\nComment l’ACP est-elle modifiée si on retire Singapour de l’analyse ?\n\ndata.pca.sans.singapore &lt;- data.pca %&gt;% filter(rownames(data.pca) != \"Singapore\")\npoids &lt;- df$pop\npca.model.sans.singapore &lt;- PCA(data.pca.sans.singapore, scale.unit = TRUE,\n                 quali.sup = c(\"region\", \"income_group\"),\n                 graph = FALSE,\n                 row.w = data.pca.sans.singapore$pop,\n                 quanti.sup = 6)\n\n##-- explor(pca.model)\n\n\nplt.eig.sans.sing &lt;- fviz_eig(pca.model.sans.singapore, title = \"Valeurs propres sans Singapore\") \ncomp.eig &lt;-  cowplot::plot_grid(\n  plt.eig,\n  plt.eig.sans.sing,\n  ncol = 2\n)+ theme_light()\ncomp.eig\n\n\n\n\nFigure 5 : Comparaison des valeurs propres issues de l’ACP aevc et sans Singapore\n\n\n\n\n  On voit que rien ne se passe (pas de changement brusque) au niveau de la qualité des axes. Voyons de plus prêt ce qui se passe :\n\nplot.indiv.avec.sing &lt;- fviz_pca_ind(pca.model) + \n                        theme_light()\nplot.indiv.avec.sing\n\n\n\n\nFigure 6 : Comparaison des valeurs propres issues de l’ACP aevc et sans Singapore\n\n\n\n\n  On voit que Singapore est atypique. Cela pourrait signifier que Singapore participe fortement à la formation de l’axe 2 (point plus proche de l’axe 1).\n\ndata &lt;- as.data.frame(pca.model$ind$contrib)\ndata &lt;-  data %&gt;% arrange(desc(Dim.2)) %&gt;% head(10)\nkableExtra::kbl(data, caption = capTab(\"Contribution des individus à la formation des axes par contribution décroissante suivant l'axe 2\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))  %&gt;% add_footnote(label = \"Source des données :  https://marieetienne.github.io/datasets/overshootday_overview.csv\")\n\n\n\nTableau 5 : Contribution des individus à la formation des axes par contribution décroissante suivant l'axe 2\n\n\n\nDim.1\nDim.2\nDim.3\nDim.4\nDim.5\n\n\n\n\nSingapore\n0,7259355\n15,139608\n28,5623680\n9,9266768\n5,6933884\n\n\nBrazil\n0,2363938\n11,748028\n0,2780798\n16,7474472\n0,1489935\n\n\nChina\n6,3338962\n10,346574\n0,9583694\n1,3599693\n36,9834265\n\n\nRussian Federation\n3,7284884\n10,223438\n4,1131775\n0,1010599\n3,8319553\n\n\nCanada\n3,7127169\n6,392768\n0,8335954\n3,8719645\n0,0267550\n\n\nJapan\n2,3347045\n4,109062\n0,8491595\n0,6848764\n2,2016647\n\n\nUnited States of America\n21,6581050\n3,434086\n3,0985377\n22,3391460\n5,4054986\n\n\nKorea, Republic of\n1,9711036\n2,758601\n0,8457859\n0,0962515\n0,0745840\n\n\nAustralia\n1,8672806\n2,568678\n0,0085414\n1,3778438\n0,3505079\n\n\nGuyana\n0,0605633\n2,548944\n1,0759061\n8,5175485\n0,9606747\n\n\n\na Source des données : https://marieetienne.github.io/datasets/overshootday_overview.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEt pourtant il contribue fortement à la formation de l’axe 2, il est même celui qui contribue les plus à la formation des axes. Le fait que Singapore contribue le plus à la formation des axes et que rien ne change lorsqu’il est retiré de l’analyse s’explique tout simplement par sa taille de population. En effet la taille de la population a été utilisée comme poids des individus qui sont ici les pays."
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#identifications-des-pays-en-fonction-de-leur-groupe-de-revenu",
    "href": "ANALYSES_FACTORIELLES/TP03.html#identifications-des-pays-en-fonction-de-leur-groupe-de-revenu",
    "title": "Djamaldbz - Méthodes d’Analyse factorielle TP02",
    "section": "Identifications des pays en fonction de leur groupe de revenu",
    "text": "Identifications des pays en fonction de leur groupe de revenu\n\nIl s’agit juste d’une parenthèse qui n’a rien avoir avec l’objectif de l’étude\n\n\n##-- Définitions des groupes de revenus\nincome_groups_definitions &lt;- c(\n  \"UM\" = \"Upper-Middle\",\n  \"LM\" = \"Lower-Middle\",\n  \"HI\" = \"High Income\",\n  \"LI\" = \"Low Income\"\n)\n\n\n##-- Ajouter une colonne avec les définitions correspondantes\ndata.pca$income_group_def &lt;- as.factor(income_groups_definitions[data.pca$income_group])\n\n\ngraph_indiv &lt;- fviz_pca_ind(\n  pca.model,\n  select.ind = list(\n    contrib = 50\n  ),\n  invisible = c(\"quanti.sup\",\"ind.sup\"),\n  habillage = data.pca$income_group_def,\n  addEllipses = TRUE,\n  repel = TRUE,\n) + theme_light() \n\n\ngraph_indiv\n\n\n\n\nFigure 7 : Affichage des 40 individus qui contribuent le plus à la formation des axes en fonction de leur groupe de revenu\n\n\n\n#hc.pca &lt;- HCPC(pca.model, nb.clust=3)\n\n      On voit que les groupes ne sont pas bien séparés, raison pour laquelle les ellipses ont des partie qui coïncident. Cela pourrait signifier que les les groupes de revenus sont trop similaires pour etre clairement séparés sur les axes sélectionnés (dans le plan des composantes principales). Cela pourrait aussi fait cas d’hétérogénéité, c’est-à-dire que les groupes ne sont pas homogènes (grande variabilité intra-groupe).\n      A bien regarder, nous aurions pu les regrouper en trois groupes de revenu, en combinant les Low income et les Low middle income, les Upper middle income (avec certains pays du High income) et enfin le dernier groupe les high income. Il faut noter que tout ça n’est que purement visuel même si on a quand même une grande partie de l’information contenue dans les données rien qu’avec ces deux plans (plus de 80%).\n\nDeux ACP différentes\n\nPourquoi réaliser deux ACP différentes ?\n\n  Pour simplement calculer la 1-ère valeur propre de chaque groupe de variables (empreinte écologique et de developpement) afin de les utiliser ponderer les variables afin qu’elles contribuent de manière équitable à la formation des axes. Pour plus de détails aller à la sous-section et sur le site de mon professeur de Méthodes d’Analyses Factorielles en cliquanr sur ce lien https://marieetienne.github.io/MAF/01_afm.html#/title-slide.\nOn préfère utiliser la première valeur propre (\\(\\lambda_{k1}\\)) car elle capturerait l’essentiel de l’inertie d’un groupe et permet une pondération cohérente et équilibrée dans l’AFM. La seconde valeur propre reflète des structures secondaires ou résiduelles qui ne sont pas pertinentes pour normaliser les contributions des groupes dans l’analyse globale.\n\nvariables.empreinte &lt;- df[, c(\"total_prod\", \"total_cons\", \"biocapacity\", \"number_of_earths_required\", \"overshoot_day\", \"pop\")]\nrownames(variables.empreinte) &lt;- df$country\nvariables.developpement &lt;- df[, c(\"life_expectancy\", \"hdi\", \"per_capita_gdp\",\"pop\")]\nrownames(variables.developpement) &lt;- df$country\n\n\nACP sur les variables d’empruntes écologiques\n      Il s’agit ici de faire l’ACP que sur les variables d’empruntes écologiques et de mettre les autres variables (de developpement) en quantitatives supplémentaires.\n\ndata.pca &lt;- df[,-1] ## sélectionner toute les variables sauf la variable pays\nrownames(data.pca) &lt;- df[,1] ## renommer les lignes avec les noms des pays (individus)\npoids &lt;- df$pop\nacp_empreinte &lt;- PCA(data.pca, scale.unit = TRUE,\n                 quali.sup = c(\"region\", \"income_group\"),\n                 graph = FALSE,\n                 row.w = data.pca$pop,\n                 quanti.sup = c(6,1,2,3))\n\n##-- 1ere valeur propre\nacp_empreinte$eig[1,1]\n\n[1] 4,115324\n\n\nLa première valeur propre est : 4,12\n\n\nACP sur les variables d’empruntes écologiques\n\nacp.developpement &lt;- PCA(data.pca, scale.unit = TRUE,\n                 quali.sup = c(\"region\", \"income_group\"),\n                 graph = FALSE,\n                 row.w = data.pca$pop,\n                 quanti.sup = 6:12)\n\n##-- 1ere valeur propre\nacp.developpement$eig[1,1]\n\n[1] 2,592046\n\n\nLa première valeur propre est : 2,59\n\n\nRéalisons l’AFM manuellement\n\nvariables.empreinte.pond &lt;- variables.empreinte[,-ncol(variables.empreinte)]/sqrt(acp_empreinte$eig[1,1])\n\nvariables.developpement.pond &lt;- variables.developpement[,-ncol(variables.developpement)]/sqrt(\n  acp.developpement$eig[1,1]\n)\n\nvariables.empreinte.pond$group &lt;- \"Empreinte écologique\"\nvariables.developpement.pond$group &lt;- \"developpement\"\n\ndf.afm &lt;- cbind(variables.empreinte.pond, \n                variables.developpement.pond,\n                pop = df$pop,\n                region = df$region,\n                income_group = df$income_group)\n\nacp.afm &lt;- PCA(df.afm, scale.unit = TRUE,\n                 quali.sup = c(\"region\", \"income_group\", \"group\"),\n                 graph = FALSE,\n                 row.w = df.afm$pop,\n                 quanti.sup = 9)\n\nvariance.cum.val.prop.2acp &lt;- acp.afm$eig[, c(1,3)]\ncolnames(variance.cum.val.prop.2acp) &lt;- c(\"Valeur propres\", \"Pourcentage de variance cumulée\")\n\n\nkableExtra::kbl(variance.cum.val.prop.2acp, caption = capTab(\"Valeurs propres et variances cumulées de chaque axes issues d'une AFM manuelle\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;% add_footnote(label = \"Source des données :  https://marieetienne.github.io/datasets/overshootday_overview.csv\")\n\n\n\nTableau 6 : Valeurs propres et variances cumulées de chaque axes issues d'une AFM manuelle\n\n\n\nValeur propres\nPourcentage de variance cumulée\n\n\n\n\ncomp 1\n5,4257689\n67,82211\n\n\ncomp 2\n1,3223267\n84,35120\n\n\ncomp 3\n0,6522681\n92,50455\n\n\ncomp 4\n0,3973880\n97,47190\n\n\ncomp 5\n0,1005256\n98,72847\n\n\ncomp 6\n0,0689548\n99,59040\n\n\ncomp 7\n0,0327679\n100,00000\n\n\ncomp 8\n0,0000000\n100,00000\n\n\n\na Source des données : https://marieetienne.github.io/datasets/overshootday_overview.csv"
  },
  {
    "objectID": "ANALYSES_FACTORIELLES/TP03.html#afm",
    "href": "ANALYSES_FACTORIELLES/TP03.html#afm",
    "title": "Djamaldbz - Méthodes d’Analyse factorielle TP02",
    "section": "REALISATION DE L’AFM",
    "text": "REALISATION DE L’AFM\n\nPourquoi réaliser une AFM au lieu d’une ACP tout court?\n\n      L’Analyse Factorielle Multiple (AFM) permet d’aller au-delà des limites d’une Analyse en Composantes Principales (ACP) classique, particulièrement lorsque les variables d’un jeu de données ne sont pas à la même échelle ou lorsqu’elles sont organisées en groupes. La normalisation dans l’ACP sert à ramener toutes les variables à une même échelle, évitant ainsi que certaines variables dominent artificiellement l’analyse en raison de leur variance plus élevée. Par exemple d’autres ont une contribution élevée que d’autres alors que c’est juste l’unité de mésure qui pèse plus.\n  Cependant, cette normalisation n’est pas suffisante lorsque les variables sont regroupées par thématique ou nature. Par exemple, supposons un jeu de données contenant \\(n\\) variables, parmi lesquelles \\(n - k\\) \\(\\text{avec k telque  } \\forall \\text{ j} \\neq \\text{k, }\\)\n\\(\\text{n - k} &gt; \\text{n - j où n - j est le nombre de variables dans tous les autres groupes ou dans un autre groupe j}\\) appartiennent à un groupe \\(i\\) .Dans ce cas, le groupe \\(i\\) peut influencer de manière disproportionnée les résultats de l’ACP, simplement en raison de la taille du groupe. Cela signifie que, même après normalisation, le poids collectif du groupe \\(i\\) dans la construction des composantes principales pourrait être trop important par rapport aux autres groupes.\n  L’AFM résout ce problème en intégrant un poids équilibré entre les groupes. Elle considère chaque groupe comme une entité, indépendamment du nombre de variables qu’il contient. Cela permet une contribution équitable des groupes aux axes factoriels. Par conséquent, l’AFM est particulièrement adaptée dans des contextes où les variables appartiennent à des thématiques distinctes (par exemple, des groupes liés à des disciplines différentes : santé, économie, environnement).\nIl est crucial de préserver l’équilibre des contributions entre ces thématiques pour éviter les biais d’interprétation. Ainsi, l’AFM fournit une perspective multidimensionnelle plus équilibrée et pertinente pour analyser des jeux de données complexes, tout en respectant la structure inhérente des variables\n\nRéalisons l’AFM à présent\n\n\n#-- création de la table pour l'AFM. Les vriables doivent être rangées \n#-- suivant le groupe (variables du groupe 1 ensuite celles du groupe 2 ...)\ndata.afm &lt;- data.pca %&gt;%\n  select(\n    life_expectancy, hdi, per_capita_gdp,  ##-- Variables de developpement\n    total_prod, total_cons, biocapacity, ##------ Variables\n    number_of_earths_required, overshoot_day ##-- d'empreinte écologique\n)\n\nmodel.afm &lt;- MFA(\n    data.afm, \n    group = c(5, 3), ##-- Spécifie le nombre de variables dans chaque groupe\n    type = rep(\"s\", 2), ##-- Indique que les variables doivent être normalisées pour chaque groupe\n    name.group = c(\"Developpement\", \"Empreinte ecologique\"), ##-- Nommer les groupes\n    graph = F  ##-- Générer un graphique\n)\nvariance.cum.val.prop.afm &lt;- model.afm$eig[, c(1,3)]\ncolnames(variance.cum.val.prop.afm) &lt;- c(\"Valeur propres\", \"Pourcentage de variance cumulée\")\n\n\nkableExtra::kbl(variance.cum.val.prop.afm, caption = capTab(\"Valeurs propres et variances cumulées de chaque axes issues d'une AFM avec R\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;% add_footnote(label = \"Source des données :  https://marieetienne.github.io/datasets/overshootday_overview.csv\")\n\n\n\nTableau 7 : Valeurs propres et variances cumulées de chaque axes issues d'une AFM avec R\n\n\n\nValeur propres\nPourcentage de variance cumulée\n\n\n\n\ncomp 1\n1,9203865\n67,85068\n\n\ncomp 2\n0,5123847\n85,95414\n\n\ncomp 3\n0,2050340\n93,19836\n\n\ncomp 4\n0,0790131\n95,99003\n\n\ncomp 5\n0,0659024\n98,31848\n\n\ncomp 6\n0,0327210\n99,47457\n\n\ncomp 7\n0,0148713\n100,00000\n\n\ncomp 8\n0,0000000\n100,00000\n\n\n\na Source des données : https://marieetienne.github.io/datasets/overshootday_overview.csv\n\n\n\n\n\n\n\n\n\n\n\nOn voit qu’il n’y a pas très grande différence entre les pourcentage de variances cumulées des deux AFM (manuellement @variance.cum.val.prop.2acp et avec R) parcontre les valeurs propres ne sont pas les mêmes.\n\nOn peut visualiser les variables\n\n\nfviz_mfa_var(model.afm, axes = c(1,2), choice= \"quanti.var\", repel = T) + theme_light()\n\n\n\n\nFigure 8 : Visualisation des variables dans le plan (1,2) avec les résultats de l’AFM\n\n\n\n\n\nOn peut visualiser leur qualité de representation\n\n\ngraph.cos.var.afm &lt;- fviz_mfa_var(model.afm, axes = c(1,2), choice= \"quanti.var\", col.var=\"cos2\", gradient.cols=c(\"#F1C40F\",\"#2ECC71\",\"#8E44AD\"), repel = T, ggtheme = theme_light())\n\ngraph.cos.var.afm\n\n\n\n\nFigure 9 : Qualité de représentation des variables dans le plan (1,2) avec les résultats de l’AFM\n\n\n\n\n\nLeur contribution à la formation des axes\n\n\ngraph.contrib.var.afm &lt;- fviz_mfa_var(model.afm,col.var=\"contrib\", gradient.cols=c(\"#F1C40F\",\"#2ECC71\",\"#8E44AD\"), repel=TRUE, ggtheme = theme_light())\n\ngraph.contrib.var.afm\n\n\n\n\nFigure 10 : Cercle de corrélation des variables et leur contribution à la formation des axes"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#contexte",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#contexte",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Contexte",
    "text": "Contexte\n\nLe jeu de données mtcars est l’un des ensembles de données les plus connus en statistiques et science des données. Il contient des informations sur les spécifications techniques et les performances de 32 modèles de voitures des années 1970. Ce dataset offre une opportunité unique d’explorer des relations entre des variables mécaniques, comme la consommation en carburant, la puissance ou encore le poids des véhicules."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#problématique",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#problématique",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Problématique",
    "text": "Problématique\n\nComment exploiter les relations entre les caractéristiques des voitures pour identifier des groupes ou des tendances qui pourraient aider à la prise de décision dans le secteur automobile ?"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectif-général",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectif-général",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Objectif général",
    "text": "Objectif général\n\nÉtudier les relations entre les caractéristiques techniques des voitures afin de dégager des tendances et des informations utiles pour la conception ou la sélection des véhicules."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectifs-spécifiques",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#objectifs-spécifiques",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Objectifs spécifiques",
    "text": "Objectifs spécifiques\n\n\nExplorer les relations entre la consommation en carburant (mpg) et les caractéristiques mécaniques\n\n\n\n\nIdentifier des groupes de voitures ayant des caractéristiques similaires à l’aide d’analyses descriptives et graphiques."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#matériels",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#matériels",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Matériels",
    "text": "Matériels\n\nLogiciel utilisé : RStudio avec les packages nécessaires (ggplot2, dplyr, cowplot, etc.)\nSource des données : Jeu de données intégré mtcars."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\n\nNettoyage des données : Vérification des valeurs manquantes ou aberrantes.\nAnalyse descriptive : Moyennes, médianes, écart-types pour chaque variable."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\nModélisation multivariée : Variables utilisées\n\nmpg : Consommation de carburant en miles par gallon (variable dépendante).\nwt : Poids du véhicule (en milliers de livres).\ncyl : Nombre de cylindres du moteur.\nam: Type de transmission (0 = automatique, 1 = manuelle).\ncarb : Nombre de carburateurs.\nhp : Puissance brute du moteur (en chevaux-vapeur)."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-2",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-2",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\nModélisation multivariée :\n\\[\n\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]\noù :\n\n\\(Y\\): Vecteur des valeurs observées (dépendantes ici mpg)\n\\(X\\) : Matrice des variables explicatives (indépendantes), incluant une colonne de 1 pour l’intercept.\n\\(\\beta\\) : Vecteur des coefficients estimés du modèle.\n\\(\\epsilon\\) : Vecteur des erreurs résiduelles."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-3",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-3",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\nTests de significativité des coefficients\nTest t de Student\n\nHypothèse nulle  \\(H_0\\) : le coefficient est égal à zéro (c’est-à-dire, la variable n’a pas d’effet significatif).\nHypothèse alternative \\(H_a\\) : Le coefficient est différent de zéro.\n\nSi la p-valeur est inférieure à un seuil significatif \\(p &lt; 0.05\\), nous rejetons l’hypothèse nulle et concluons que la variable a un effet significatif sur la variable dépendante."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-4",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-4",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\nSignificativité globale du modèle : Test F\n\nHypothèse nulle \\(H_0\\): Tous les coefficients sont égaux à zéro (pas de pouvoir explicatif).\nHypothèse alternative \\(H_a\\) : Au moins un coefficient est différent de zéro (le modèle est significatif).\n\nSi la p-valeur du test \\(F\\) est inférieure à \\(0.05\\), nous rejetons l’hypothèse nulle et concluons que le modèle est significatif."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-5",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-5",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\nR-carré : qualité d’ajustement\n\n\\(R^2\\) varie entre 0 et 1 :\n\nUn \\(R^2\\) proche de 1 signifie que le modèle explique bien les variations de la variable dépendante.\nUn \\(R^2\\) proche de 0 indique que le modèle n’explique que peu ou pas les variations de la variable dépendante."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-6",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#méthode-6",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Méthode",
    "text": "Méthode\n\nVisualisations :\n\nGraphiques de dispersion (scatterplots) pour étudier les corrélations\nHistogrammes pour analyser la distribution des variables"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#présentation-de-léchantillon",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#présentation-de-léchantillon",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Présentation de l’échantillon",
    "text": "Présentation de l’échantillon\n\n\n\n\n\nTable 1 : Description des variables du jeu de données\n\n\nColonne\nNom\nDescription\n\n\n\n\n[,1]\nmpg\nMiles par gallon (US)\n\n\n[,2]\ncyl\nNombre de cylindres\n\n\n[,3]\ndisp\nCylindrée (en pouces cubes)\n\n\n[,4]\nhp\nPuissance brute (chevaux)\n\n\n[,5]\ndrat\nRapport du pont arrière\n\n\n[,6]\nwt\nPoids (en milliers de livres)\n\n\n[,7]\nqsec\nTemps pour parcourir 1/4 de mile\n\n\n[,8]\nvs\nType de moteur (0 = V, 1 = ligne droite)\n\n\n[,9]\nam\nType de transmission (0 = automatique, 1 = manuelle)\n\n\n[,10]\ngear\nNombre de vitesses avant\n\n\n[,11]\ncarb\nNombre de carburateurs\n\n\n\na R : mtcars"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#présentation-de-léchantillon-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#présentation-de-léchantillon-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Présentation de l’échantillon",
    "text": "Présentation de l’échantillon\nRésumé statistiques\n\n\n\n\nTable 2 : Résumé statistique des variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nmin\n10.400000\n4.000000\n71.1000\n52.00000\n2.7600000\n1.5130000\n14.500000\n0.0000000\n0.0000000\n3.0000000\n1.0000\n\n\nQ1.25%\n15.425000\n4.000000\n120.8250\n96.50000\n3.0800000\n2.5812500\n16.892500\n0.0000000\n0.0000000\n3.0000000\n2.0000\n\n\nQ3.75%\n22.800000\n8.000000\n326.0000\n180.00000\n3.9200000\n3.6100000\n18.900000\n1.0000000\n1.0000000\n4.0000000\n4.0000\n\n\nmed.50%\n19.200000\n6.000000\n196.3000\n123.00000\n3.6950000\n3.3250000\n17.710000\n0.0000000\n0.0000000\n4.0000000\n2.0000\n\n\nmean\n20.090625\n6.187500\n230.7219\n146.68750\n3.5965625\n3.2172500\n17.848750\n0.4375000\n0.4062500\n3.6875000\n2.8125\n\n\nmax\n33.900000\n8.000000\n472.0000\n335.00000\n4.9300000\n5.4240000\n22.900000\n1.0000000\n1.0000000\n5.0000000\n8.0000\n\n\ncount\n32.000000\n32.000000\n32.0000\n32.00000\n32.0000000\n32.0000000\n32.000000\n32.0000000\n32.0000000\n32.0000000\n32.0000\n\n\nsd\n6.026948\n1.785922\n123.9387\n68.56287\n0.5346787\n0.9784574\n1.786943\n0.5040161\n0.4989909\n0.7378041\n1.6152\n\n\nNA’s\n0.000000\n0.000000\n0.0000\n0.00000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.0000\n\n\n\nNote: aR : mtcars"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats principaux : Modélisation",
    "text": "Résultats principaux : Modélisation\nSélection de modèle en ajoutant ou en supprimant des variables pour minimiser l’AIC\n\n\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI\n      p-value\n    \n  \n  \n    wt\n-3.9\n-5.4, -2.5\n&lt;0.001\n    am\n2.9\n0.05, 5.8\n0.047\n    qsec\n1.2\n0.63, 1.8\n&lt;0.001\n  \n  \n    \n      Abbreviation: CI = Confidence Interval"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats principaux : Modélisation",
    "text": "Résultats principaux : Modélisation\n\nPoids (wt) : L’augmentation du poids réduit la consommation de carburant, avec un coefficient négatif significatif (p-value = 0.000199)\nNombre de cylindres (cyl) : L’effet des cylindres est légèrement négatif, mais le lien reste faible. p-value = 0.098480 (juste au seuil de signification à 0.1)."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation-2",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-principaux-modélisation-2",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats principaux : Modélisation",
    "text": "Résultats principaux : Modélisation\n\nPuissance (hp) : Pas de relation directe significative entre la puissance et la consommation. p-value = 0.140015.\nOptimisation : Le modèle suggère que la réduction du poids des voitures pourrait améliorer leur efficacité énergétique."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-sécondaires",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-sécondaires",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats sécondaires",
    "text": "Résultats sécondaires\nRépartition des voitures par cylindres\n\nLa majorité des voitures ont 4 ou 8 cylindres."
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-sécondaires-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-sécondaires-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats sécondaires",
    "text": "Résultats sécondaires\nRépartition des voitures par transmission"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-sécondaires-2",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#résultats-sécondaires-2",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Résultats sécondaires",
    "text": "Résultats sécondaires"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#discussions-1",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#discussions-1",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Discussions",
    "text": "Discussions\n\nPoids (wt) : Impact significatif sur la consommation en carburant (mpg) avec une p-valeur très faible\nNombre de cylindres (cyl) : Effet marginalement significatif (p = 0,098)\nPuissance (hp) : Pas d’impact significatif sur la consommation (p = 0,14)"
  },
  {
    "objectID": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#discussions-2",
    "href": "FORMATIONS/DATA_PRESENTATIONS/presentation_mtcars.html#discussions-2",
    "title": "ANALYSE EXPLORATOIRE DES DONNEES MTCARS",
    "section": "Discussions",
    "text": "Discussions\n\nR² ajusté : 82,6 %, ce qui indique un bon ajustement du modèle\nTest F : Le modèle est globalement significatif (p &lt; 0,05).\nPuissance (hp) : Pas de relation directe significative entre la puissance et la consommation. p-value = 0.140015."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html",
    "href": "FORMATIONS/logistic_regression_diabetes.html",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "",
    "text": "Le modèle logistique est une technique statistique largement utilisée pour modéliser des variables dépendantes binaires ou des proportions. Il est fondamental en économétrie, en sciences sociales, en biostatistique et dans de nombreux autres domaines."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#importation-des-bibliothèques-necessaires",
    "href": "FORMATIONS/logistic_regression_diabetes.html#importation-des-bibliothèques-necessaires",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Importation des bibliothèques necessaires",
    "text": "Importation des bibliothèques necessaires\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nimport numpy as np"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-données-et-verification-suscinte-de-leur-qualité",
    "href": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-données-et-verification-suscinte-de-leur-qualité",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Chargement des données et verification suscinte de leur qualité",
    "text": "Chargement des données et verification suscinte de leur qualité\n\ndf = pd.read_csv('diabetes-dataset.csv')\nprint('\\nAffichage des données\\n')\n\n\nAffichage des données\n\ndisplay(df.head(5))\n\n   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n0            6      148             72  ...                     0.627   50        1\n1            1       85             66  ...                     0.351   31        0\n2            8      183             64  ...                     0.672   32        1\n3            1       89             66  ...                     0.167   21        0\n4            0      137             40  ...                     2.288   33        1\n\n[5 rows x 9 columns]\n\nprint('\\nInformations sur les données\\n')\n\n\nInformations sur les données\n\ndisplay(df.info)\n\n&lt;bound method DataFrame.info of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;\n\nprint('\\nResumé statistique des données\\n')\n\n\nResumé statistique des données\n\ndisplay(df.describe)\n\n&lt;bound method NDFrame.describe of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-données-et-verification-suscinte-de-leur-qualité-1",
    "href": "FORMATIONS/logistic_regression_diabetes.html#chargement-des-données-et-verification-suscinte-de-leur-qualité-1",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Chargement des données et verification suscinte de leur qualité",
    "text": "Chargement des données et verification suscinte de leur qualité\n\nAffichage des informations sur les données\n\ndf = pd.read_csv('diabetes-dataset.csv')\nprint('\\nAffichage des données\\n')\n\n\nAffichage des données\n\ndisplay(df.head(5))\n\n   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n0            6      148             72  ...                     0.627   50        1\n1            1       85             66  ...                     0.351   31        0\n2            8      183             64  ...                     0.672   32        1\n3            1       89             66  ...                     0.167   21        0\n4            0      137             40  ...                     2.288   33        1\n\n[5 rows x 9 columns]\n\nprint('\\nInformations sur les données\\n')\n\n\nInformations sur les données\n\ndisplay(df.info)\n\n&lt;bound method DataFrame.info of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;\n\nprint('\\nResumé statistique des données\\n')\n\n\nResumé statistique des données\n\ndisplay(df.describe)\n\n&lt;bound method NDFrame.describe of      Pregnancies  Glucose  ...  Age  Outcome\n0              6      148  ...   50        1\n1              1       85  ...   31        0\n2              8      183  ...   32        1\n3              1       89  ...   21        0\n4              0      137  ...   33        1\n..           ...      ...  ...  ...      ...\n763           10      101  ...   63        0\n764            2      122  ...   27        0\n765            5      121  ...   30        0\n766            1      126  ...   47        1\n767            1       93  ...   23        0\n\n[768 rows x 9 columns]&gt;\n\n\n\n\nVérification des valeurs manquantes\n\ndf.columns.isna().sum()\n\n0\n\n\n      Il y’ a aucune valeur manquante car les données ont bien été nettoyées avant d’être mise à disposition sur kaggle.\n\n\nAffichage des statistiques des variables\n      Etant données que les informations sur les variables sont en ce moment ou j’écris indisponibles sur kaggle.\n\nprint('Affichage des valeurs uniques des variables\\n')\n\nAffichage des valeurs uniques des variables\n\nfor variable in df.columns:\n    if variable != \"DiabetesPedigreeFunction\": # je saute car ça fait beaucoup long à l'affichage\n      print(f'\\n {variable}\\n')\n      print(df[variable].unique())\n\n\n Pregnancies\n\n[ 6  1  8  0  5  3 10  2  4  7  9 11 13 15 17 12 14]\n\n Glucose\n\n[148  85 183  89 137 116  78 115 197 125 110 168 139 189 166 100 118 107\n 103 126  99 196 119 143 147  97 145 117 109 158  88  92 122 138 102  90\n 111 180 133 106 171 159 146  71 105 101 176 150  73 187  84  44 141 114\n  95 129  79   0  62 131 112 113  74  83 136  80 123  81 134 142 144  93\n 163 151  96 155  76 160 124 162 132 120 173 170 128 108 154  57 156 153\n 188 152 104  87  75 179 130 194 181 135 184 140 177 164  91 165  86 193\n 191 161 167  77 182 157 178  61  98 127  82  72 172  94 175 195  68 186\n 198 121  67 174 199  56 169 149  65 190]\n\n BloodPressure\n\n[ 72  66  64  40  74  50   0  70  96  92  80  60  84  30  88  90  94  76\n  82  75  58  78  68 110  56  62  85  86  48  44  65 108  55 122  54  52\n  98 104  95  46 102 100  61  24  38 106 114]\n\n SkinThickness\n\n[35 29  0 23 32 45 19 47 38 30 41 33 26 15 36 11 31 37 42 25 18 24 39 27\n 21 34 10 60 13 20 22 28 54 40 51 56 14 17 50 44 12 46 16  7 52 43 48  8\n 49 63 99]\n\n Insulin\n\n[  0  94 168  88 543 846 175 230  83  96 235 146 115 140 110 245  54 192\n 207  70 240  82  36  23 300 342 304 142 128  38 100  90 270  71 125 176\n  48  64 228  76 220  40 152  18 135 495  37  51  99 145 225  49  50  92\n 325  63 284 119 204 155 485  53 114 105 285 156  78 130  55  58 160 210\n 318  44 190 280  87 271 129 120 478  56  32 744 370  45 194 680 402 258\n 375 150  67  57 116 278 122 545  75  74 182 360 215 184  42 132 148 180\n 205  85 231  29  68  52 255 171  73 108  43 167 249 293  66 465  89 158\n  84  72  59  81 196 415 275 165 579 310  61 474 170 277  60  14  95 237\n 191 328 250 480 265 193  79  86 326 188 106  65 166 274  77 126 330 600\n 185  25  41 272 321 144  15 183  91  46 440 159 540 200 335 387  22 291\n 392 178 127 510  16 112]\n\n BMI\n\n[33.6 26.6 23.3 28.1 43.1 25.6 31.  35.3 30.5  0.  37.6 38.  27.1 30.1\n 25.8 30.  45.8 29.6 43.3 34.6 39.3 35.4 39.8 29.  36.6 31.1 39.4 23.2\n 22.2 34.1 36.  31.6 24.8 19.9 27.6 24.  33.2 32.9 38.2 37.1 34.  40.2\n 22.7 45.4 27.4 42.  29.7 28.  39.1 19.4 24.2 24.4 33.7 34.7 23.  37.7\n 46.8 40.5 41.5 25.  25.4 32.8 32.5 42.7 19.6 28.9 28.6 43.4 35.1 32.\n 24.7 32.6 43.2 22.4 29.3 24.6 48.8 32.4 38.5 26.5 19.1 46.7 23.8 33.9\n 20.4 28.7 49.7 39.  26.1 22.5 39.6 29.5 34.3 37.4 33.3 31.2 28.2 53.2\n 34.2 26.8 55.  42.9 34.5 27.9 38.3 21.1 33.8 30.8 36.9 39.5 27.3 21.9\n 40.6 47.9 50.  25.2 40.9 37.2 44.2 29.9 31.9 28.4 43.5 32.7 67.1 45.\n 34.9 27.7 35.9 22.6 33.1 30.4 52.3 24.3 22.9 34.8 30.9 40.1 23.9 37.5\n 35.5 42.8 42.6 41.8 35.8 37.8 28.8 23.6 35.7 36.7 45.2 44.  46.2 35.\n 43.6 44.1 18.4 29.2 25.9 32.1 36.3 40.  25.1 27.5 45.6 27.8 24.9 25.3\n 37.9 27.  26.  38.7 20.8 36.1 30.7 32.3 52.9 21.  39.7 25.5 26.2 19.3\n 38.1 23.5 45.5 23.1 39.9 36.8 21.8 41.  42.2 34.4 27.2 36.5 29.8 39.2\n 38.4 36.2 48.3 20.  22.3 45.7 23.7 22.1 42.1 42.4 18.2 26.4 45.3 37.\n 24.5 32.2 59.4 21.2 26.7 30.2 46.1 41.3 38.8 35.2 42.3 40.7 46.5 33.5\n 37.3 30.3 26.3 21.7 36.4 28.5 26.9 38.6 31.3 19.5 20.1 40.8 23.4 28.3\n 38.9 57.3 35.6 49.6 44.6 24.1 44.5 41.2 49.3 46.3]\n\n Age\n\n[50 31 32 21 33 30 26 29 53 54 34 57 59 51 27 41 43 22 38 60 28 45 35 46\n 56 37 48 40 25 24 58 42 44 39 36 23 61 69 62 55 65 47 52 66 49 63 67 72\n 81 64 70 68]\n\n Outcome\n\n[1 0]\n\n\nAu vu de ces valeurs, on peut dire que (vu qu’il n’y a aucune description des disponible sur kaggle):\n\npregnancies represente le nombre de grossesses contractées;\nglucose represente la quantité de glucose dans le sang;\nBloodPressure represente la pression sanguine;\nSkinThickness represente l’épaisseur du pli cutané tricipital;\nBMI correspond à l’Indice de Masse Corporelle (IMC)\nAge de la patiente\nInsulin représente la concentration sérique d’insuline mesurée (généralement en micro-unités par millilitre (μU/ml))\nDiabetesPedigreeFunction représente une mesure de la prédisposition génétique au diabète\nOutcome represente l’état de la patiente (atteinte ou non du diabète)"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#analyse-exploratoire-des-données",
    "href": "FORMATIONS/logistic_regression_diabetes.html#analyse-exploratoire-des-données",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Analyse exploratoire des données",
    "text": "Analyse exploratoire des données\n      Cette analyse est effectuée dans l’optique de mieux comprendre les données afin de pouvoir bien spécifier le modèle logistique.\n\nAnalyse descriptives rapides (Voir la distribution des données)\n\n# Création de la figure avec une grille 3 lignes x 2 colonnes\nfig, axes = plt.subplots(3, 2, figsize=(12, 12))\n\n# Premier sous-graphe : Distribution du nombre de grossesses\nsns.histplot(data=df['Pregnancies'], ax=axes[0, 0])\naxes[0, 0].set_title(\"Distribution du nombre de grossesses\")\n\n# Deuxième sous-graphe : Distribution du niveau de glucose\nsns.histplot(data=df['Glucose'], ax=axes[0, 1])\naxes[0, 1].set_title(\"Distribution du niveau de glucose\")\n\n# Troisième sous-graphe : Distribution de la pression sanguine\nsns.histplot(data=df['BloodPressure'], ax=axes[1, 0])\naxes[1, 0].set_title(\"Distribution de la pression sanguine\")\n\n# Quatrième sous-graphe : Distribution de l'épaisseur du pli cutané (SkinThickness)\nsns.histplot(data=df['SkinThickness'], ax=axes[1, 1])\naxes[1, 1].set_title(\"Distribution de l'épaisseur du pli cutané\")\n\n# Cinquième sous-graphe : Distribution de l'insuline\nsns.histplot(data=df['Insulin'], ax=axes[2, 0])\naxes[2, 0].set_title(\"Distribution de l'insuline\")\n\n# Sixième sous-graphe : Distribution de l'IMC (BMI)\nsns.histplot(data=df['BMI'], ax=axes[2, 1])\naxes[2, 1].set_title(\"Distribution de l'IMC\")\n\n# Ajustement automatique des espaces pour\n# éviter le chevauchement des titres et labels\nplt.tight_layout()\n\n# Affichage de la figure\nplt.show()\n\n\n\n\n\n\nVerification de la colinéarité\n      En effet avant de spécifier un modèle, il faut s’assurer qu’il n’y a pas multicolinéarité. C’est-à-dire verifier que les variables ne sont pas corrélées entre elles ce qui permettra d’éviter de fausses estimations.\n\n# Sélectionner que les variables numériques des données\ndf_variables_numeriques = df.select_dtypes(include=[np.number])\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(df_variables_numeriques.corr(), annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Heatmap de correlation des variables numériques du jeu de données')\nplt.tight_layout()\nplt.show()\n\n\n\n\n      Ce corrélollogramme montre que les variables ne sont pas linéairement corrélées entre elle. Donc on peut ajuster le modèle de regression logistique."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#spécification-et-évalution-du-modèle-logistique",
    "href": "FORMATIONS/logistic_regression_diabetes.html#spécification-et-évalution-du-modèle-logistique",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Spécification et évalution du modèle logistique",
    "text": "Spécification et évalution du modèle logistique\n      A ce niveau, j’ai partitionné les données en ammont dans le but de faire du machine learning (ajustement, prediction et validation du modèle) plus tard (dans la section suivante). Nous avons les données d’entrainement qui constituent 80% des données et des données de test qui en constituent 20. Ici j’ajuste juste un modèle de regression logistique aux données que j’essaie d’interpreter.\n\n# Les bibliothèques de machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\nfrom sklearn.inspection import permutation_importance\n\n\n# Separation des variables explicatives and de la variable dépendante\n\n# X : variable dépendante\nX = df.drop('Outcome', axis=1)\n\n# y : matrice des variables explicatives\ny = df['Outcome']\n\n# partition des données en données de tests et d'entrainement\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nimport statsmodels.api as sm\n\n# Ajout d'une colonne de 1 pour l'intercept (obligatoire dans statsmodels)\nX_train_const = sm.add_constant(X_train)\n\n# Création du modèle logistique\nmodel = sm.Logit(y_train, X_train_const)\n\n# Ajustement du modèle\nresult = model.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.467835\n         Iterations 6\n\n# Affichage du résumé avec les p-values\ndisplay(result.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                Outcome   No. Observations:                  614\nModel:                          Logit   Df Residuals:                      605\nMethod:                           MLE   Df Model:                            8\nDate:                Thu, 03 Apr 2025   Pseudo R-squ.:                  0.2752\nTime:                        09:56:15   Log-Likelihood:                -287.25\nconverged:                       True   LL-Null:                       -396.34\nCovariance Type:            nonrobust   LLR p-value:                 9.311e-43\n============================================================================================\n                               coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nconst                       -9.0359      0.837    -10.802      0.000     -10.675      -7.396\nPregnancies                  0.0645      0.036      1.791      0.073      -0.006       0.135\nGlucose                      0.0341      0.004      8.055      0.000       0.026       0.042\nBloodPressure               -0.0139      0.006     -2.260      0.024      -0.026      -0.002\nSkinThickness                0.0031      0.008      0.397      0.691      -0.012       0.019\nInsulin                     -0.0018      0.001     -1.782      0.075      -0.004       0.000\nBMI                          0.1026      0.017      5.948      0.000       0.069       0.136\nDiabetesPedigreeFunction     0.6945      0.330      2.107      0.035       0.049       1.341\nAge                          0.0371      0.011      3.400      0.001       0.016       0.058\n============================================================================================"
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#qualité-dajustement",
    "href": "FORMATIONS/logistic_regression_diabetes.html#qualité-dajustement",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Qualité d’Ajustement",
    "text": "Qualité d’Ajustement\n\nLog-Likelihood : -287.25. Un log-vraisemblance plus élevé (moins négatif) indique un meilleur ajustement.\nPseudo R-squared : 0.2752. Cela signifie que le modèle explique environ 27.52% de la variabilité dans les données, ce qui indique un ajustement modéré. Dans les modèles linéaires généralisés, il est fréquent d’avoir des pseudo-R2 un peu faible.\nLLR p-value : 9.311e-43, très faible, indiquant que le modèle est significatif globalement."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#adéquation-du-modèle",
    "href": "FORMATIONS/logistic_regression_diabetes.html#adéquation-du-modèle",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Adéquation du Modèle",
    "text": "Adéquation du Modèle\n\nConvergence : Le modèle a convergé en 6 itérations, suggérant un bon comportement de l’algorithme d’optimisation.\nDf Model : 8, indiquant 8 variables explicatives."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#interprétation-des-coefficients-1",
    "href": "FORMATIONS/logistic_regression_diabetes.html#interprétation-des-coefficients-1",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Interprétation des Coefficients",
    "text": "Interprétation des Coefficients\nLa probabilité \\(p\\) que \\(y = 1\\) (c’est-à-dire que la patiente ait le diabète) est donnée par la fonction sigmoïde :\n\\(p = \\frac{1}{1 + \\exp(-\\beta)}\\)\noù : - \\(\\beta\\) est le coefficient du modèle de régression logistique.\n\n\\(\\exp(-\\beta)\\) représente l’exponentielle de ( -).\n\nAinsi, cette fonction transforme la valeur linéaire ( ) en une probabilité entre 0 et 1.\n\nIntercept (-9.0359) : Lorsque toutes les variables sont à 0, la probabilité prédite que y=1 est proche de 0.\nGlucose (0.0341, p&lt;0.001) : Une augmentation de 1 unité de glucose augmente significativement les odds de l’issue y=1.\nBMI (0.1026, p&lt;0.001) : Indique une relation positive forte entre l’IMC et l’issue.\nBloodPressure (-0.0139, p=0.024) : Relation négative significative, mais l’effet est faible.\nDiabetesPedigreeFunction (0.6945, p=0.035) : Un antécédent familial a un impact positif significatif.\nAge (0.0371, p=0.001) : L’âge est un facteur significatif.\nSkinThickness et Insulin : Effet non significatif (au seuil de risque \\(\\alpha\\) = 0,05)."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#conclusion-1",
    "href": "FORMATIONS/logistic_regression_diabetes.html#conclusion-1",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Conclusion",
    "text": "Conclusion\nLe modèle a une bonne capacité prédictive mais n’explique pas toute la variabilité. Certaines variables sont significatives (Glucose, BMI, Age), alors que d’autres, comme l’Insuline, ne le sont pas."
  },
  {
    "objectID": "FORMATIONS/logistic_regression_diabetes.html#machine-learning",
    "href": "FORMATIONS/logistic_regression_diabetes.html#machine-learning",
    "title": "Djamaldbz - Modélisation des données à variables dépendantes qualitatives : Regression logistique à variable dépendante dichotomique",
    "section": "Machine learning",
    "text": "Machine learning\n\nAjustement du modèle aux données d’apprentissage\n\n# Initialisation et entrainnement du classificateur (Regression Logistique)\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=1000)\n\n# faire les prediction sur les données de test\ny_pred = model.predict(X_test)\n\n# calcul du score de précision\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy Score: {accuracy:.4f}')\n\nAccuracy Score: 0.7468\n\n\n      L’accuracy score de 0.7468 signifie que le modèle a correctement classé 74.68% des échantillons dans le jeu de test. Cette métrique donne une indication de la proportion des prédictions correctes par rapport au nombre total d’observations. Plus l’accuracy est proche de 1 (ou 100%), plus le modèle est performant.\n\n\nEvaluation du modèle\n\n\nMatrice de confusion\n\n\n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Matrice de confusion')\nplt.xlabel('Données prédites')\nplt.ylabel('Données observées')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\\[\n\\textbf{Vrais Positifs (VP)} = 37 \\quad \\text{(Modèle prédit que la patiente a le diabète et c'est correct)}\n\\] \\[\n\\textbf{Faux Positifs (FP)} = 21 \\quad \\text{(Modèle prédit que la patiente a le diabète, mais c'est incorrect)}\n\\]\n\\[\n\\textbf{Faux Négatifs (FN)} = 18 \\quad \\text{(Modèle prédit que la patiente n'a pas le diabète, mais c'est incorrect)}\n\\]\n\\[\n\\textbf{Vrais Négatifs (VN)} = 78 \\quad \\text{(Modèle prédit que la patiente n'a pas le diabète et c'est correct)}\n\\]\n\n\n\nMétriques de performance\n\n\n\n\\[\n\\textbf{Précision} (Precision) :\n\\text{Précision} = \\frac{\\text{VP}}{\\text{VP} + \\text{FP}} = \\frac{37}{37 + 21} = \\frac{37}{58} \\approx 0.6379\n\\]\n\\[\n\\textbf{Rappel} (Recall) :\n\\text{Rappel} = \\frac{\\text{VP}}{\\text{VP} + \\text{FN}} = \\frac{37}{37 + 18} = \\frac{37}{55} \\approx 0.6727\n\\]\n\\[\n\\textbf{Score F1} (F1-Score) :\n\\text{F1-Score} = 2 \\times \\frac{\\text{Précision} \\times \\text{Rappel}}{\\text{Précision} + \\text{Rappel}} = 2 \\times \\frac{0.6379 \\times 0.6727}{0.6379 + 0.6727} \\approx 0.6548\n\\]\n\\[\n\\textbf{Exactitude} (Accuracy) :\n\\text{Exactitude} = \\frac{\\text{VP} + \\text{VN}}{\\text{Total}} = \\frac{37 + 78}{37 + 78 + 21 + 18} = \\frac{115}{154} \\approx 0.7468\n\\]\n\n\nCourbe de ROC\n\n\n\ny_prob = model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6,4))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Courbe ROC (Aire = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('Taux de Faux Positifs')\nplt.ylabel('Taux de Vrais Positifs')\nplt.title('Caractéristique de Performance du Modèle (Courbe ROC)')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n      La courbe ROC (Receiver Operating Characteristic) est un graphique qui permet d’évaluer la performance d’un modèle de classification binaire. Elle trace la relation entre :\n\nLe Taux de Vrais Positifs (TPR, True Positive Rate) : La proportion des vrais positifs parmi les cas positifs réels.\nLe Taux de Faux Positifs (FPR, False Positive Rate) : La proportion des faux positifs parmi les cas négatifs réels.\n\n      La courbe ROC montre comment le modèle se comporte pour différents seuils de décision. Un modèle parfait aura une courbe qui monte rapidement vers le coin supérieur gauche (haute TPR et faible FPR), tandis qu’un modèle aléatoire suivra la diagonale du graphique (FPR = TPR).\n      L’Aire Sous la Courbe (AUC) mesure la qualité globale du modèle. Une AUC proche de 1 indique un excellent modèle, tandis qu’une AUC proche de 0.5 indique un modèle équivalent à un choix aléatoire.\nDans notre cas AUC vaut 0,81 donc notre modèle tient la route.\n\n\n\nVerifions qu’on a les même coefficients que ceux de l’ajustement à la section précédente\n\n\n\n\nmodel.intercept_\n\narray([-9.00605969])\n\n\n\n# affichage des coefficients estimés du modèle\nmodel.coef_\n\narray([[ 0.06439026,  0.03409647, -0.01388056,  0.00329364, -0.00180358,\n         0.10259306,  0.62659359,  0.03708443]])\n\n\nEt oui on a les mêmes coefficients.\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour à la page d’accueuil"
  },
  {
    "objectID": "FORMATIONS/PAYANTES/R/FormationR.html",
    "href": "FORMATIONS/PAYANTES/R/FormationR.html",
    "title": "Djamaldbz - Formations en R en présentiel et en ligne avec Djamal et Saïd",
    "section": "",
    "text": "Ces formations sont conçues pour différents publics cibles : étudiants en pharmacie, médecine, biologie, statistiques et ceux aui sont dans des domaines nécessitant les stats ou pas. Chaque session dure 2 heures, avec une fréquence de 2 sessions par semaine. Les formations débutent le 22 février 2025.\n\n\n\n\n5 000 FCFA par session de 2 heures.\nChaque formation complète comprend 4 sessions, soit 20 000 FCFA par participant.\n\n\n\n\n\n\n\nCalendrier des Formations\n\n\nDate\nPublic.cible\nSujet\n\n\n\n\n22 février\nPharmacie\nIntroduction à R\n\n\n22 février\nMédecine\nIntroduction à R\n\n\n22 février\nBiologie\nIntroduction à R\n\n\n26 février\nStatistiques\nR pour les statisticiens\n\n\n\n\n\n\n\n\n\n\n\nObjectif : Apprendre à gérer, analyser et visualiser des données pharmacologiques.\nSessions :\n\nIntroduction à R.\nGestion des données pharmacologiques.\nVisualisation des données.\nAnalyse statistique (tests t, ANOVA).\n\n\n\n\n\n\nObjectif : Explorer des données cliniques et épidémiologiques.\nSessions :\n\nIntroduction à R.\nStatistiques descriptives.\nVisualisation des données médicales.\n\n\n\n\n\n\nObjectif : Analyser des données biologiques\nSessions :\n\nIntroduction à R.\nVisualisation des données biologiques.\nAnalyse statistique.\n\n\n\n\n\n\nObjectif : Approfondir les outils statistiques et analytiques.\nSessions :\n\nR pour les statisticiens.\nVisualisations avancées avec ggplot2.\nModélisation statistique (modèles linéaires, généralisés).\nProgrammation avancée (création de fonctions, etc …).\n\n\n\n\n\n\n\n\nRappel\n\n\n\nPour celles et ceux qui ne font pas partie des domaines mentionnés, ne vous inquiétez pas : cette formation est conçue pour être accessible et adaptée à tous les profils. Vous en tirerez pleinement profit !\n\n\n\n\n\n\n\n\nLocalisation pour la formation en présentiel\n\n\n\nLes participants doivent être au Burkina-Faso, plus précisement dans la ville de Bobo-Dioulasso. Les séances en ligne interviendront rarement. Elles serviront à donner certains details et seront une alternatives en cas d’empêchement !!!\n\n\n\n\n\n\n\n\nLocalisation pour la formation en présentiel\n\n\n\nPour les participants ayant des empechements (localisation géographique, timing etc …), une formartion ligne sera possible mais au lieu de 2h ce sera 1h30 !!!\n\n\n\n\n\n\nLes participants bénéficieront de formations pratiques, avec des cas d’utilisation adaptés à leur domaine. Inscrivez-vous dès maintenant pour réserver votre place! 😊\n\nPOUR PLUS D’INFORMATIONS !!!\n\n      Veuillez contacter le numéro whatsapp suivant : +226 57036356"
  },
  {
    "objectID": "FORMATIONS/PAYANTES/R/FormationR.html#plan-des-formations-en-r---niveau-1",
    "href": "FORMATIONS/PAYANTES/R/FormationR.html#plan-des-formations-en-r---niveau-1",
    "title": "Djamaldbz - Formations en R en présentiel et en ligne avec Djamal et Saïd",
    "section": "",
    "text": "Ces formations sont conçues pour différents publics cibles : étudiants en pharmacie, médecine, biologie, statistiques et ceux aui sont dans des domaines nécessitant les stats ou pas. Chaque session dure 2 heures, avec une fréquence de 2 sessions par semaine. Les formations débutent le 22 février 2025.\n\n\n\n\n5 000 FCFA par session de 2 heures.\nChaque formation complète comprend 4 sessions, soit 20 000 FCFA par participant.\n\n\n\n\n\n\n\nCalendrier des Formations\n\n\nDate\nPublic.cible\nSujet\n\n\n\n\n22 février\nPharmacie\nIntroduction à R\n\n\n22 février\nMédecine\nIntroduction à R\n\n\n22 février\nBiologie\nIntroduction à R\n\n\n26 février\nStatistiques\nR pour les statisticiens\n\n\n\n\n\n\n\n\n\n\n\nObjectif : Apprendre à gérer, analyser et visualiser des données pharmacologiques.\nSessions :\n\nIntroduction à R.\nGestion des données pharmacologiques.\nVisualisation des données.\nAnalyse statistique (tests t, ANOVA).\n\n\n\n\n\n\nObjectif : Explorer des données cliniques et épidémiologiques.\nSessions :\n\nIntroduction à R.\nStatistiques descriptives.\nVisualisation des données médicales.\n\n\n\n\n\n\nObjectif : Analyser des données biologiques\nSessions :\n\nIntroduction à R.\nVisualisation des données biologiques.\nAnalyse statistique.\n\n\n\n\n\n\nObjectif : Approfondir les outils statistiques et analytiques.\nSessions :\n\nR pour les statisticiens.\nVisualisations avancées avec ggplot2.\nModélisation statistique (modèles linéaires, généralisés).\nProgrammation avancée (création de fonctions, etc …).\n\n\n\n\n\n\n\n\nRappel\n\n\n\nPour celles et ceux qui ne font pas partie des domaines mentionnés, ne vous inquiétez pas : cette formation est conçue pour être accessible et adaptée à tous les profils. Vous en tirerez pleinement profit !\n\n\n\n\n\n\n\n\nLocalisation pour la formation en présentiel\n\n\n\nLes participants doivent être au Burkina-Faso, plus précisement dans la ville de Bobo-Dioulasso. Les séances en ligne interviendront rarement. Elles serviront à donner certains details et seront une alternatives en cas d’empêchement !!!\n\n\n\n\n\n\n\n\nLocalisation pour la formation en présentiel\n\n\n\nPour les participants ayant des empechements (localisation géographique, timing etc …), une formartion ligne sera possible mais au lieu de 2h ce sera 1h30 !!!\n\n\n\n\n\n\nLes participants bénéficieront de formations pratiques, avec des cas d’utilisation adaptés à leur domaine. Inscrivez-vous dès maintenant pour réserver votre place! 😊\n\nPOUR PLUS D’INFORMATIONS !!!\n\n      Veuillez contacter le numéro whatsapp suivant : +226 57036356"
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html",
    "href": "FORMATIONS/poisson_paludisme.html",
    "title": "Djamaldbz - Modélisation des données de comptage",
    "section": "",
    "text": "Le monde actuel est confronté à de multiples risques sanitaires, notamment ceux liés aux maladies vectorielles telles que le paludisme. En effet, le paludisme est la maladie la plus mortelle transmise par les moustiques dans le monde ((OMS) 2023). Selon l’OMS, plusieurs millions de personnes ont été infectées par le paludisme en 2022 (environ 249 millions), entraînant près de 608 000 décès(Mondiale de la Santé) 2023).\nPlusieurs actions ont été menées pour lutter contre ce fléau, notamment la distribution de moustiquaires, les campagnes de sensibilisation à l’hygiène, la chimioprévention saisonnière, ainsi que le traitement intermittent pour les femmes enceintes.\nDjamaland a été choisi comme pays pour la mise en oeuvre d’une intervention progressive, principalement en raison de sa forte incidence du paludisme. L’intervention comprend quatre phases et couvre l’ensemble des régions du pays.\nVoici la description de chaque phase :\n\nPhase 1 : Aucun village n’a reçu d’intervention.\nPhase 2 : Les quatre régions ont bénéficié de la distribution de moustiquaires.\nPhase 3 : En plus de la distribution de moustiquaires, des actions de sensibilisation sur les bonnes pratiques d’utilisation ont été mises en place.\nPhase 3 suite : En complément de la distribution et de la sensibilisation, un programme de partage des techniques de bonne hygiène a été intégré.\n\nLa base contenait également des informations sur les facteurs environnementaux (pression atmosphérique, vitesse du vent, indice UV, humidité relative).\nLe but de cette étude est donc d’évaluer l’impact de l’intervention durant ces différentes phases.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(MASS) \nlibrary(car)\n\n\nInformation sur les variables\n\n      On affiche ici les informations sur les variables de la base de données. On voit qu’il y’a 19 colonnes (variables) et 1040 lignes (observations).\n\ndata %&gt;%\n  glimpse()\n\nRows: 1.040\nColumns: 19\n$ Semaine                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ Région                   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R…\n$ Saison                   &lt;chr&gt; \"Seche\", \"Seche\", \"Seche\", \"Seche\", \"Seche\", …\n$ Phase                    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Taux de couverture`     &lt;dbl&gt; 0,1575155, 0,2576610, 0,1817954, 0,2766035, 0…\n$ Température              &lt;dbl&gt; 25,52402, 31,06084, 28,69764, 28,11317, 30,57…\n$ Humidité                 &lt;dbl&gt; 43,75700, 44,61046, 51,25039, 49,40113, 48,49…\n$ Pluviométrie             &lt;dbl&gt; 16,0111516, 8,2997481, 44,7344354, 38,9232328…\n$ `Vitesse du vent`        &lt;dbl&gt; 6,239284, 7,855973, 3,604419, 6,324740, 2,649…\n$ `Pression atmosphérique` &lt;dbl&gt; 1015,6362, 1008,8631, 1008,3488, 1022,2085, 1…\n$ `Indice de chaleur`      &lt;dbl&gt; 19,83300, 25,96362, 23,67617, 22,94634, 25,61…\n$ `Couverture nuageuse`    &lt;dbl&gt; 82,280557, 57,209495, 32,469455, 14,619983, 1…\n$ `Vent en hauteur`        &lt;dbl&gt; 14,668663, 8,698154, 11,789413, 14,660355, 11…\n$ `Indice UV`              &lt;dbl&gt; 7,03602105, 10,09389904, 0,64470344, 5,463245…\n$ `Température de l'eau`   &lt;dbl&gt; 33,22569, 28,26913, 28,00962, 29,71320, 28,66…\n$ `Humidité à l’ombre`     &lt;dbl&gt; 53,83643, 64,71406, 55,98915, 63,04538, 62,21…\n$ Aérosols                 &lt;dbl&gt; 1,477347, 4,530642, 34,596087, 80,345317, 18,…\n$ `Cas palustres`          &lt;dbl&gt; 97, 62, 76, 66, 163, 67, 154, 168, 79, 58, 15…\n$ Dates                    &lt;date&gt; 2021-01-08, 2021-01-15, 2021-01-22, 2021-01-…\n\n\n      On affiche ensuite un résumé statistique des variables dans le but de reperer certaines anomalies s’il y en a. Mais dans ce cas, il y’en a pas car j’ai moi même généré les données et donc j’ai veillé à ce qu’il n y ait pas de valeurs manquantes.\n\nlibrary(dplyr)\ndata %&gt;%\n  summary()\n\n    Semaine       Région      Saison          Phase   Taux de couverture\n Min.   :  1,00   R1:260   Length:1040        0:516   Min.   :0,1001    \n 1st Qu.: 65,75   R2:260   Class :character   1:104   1st Qu.:0,2019    \n Median :130,50   R3:260   Mode  :character   2:208   Median :0,4023    \n Mean   :130,50   R4:260                      3:212   Mean   :0,4301    \n 3rd Qu.:195,25                                       3rd Qu.:0,6584    \n Max.   :260,00                                       Max.   :0,8998    \n  Température       Humidité      Pluviométrie       Vitesse du vent\n Min.   :13,64   Min.   :35,41   Min.   :  0,04131   Min.   :2,001  \n 1st Qu.:23,97   1st Qu.:48,81   1st Qu.: 21,90016   1st Qu.:3,975  \n Median :27,09   Median :54,81   Median : 41,11740   Median :5,975  \n Mean   :27,08   Mean   :61,99   Mean   : 64,46301   Mean   :5,913  \n 3rd Qu.:30,21   3rd Qu.:78,54   3rd Qu.:106,59262   3rd Qu.:7,826  \n Max.   :40,19   Max.   :95,11   Max.   :199,59270   Max.   :9,998  \n Pression atmosphérique Indice de chaleur Couverture nuageuse Vent en hauteur \n Min.   : 980,4         Min.   : 8,405    Min.   : 0,0502     Min.   : 5,007  \n 1st Qu.:1006,5         1st Qu.:19,530    1st Qu.:25,7869     1st Qu.: 7,288  \n Median :1012,9         Median :22,434    Median :50,2530     Median : 9,831  \n Mean   :1012,9         Mean   :22,400    Mean   :50,5150     Mean   : 9,916  \n 3rd Qu.:1019,5         3rd Qu.:25,580    3rd Qu.:76,9193     3rd Qu.:12,654  \n Max.   :1041,6         Max.   :36,178    Max.   :99,9899     Max.   :14,995  \n   Indice UV         Température de l'eau Humidité à l’ombre    Aérosols       \n Min.   : 0,001759   Min.   :20,80        Min.   : 42,66     Min.   : 0,00251  \n 1st Qu.: 3,201972   1st Qu.:26,32        1st Qu.: 58,98     1st Qu.:23,49266  \n Median : 5,851642   Median :28,40        Median : 64,76     Median :47,47861  \n Mean   : 6,034127   Mean   :28,40        Mean   : 72,04     Mean   :48,53851  \n 3rd Qu.: 9,109514   3rd Qu.:30,54        3rd Qu.: 88,35     3rd Qu.:73,70956  \n Max.   :11,971607   Max.   :37,70        Max.   :104,38     Max.   :99,89207  \n Cas palustres       Dates           \n Min.   : 13,0   Min.   :2021-01-08  \n 1st Qu.: 58,0   1st Qu.:2022-04-06  \n Median : 93,0   Median :2023-07-03  \n Mean   :111,2   Mean   :2023-07-03  \n 3rd Qu.:145,2   3rd Qu.:2024-09-28  \n Max.   :466,0   Max.   :2025-12-26  \n\n\n      Pour cette étude, la variable d’intérêt est le nombre de nouveaux cas de paludisme enregistrés chaque semaine (t), avec des valeurs variant de 1 à 260 dans les quatre régions du pays."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#description-du-jeu-de-données",
    "href": "FORMATIONS/poisson_paludisme.html#description-du-jeu-de-données",
    "title": "Djamaldbz - Modélisation des données de comptage",
    "section": "",
    "text": "Le monde actuel est confronté à de multiples risques sanitaires, notamment ceux liés aux maladies vectorielles telles que le paludisme. En effet, le paludisme est la maladie la plus mortelle transmise par les moustiques dans le monde ((OMS) 2023). Selon l’OMS, plusieurs millions de personnes ont été infectées par le paludisme en 2022 (environ 249 millions), entraînant près de 608 000 décès(Mondiale de la Santé) 2023).\nPlusieurs actions ont été menées pour lutter contre ce fléau, notamment la distribution de moustiquaires, les campagnes de sensibilisation à l’hygiène, la chimioprévention saisonnière, ainsi que le traitement intermittent pour les femmes enceintes.\nDjamaland a été choisi comme pays pour la mise en oeuvre d’une intervention progressive, principalement en raison de sa forte incidence du paludisme. L’intervention comprend quatre phases et couvre l’ensemble des régions du pays.\nVoici la description de chaque phase :\n\nPhase 1 : Aucun village n’a reçu d’intervention.\nPhase 2 : Les quatre régions ont bénéficié de la distribution de moustiquaires.\nPhase 3 : En plus de la distribution de moustiquaires, des actions de sensibilisation sur les bonnes pratiques d’utilisation ont été mises en place.\nPhase 3 suite : En complément de la distribution et de la sensibilisation, un programme de partage des techniques de bonne hygiène a été intégré.\n\nLa base contenait également des informations sur les facteurs environnementaux (pression atmosphérique, vitesse du vent, indice UV, humidité relative).\nLe but de cette étude est donc d’évaluer l’impact de l’intervention durant ces différentes phases.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(MASS) \nlibrary(car)\n\n\nInformation sur les variables\n\n      On affiche ici les informations sur les variables de la base de données. On voit qu’il y’a 19 colonnes (variables) et 1040 lignes (observations).\n\ndata %&gt;%\n  glimpse()\n\nRows: 1.040\nColumns: 19\n$ Semaine                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ Région                   &lt;fct&gt; R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R1, R…\n$ Saison                   &lt;chr&gt; \"Seche\", \"Seche\", \"Seche\", \"Seche\", \"Seche\", …\n$ Phase                    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Taux de couverture`     &lt;dbl&gt; 0,1575155, 0,2576610, 0,1817954, 0,2766035, 0…\n$ Température              &lt;dbl&gt; 25,52402, 31,06084, 28,69764, 28,11317, 30,57…\n$ Humidité                 &lt;dbl&gt; 43,75700, 44,61046, 51,25039, 49,40113, 48,49…\n$ Pluviométrie             &lt;dbl&gt; 16,0111516, 8,2997481, 44,7344354, 38,9232328…\n$ `Vitesse du vent`        &lt;dbl&gt; 6,239284, 7,855973, 3,604419, 6,324740, 2,649…\n$ `Pression atmosphérique` &lt;dbl&gt; 1015,6362, 1008,8631, 1008,3488, 1022,2085, 1…\n$ `Indice de chaleur`      &lt;dbl&gt; 19,83300, 25,96362, 23,67617, 22,94634, 25,61…\n$ `Couverture nuageuse`    &lt;dbl&gt; 82,280557, 57,209495, 32,469455, 14,619983, 1…\n$ `Vent en hauteur`        &lt;dbl&gt; 14,668663, 8,698154, 11,789413, 14,660355, 11…\n$ `Indice UV`              &lt;dbl&gt; 7,03602105, 10,09389904, 0,64470344, 5,463245…\n$ `Température de l'eau`   &lt;dbl&gt; 33,22569, 28,26913, 28,00962, 29,71320, 28,66…\n$ `Humidité à l’ombre`     &lt;dbl&gt; 53,83643, 64,71406, 55,98915, 63,04538, 62,21…\n$ Aérosols                 &lt;dbl&gt; 1,477347, 4,530642, 34,596087, 80,345317, 18,…\n$ `Cas palustres`          &lt;dbl&gt; 97, 62, 76, 66, 163, 67, 154, 168, 79, 58, 15…\n$ Dates                    &lt;date&gt; 2021-01-08, 2021-01-15, 2021-01-22, 2021-01-…\n\n\n      On affiche ensuite un résumé statistique des variables dans le but de reperer certaines anomalies s’il y en a. Mais dans ce cas, il y’en a pas car j’ai moi même généré les données et donc j’ai veillé à ce qu’il n y ait pas de valeurs manquantes.\n\nlibrary(dplyr)\ndata %&gt;%\n  summary()\n\n    Semaine       Région      Saison          Phase   Taux de couverture\n Min.   :  1,00   R1:260   Length:1040        0:516   Min.   :0,1001    \n 1st Qu.: 65,75   R2:260   Class :character   1:104   1st Qu.:0,2019    \n Median :130,50   R3:260   Mode  :character   2:208   Median :0,4023    \n Mean   :130,50   R4:260                      3:212   Mean   :0,4301    \n 3rd Qu.:195,25                                       3rd Qu.:0,6584    \n Max.   :260,00                                       Max.   :0,8998    \n  Température       Humidité      Pluviométrie       Vitesse du vent\n Min.   :13,64   Min.   :35,41   Min.   :  0,04131   Min.   :2,001  \n 1st Qu.:23,97   1st Qu.:48,81   1st Qu.: 21,90016   1st Qu.:3,975  \n Median :27,09   Median :54,81   Median : 41,11740   Median :5,975  \n Mean   :27,08   Mean   :61,99   Mean   : 64,46301   Mean   :5,913  \n 3rd Qu.:30,21   3rd Qu.:78,54   3rd Qu.:106,59262   3rd Qu.:7,826  \n Max.   :40,19   Max.   :95,11   Max.   :199,59270   Max.   :9,998  \n Pression atmosphérique Indice de chaleur Couverture nuageuse Vent en hauteur \n Min.   : 980,4         Min.   : 8,405    Min.   : 0,0502     Min.   : 5,007  \n 1st Qu.:1006,5         1st Qu.:19,530    1st Qu.:25,7869     1st Qu.: 7,288  \n Median :1012,9         Median :22,434    Median :50,2530     Median : 9,831  \n Mean   :1012,9         Mean   :22,400    Mean   :50,5150     Mean   : 9,916  \n 3rd Qu.:1019,5         3rd Qu.:25,580    3rd Qu.:76,9193     3rd Qu.:12,654  \n Max.   :1041,6         Max.   :36,178    Max.   :99,9899     Max.   :14,995  \n   Indice UV         Température de l'eau Humidité à l’ombre    Aérosols       \n Min.   : 0,001759   Min.   :20,80        Min.   : 42,66     Min.   : 0,00251  \n 1st Qu.: 3,201972   1st Qu.:26,32        1st Qu.: 58,98     1st Qu.:23,49266  \n Median : 5,851642   Median :28,40        Median : 64,76     Median :47,47861  \n Mean   : 6,034127   Mean   :28,40        Mean   : 72,04     Mean   :48,53851  \n 3rd Qu.: 9,109514   3rd Qu.:30,54        3rd Qu.: 88,35     3rd Qu.:73,70956  \n Max.   :11,971607   Max.   :37,70        Max.   :104,38     Max.   :99,89207  \n Cas palustres       Dates           \n Min.   : 13,0   Min.   :2021-01-08  \n 1st Qu.: 58,0   1st Qu.:2022-04-06  \n Median : 93,0   Median :2023-07-03  \n Mean   :111,2   Mean   :2023-07-03  \n 3rd Qu.:145,2   3rd Qu.:2024-09-28  \n Max.   :466,0   Max.   :2025-12-26  \n\n\n      Pour cette étude, la variable d’intérêt est le nombre de nouveaux cas de paludisme enregistrés chaque semaine (t), avec des valeurs variant de 1 à 260 dans les quatre régions du pays."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#description-du-nombre-de-cas-pour-chaque-région",
    "href": "FORMATIONS/poisson_paludisme.html#description-du-nombre-de-cas-pour-chaque-région",
    "title": "Djamaldbz - Modélisation des données de comptage",
    "section": "Description du nombre de cas pour chaque région",
    "text": "Description du nombre de cas pour chaque région\n\np1 &lt;- ggplot(data, aes(x = Dates, y = `Cas palustres`, color = Région)) +\n  geom_line() +\n  facet_wrap(~Région, scales = \"free_y\") +\n  labs(title = \"\", x = \"Année\", y = \"Nombre de cas\") +\n  geom_vline(xintercept = as.numeric(as.Date(\"2023-06-30\")), \n           linetype = \"dashed\", color = \"darkred\", size = 0.5) +\n  geom_vline(xintercept = as.numeric(as.Date(\"2023-12-29\")), \n           linetype = \"dashed\", color = \"darkblue\", size = 0.5) +\n  geom_vline(xintercept = as.numeric(as.Date(\"2024-12-24\")), \n           linetype = \"dashed\", color = \"royalblue\", size = 0.5) +\n  theme_light() +\n  scale_x_date(date_breaks = \"12 months\", date_labels = \"%b %Y\") +  \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\np_interactif1 &lt;- ggplotly(p1)\n\np_interactif1\n\n\n\nFigure 1 : Evolution du nombre de cas de paludisme entre 2021 et 2025\n\n\n      La courbe des séries temporelles des cas de paludisme de 2021 à 2025 pour les quatre régions de l’étude montre une tendance générale à la baisse, particulièrement marquée après la mise en place des interventions. L’interpretation reste quasi pareille pour toute les regions.\nLe test de Mann-Kendall confirme statistiquement cette tendance décroissante significative (p-value &lt; 0.05), avec une diminution notable observée dans chaque région dès l’implémentation de la première phase du projet (figure @ref{fig:evolution}).\nPar ailleurs, le test de Kruskal-Wallis appliqué aux différentes phases du projet révèle une différence significative entre le nombre de cas observés avant et après les interventions (p-value &lt; 0.05), suggérant un impact positif des mesures mises en place.\nEnfin, le pic épidémique le plus élevé a été observé en 2021 dans les régions 1, 3 et 4, avec respectivement 392, 396 et 466 cas de paludisme enregistrés aux mois de septembre et octobre. Pour la région 2, le pic a été atteint en 2022, avec 384 cas observés (figure @ref{fig:evolution})."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#modélisation",
    "href": "FORMATIONS/poisson_paludisme.html#modélisation",
    "title": "Djamaldbz - Modélisation des données de comptage",
    "section": "Modélisation",
    "text": "Modélisation\n\nAnalyse de la corrélation entre les variables météorologiques\n      L’analyse de la corrélation entre les variables montre des liens de corrélation relativement faibles. De plus, le calcul de l’indice de KMO, permettant de vérifier l’adéquation des données à l’analyse en composantes principales, a montré une valeur de 0,5, confirmant le faible niveau de corrélation entre les covariables et ne justifiant ainsi pas la réalisation d’une ACP.\n\ndata_meteo &lt;- data[ , c(6:10 , 13)]\n\n##-- Calcul de la matrice de corrélation\ncor_matrix &lt;- cor(data_meteo, use = \"complete.obs\")\n\n##-- Transformation de la matrice de corrélation en format long pour ggplot2\ncor_melted &lt;- melt(cor_matrix)\ncor_melted$value &lt;- round(cor_melted$value , 2)\ncolnames(cor_melted)[3] &lt;- \"Coefficient de corrélation\"\n\n##-- Création de la heatmap\ncor_plot &lt;- ggplot(cor_melted, aes(x = Var1, y = Var2, fill = `Coefficient de corrélation`)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +  \n##-- Bleu pour négatif, rouge pour positif\n  theme_light() +\n  labs(x = \"Variables\",\n       y = \"Variables\") +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, size = 10, face = \"bold\"),\n    axis.text.y = element_text(size = 10, face = \"bold\"),\n    axis.title.x = element_text(size = 10, face = \"bold\"),\n    axis.title.y = element_text(size = 10, face = \"bold\")\n  )\nggplotly(cor_plot)\n\n\n\nFigure 2 : Heatmap des Corrélations entre Variables Météorologiques\n\n\n\nlibrary(psych)\ndata_meteo &lt;- data[, 6:17]\nKMO(data_meteo)\n\nError in solve.default(r) : \n  system is computationally singular: reciprocal condition number = 2.76177e-18\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = data_meteo)\nOverall MSA =  0,5\nMSA for each item = \n           Température               Humidité           Pluviométrie \n                   0,5                    0,5                    0,5 \n       Vitesse du vent Pression atmosphérique      Indice de chaleur \n                   0,5                    0,5                    0,5 \n   Couverture nuageuse        Vent en hauteur              Indice UV \n                   0,5                    0,5                    0,5 \n  Température de l'eau     Humidité à l’ombre               Aérosols \n                   0,5                    0,5                    0,5 \n\n\n\n\nModélisation\n\nmodel &lt;- glm(`Cas palustres` ~ Température + `Taux de couverture` + Humidité + Pluviométrie + `Vitesse du vent` + `Pression atmosphérique` + Phase, \n             data = data, family = poisson())\n\n#summary(model, exponentiate = TRUE)\n\n\ntbl_regression(model, exponentiate = TRUE) %&gt;%\n  add_global_p() %&gt;%\n  modify_header(label = \"**Variables**\") %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(caption = capTab(\"Résultats de la régression de Poisson : Analyse des facteurs de risque\"))\n\n\n\n\n\n  Tableau 1 : Résultats de la régression de Poisson : Analyse des facteurs de risque\n  \n    \n      Variables\n      IRR\n      95% CI\n      p-value\n    \n  \n  \n    Température\n1,00\n1,00, 1,00\n0,002\n    Taux de couverture\n1,13\n1,07, 1,20\n&lt;0,001\n    Humidité\n1,01\n1,01, 1,01\n&lt;0,001\n    Pluviométrie\n1,00\n1,00, 1,00\n&lt;0,001\n    Vitesse du vent\n0,97\n0,97, 0,98\n&lt;0,001\n    Pression atmosphérique\n1,00\n1,00, 1,00\n&lt;0,001\n    Phase\n\n\n&lt;0,001\n        0\n—\n—\n\n        1\n0,41\n0,39, 0,42\n\n        2\n0,45\n0,43, 0,46\n\n        3\n0,49\n0,47, 0,51\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n    \n  \n  \n\n\n\n\n\nÉvaluation du modèle de Poisson\n\n      Ici le stepAIC permet de fournir une sélection de variables qui améliore le modèle (critère d’AIC que j’aborderai dans une autre publication). L’objectif est de minimiser l’AIC, donc nous devons conserver les variables dont la suppression entraîne une forte augmentation de l’AIC.\n\nDécision de suppression des variables\n\nLa sélection des variables repose sur leur impact sur l’AIC (Akaike Information Criterion). Plus l’AIC augmente après suppression d’une variable, plus cette dernière est importante pour le modèle. Les variables sont classées en deux groupes : celles à conserver absolument et celles qui ont un impact modéré. La fonction stepAIC permet de faire automatiquement la sélection des variables importante dans le modèle.\n\nmod1_poisson &lt;- stepAIC(model) \n\nStart:  AIC=22725,07\n`Cas palustres` ~ Température + `Taux de couverture` + Humidité + \n    Pluviométrie + `Vitesse du vent` + `Pression atmosphérique` + \n    Phase\n\n                           Df Deviance   AIC\n&lt;none&gt;                           16104 22725\n- Température               1    16114 22733\n- `Taux de couverture`      1    16121 22740\n- `Pression atmosphérique`  1    16132 22751\n- `Vitesse du vent`         1    16525 23144\n- Humidité                  1    17231 23850\n- Pluviométrie              1    18145 24763\n- Phase                     3    19109 25723\n\n\n\n\nA conserver absolument\n\n\nCes variables entraînent une forte augmentation de l’AIC si elles sont supprimées, ce qui indique qu’elles contribuent de manière significative à l’explication des cas palustres.\n\nPhase : +2998 d’AIC\nPluviométrie : +2038 d’AIC\nHumidité : +1125 d’AIC\n\n\n\nVariables modérément importantes\n\n\nCes variables ont un impact plus faible sur l’AIC et peuvent potentiellement être supprimées sans altérer significativement la qualité du modèle.\n\nVitesse du vent : +419 d’AIC\nPression atmosphérique : +26 d’AIC\nTaux de couverture : +15 d’AIC\nTempérature : +8 d’AIC\n\n\n\nDécision\n\n\nLes variables Phase, Pluviométrie et Humidité doivent impérativement être conservées, car leur suppression entraîne une augmentation très importante de l’AIC. En revanche, Vitesse du vent, Pression atmosphérique, Taux de couverture et Température ont un impact plus limité et peuvent être envisagées pour la suppression si nécessaire.\n\ntbl_regression(model, exponentiate = TRUE) %&gt;%\n  add_global_p() %&gt;%\n  modify_header(label = \"**Variables**\") %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(caption = capTab(\"Résultats de la régression de Poisson suite au stepAIC : Analyse des facteurs de risque\"))\n\n\n\n\n\n  Tableau 2 : Résultats de la régression de Poisson suite au stepAIC : Analyse des facteurs de risque\n  \n    \n      Variables\n      IRR\n      95% CI\n      p-value\n    \n  \n  \n    Température\n1,00\n1,00, 1,00\n0,002\n    Taux de couverture\n1,13\n1,07, 1,20\n&lt;0,001\n    Humidité\n1,01\n1,01, 1,01\n&lt;0,001\n    Pluviométrie\n1,00\n1,00, 1,00\n&lt;0,001\n    Vitesse du vent\n0,97\n0,97, 0,98\n&lt;0,001\n    Pression atmosphérique\n1,00\n1,00, 1,00\n&lt;0,001\n    Phase\n\n\n&lt;0,001\n        0\n—\n—\n\n        1\n0,41\n0,39, 0,42\n\n        2\n0,45\n0,43, 0,46\n\n        3\n0,49\n0,47, 0,51\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n    \n  \n  \n\n\n\n\n\npar(mfrow = c(2,2))\nplot(mod1_poisson)\n\n\n\n\nFigure 3 : Graphiques de diagnostic du modèle de poisson ajusté\n\n\n\n\n      L’analyse des diagnostics du modèle montre que les résidus de Pearson présentent une répartition aléatoire des points autour de zéro, suggérant l’absence de structure particulière dans les erreurs.\nDe plus, dans le graphique Q-Q, les points suivent approximativement la ligne diagonale, indiquant que les résidus sont normalement distribués, ce qui est un bon signe pour la validité des hypothèses du modèle.\nLa structure des erreurs standard de Pearson montre également une répartition équilibrée autour de la ligne rouge de référence, et un motif aléatoire est observé au niveau des écarts types de Pearson.\nTous ces éléments suggèrent une bonne adéquation du modèle aux données et confirment que les hypothèses sous-jacentes sont raisonnablement respectées.\n\n\nAnalyse de la surdispersion dans un modèle de Poisson\n\n\n\nmod1_poisson %&gt;% \n  performance::check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =    14.991\n  Pearson's Chi-Squared = 15441.190\n                p-value =   &lt; 0.001\n\n\n      Ce resultat suggère qu’il y’a surdispersion dans les données (p-values &lt; 0,05). Dans ce cas plusieurs alternatives sont possibles. Nous avons entre autres le modèle de regression binomiale négative qui est mélange de poisson-gamma et donc prend en compte un paramètre qui est celui de la dispersion. On a également le modèle quasi-poisson qui lui supprime la surdispersion présente dans les données à l’inverse du modèle binomial négatif qui l’estime.\n\n\nAlternative : Le modele binomial negative\n\n\n      En alternative au modèle de Poisson en cas de surdispersion, le modèle binomial négatif a été mentionné (Cameron and Trivedi 2013). En effet, ce modèle intègre un paramètre supplémentaire qui permet de mieux capturer la variabilité excessive des données, offrant ainsi une estimation plus fiable et adaptée aux situations où la variance des observations est supérieure à la moyenne.\n\nmodel_nb &lt;- glm.nb(`Cas palustres` ~ Température + `Taux de couverture` + Humidité + Pluviométrie + `Vitesse du vent` + `Pression atmosphérique` + Phase, \n                   data = data)\n\nmodel_nb &lt;- stepAIC(model_nb)\n\nStart:  AIC=10479,97\n`Cas palustres` ~ Température + `Taux de couverture` + Humidité + \n    Pluviométrie + `Vitesse du vent` + `Pression atmosphérique` + \n    Phase\n\n                           Df   AIC\n- Température               1 10478\n- `Pression atmosphérique`  1 10478\n- `Taux de couverture`      1 10479\n&lt;none&gt;                        10480\n- `Vitesse du vent`         1 10505\n- Humidité                  1 10544\n- Pluviométrie              1 10568\n- Phase                     3 10662\n\nStep:  AIC=10478,09\n`Cas palustres` ~ `Taux de couverture` + Humidité + Pluviométrie + \n    `Vitesse du vent` + `Pression atmosphérique` + Phase\n\n                           Df   AIC\n- `Pression atmosphérique`  1 10476\n- `Taux de couverture`      1 10477\n&lt;none&gt;                        10478\n- `Vitesse du vent`         1 10503\n- Humidité                  1 10547\n- Pluviométrie              1 10566\n- Phase                     3 10660\n\nStep:  AIC=10476,24\n`Cas palustres` ~ `Taux de couverture` + Humidité + Pluviométrie + \n    `Vitesse du vent` + Phase\n\n                       Df   AIC\n- `Taux de couverture`  1 10476\n&lt;none&gt;                    10476\n- `Vitesse du vent`     1 10502\n- Humidité              1 10545\n- Pluviométrie          1 10564\n- Phase                 3 10658\n\nStep:  AIC=10475,49\n`Cas palustres` ~ Humidité + Pluviométrie + `Vitesse du vent` + \n    Phase\n\n                    Df   AIC\n&lt;none&gt;                 10476\n- `Vitesse du vent`  1 10501\n- Humidité           1 10544\n- Pluviométrie       1 10563\n- Phase              3 11113\n\n\n\ntbl_regression(model_nb, exponentiate = TRUE) %&gt;%\n  add_global_p() %&gt;%\n  modify_header(label = \"**Variables**\") %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(caption = capTab(\"Résultats de la régression de Binomial négative : Analyse des facteurs de risque\"))\n\n\n\n\n\n  Tableau 3 : Résultats de la régression de Binomial négative : Analyse des facteurs de risque\n  \n    \n      Variables\n      IRR\n      95% CI\n      p-value\n    \n  \n  \n    Humidité\n1,01\n1,01, 1,01\n&lt;0,001\n    Pluviométrie\n1,00\n1,00, 1,00\n&lt;0,001\n    Vitesse du vent\n0,97\n0,96, 0,98\n&lt;0,001\n    Phase\n\n\n&lt;0,001\n        0\n—\n—\n\n        1\n0,44\n0,40, 0,47\n\n        2\n0,47\n0,45, 0,51\n\n        3\n0,51\n0,48, 0,55\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n    \n  \n  \n\n\n\n\n\n\nInterpretation des résultats\n\n\n\n\n\n\n\n\nIntervalles de confiances des variables météorologiques\n\n\n\nLes intervalles de confiance des variables météorologiques sont aussi petits car les données ont été générées. Et donc du coup avec de vraies données, il est possible de se retrouver avec des intervalles de confiance qui pourraient ne pas ressembler à ceux-ci.\n\n\n\nL’humidité augmente le nombre de cas de paludisme de 1% tandis que la vitesse de vent diminue le nombre de cas de paludisme de 3% (IC =[2% ; 4%]) toute chose étant égale par ailleur (l’influence des autres variables étant retirée).\nLa première phase d’interventions a permis de reduire le nombre de cas de paludisme de 56% (IC = [53% ; 60%]) par rapport à la phase 0 pendant laquelle il n’y avait pas encore d’intervention toute chose etant égale par ailleurs.\nLa seconde phase d’interventions a permis de reduire le nombre de cas de paludisme de 53% (IC = [49% ; 55%]) par rapport à la phase 0 pendant laquelle il n’y avait pas encore d’intervention toute chose etant égale par ailleurs.\nLa troisième phase d’interventions a permis de reduire le nombre de cas de paludisme de 49% (IC = [45% ; 52%]) par rapport à la phase 0 pendant laquelle il n’y avait pas encore d’intervention toute chose etant égale par ailleurs."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#annexes",
    "href": "FORMATIONS/poisson_paludisme.html#annexes",
    "title": "Djamaldbz - Modélisation des données de comptage",
    "section": "Annexes",
    "text": "Annexes\n\nDiagnostic du modèle binomial négatif\n\n\nAnalyse des résidus\n\n\n\npar(mfrow = c(2,2))\nplot(model_nb)\n\n\n\n\nFigure 4 : Graphiques de diagnostic du modèle binomial négatif ajusté\n\n\n\n\n\n\nMulticolinéarité du modèle binomial négatif\n\n\n\nplot(performance::check_collinearity(model_nb))\n\n\n\n\nFigure 5 : VIF du modèle binomial négatif\n\n\n\n\n      On remarque que toutes les variables ont un faible VIF &lt; 5. Cela suggère qu’il n’y a pas de multicolinéarité entre les variables utilisées dans le modèle.\n\n\nTest de Mann-Kendall\n      Ce test a été utilisé avec les alternatives unilatérales droite et gauche pour tester la présence de tendances strictement croissantes ou strictement décroissantes de la serie nombre de cas hebdomadire de paludisme dans chaque région d’etudes.\nHypothèses du test\n\\[\n\\begin{cases}\nH_0 : \\text{La série ne présente pas de tendance monotone (croissante ou décroissante).} \\\\\nH_1 : \\text{La série présente une tendance monotone (croissante ou décroissante).}\n\\end{cases}\n\\] Interprétation\n\nSi la p-value est inférieure au seuil de signification choisi (généralement 0,05),\nalors il y a suffisamment de preuves pour conclure que la série (nombre de cas de paludisme\nou incidences cumulées durant une phase) présente une tendance monotone.\n\nDans le cas contraire, on conclut que la série ne présente aucune tendance significative.\n\n\n\nDescription du modèle de Poisson\n      Soit (\\(Y\\)) le nombre de cas de paludisme hebdomadire Il s’agit d’une variable quantitative discrète prenant ses valeurs dans un intervalle défini. Supposons en outre que ces événements sont indépendants, c’est-à-dire que l’occurrence d’un premier cas n’affecte pas la probabilité d’en observer un autre.\nDans ce contexte, la variable (\\(Y\\)) suit une distribution de Poisson, avec un paramètre () représentant le taux moyen d’apparition d’un cas de paludisme. La probabilité d’observer une valeur donnée de (\\(Y\\)), en fonction de (), est exprimée par la formule suivante :\n\\[ P(Y = y) = \\frac{\\lambda^y}{y!} e^{-\\lambda} \\]\nLa distribution de Poisson n’a qu’un paramètre: () correspond à la fois à sa moyenne et à sa variance.\n\\[E(\\lambda) = V(\\lambda)\\] Le modèle de Poisson a été utilisé pour identifier les facteurs associés à la survenue du cas de paludisme, principalement en raison de la nature discrète de notre variable dépendante.\nLa régression de Poisson s’inscrit dans le cadre des modèles linéaires généralisés, où la variable réponse (\\(Y\\)) suit une distribution de Poisson :\n\\[ y \\sim \\text{Poisson}(\\lambda) \\]\nPuisque () doit être un nombre positif, nous utiliserons la fonction de logarithme comme lien avec le prédicteur linéaire.\n\\[ \\log{\\lambda} = \\eta = \\beta_0 + \\sum_{i = 1}^m \\beta_i x_i \\]\n\n\nEstimation des parametres\nL’estimation des paramètres d’un modèle de Poisson repose sur la méthode du maximum de vraisemblance (MV). Voici les étapes essentielles de l’estimation :\n. Fonction de Vraisemblance\nLa fonction de vraisemblance pour (n) observations est donnée par :\n\\[L(\\beta) = \\prod_{i=1}^{n} \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!}\\]\nEn prenant le logarithme, on obtient la log-vraisemblance :\n\\[\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\log(\\lambda_i) - \\lambda_i - \\log(y_i!) \\right]\\]\nEn remplaçant ( _i ) par ( e^{X_i } ), on obtient :\n\\[\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i (X_i \\beta) - e^{X_i \\beta} - \\log(y_i!) \\right]\\]\nEstimation par Maximum de Vraisemblance\nL’estimation des paramètres ( ) se fait en maximisant la log-vraisemblance. Comme il n’existe pas de solution analytique simple, on utilise des méthodes numériques telles que l’algorithme de Newton-Raphson ou la descente de gradient.\n\n\nAnalyse de la presence de surdispersion dans les données\nTel que mentionné plus haut, l’indépendance des observations est un prérequis du modèle de Poisson. Sa non-vérification peut entraîner une surdispersion des données. Cette surdispersion est quantifiée par un paramètre ( ) qui multiplie la variance attendue : pour une moyenne ( ), la variance devient donc ( ).\nPlus rarement, il peut arriver que ( &lt; 1 ), ce qui correspond à une sous-dispersion des observations. Contrairement à la surdispersion, où les observations ont tendance à être regroupées, la sous-dispersion traduit une répartition plus régulière que prévu.\nAfin de s’assurer de la pertinence du modèle choisi, une analyse de la surdispersion a été réalisée à l’aide du **test de surdispersion*. Les hypothèses du test étaient les suivantes :\n\nHypothèse nulle ((H_0)) : absence de surdispersion (le modèle de Poisson est approprié).\nHypothèse alternative ((H_1)) : présence de surdispersion (le modèle de Poisson n’est pas adapté).\n\nCritère de décision : Une p-value inférieure à 0,05 conduit au rejet de ( H_0), indiquant la présence d’une surdispersion et la nécessité d’envisager un modèle alternatif (comme le quasi-Poisson ou le Poisson négatif)."
  },
  {
    "objectID": "FORMATIONS/poisson_paludisme.html#référence",
    "href": "FORMATIONS/poisson_paludisme.html#référence",
    "title": "Djamaldbz - Modélisation des données de comptage",
    "section": "Référence",
    "text": "Référence\n\n\nCameron, A. Colin, and Pravin K. Trivedi. 2013. Regression Analysis of Count Data. 2nd ed. Econometric Society Monographs. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9781139013567.\n\n\nMondiale de la Santé), OMS (Organisation. 2023. “World Malaria-report 2023-briefing-kit-fre.pdf.” https://cdn.who.int/media/docs/default-source/malaria/world-malaria-reports/wmr2022-regional-briefing-kit-fre.pdf?sfvrsn=7cb400ed_6&download=true.\n\n\n(OMS), Organisation MOndiale de la santé. 2023. “Global technical strategy for malaria 2016-2030.” https://iris.who.int/handle/10665/176712."
  },
  {
    "objectID": "FORMATIONS/presentations.html",
    "href": "FORMATIONS/presentations.html",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "",
    "text": "packages &lt;- c(\"ggplot2\",\"haven\", \"gtsummary\", \"corrr\", \"MASS\",\n              \"dplyr\",\"haven\", \"rstatix\", \"tidyverse\", \"ggpubr\",\n              \"glue\", \"dplyr\",\"ggspatial\", \"ggrepel\",\n              \"readxl\", \"stringr\", \"colorspace\") \n            \nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}"
  },
  {
    "objectID": "FORMATIONS/presentations.html#faire-ses-présentations-directement-avec-r-et-rstudio",
    "href": "FORMATIONS/presentations.html#faire-ses-présentations-directement-avec-r-et-rstudio",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "Faire ses présentations directement avec R et Rstudio",
    "text": "Faire ses présentations directement avec R et Rstudio\n\nPourquoi utiliser R et Rstudio pour ses présentations ?\n\n      R et RStudio offrent des outils puissants pour créer des présentations dynamiques, reproductibles et intégrées à vos analyses de données. Voici quelques raisons :\n\nIntégration parfaite des analyses et des présentations :\n\nNous pouvons combiner code, graphiques, tableaux et explications textuelles dans un seul document. Cela garantit une reproductibilité totale : les résultats sont automatiquement mis à jour si vos données changent.\n\nFlexibilité avec RMarkdown :\n\nCréez des présentations dans divers formats : HTML (slidy, reveal.js), PDF (Beamer), ou powerpoint ppt. Les formats sont hautement personnalisables pour répondre à vos besoins esthétiques et fonctionnels.\n\nSimplification du travail collaboratif :\n\nIl y’a une possibilité de garder un fichier .tex pour ceux qui sont à l’aise avec latex.\n\n\nMaintenant allons-y !!!\n\n\n\n\n\nCommençons par une présentation revaljs\n\n\n\n\nInstaller les packages nécessaires\n\nAssurez-vous d’avoir le package revealjs installé. Si ce n’est pas le cas, installez-le avec :\ninstall.packages(\"revealjs\")\n\nCréer un fichier RMarkdown pour une présentation\n\nCréer un nouveau fichier RMarkdown :\n\nAllez dans : File &gt; New File &gt; Quarto presentation\nDans la fenêtre qui s’ouvre : Entrez un titre et un auteur. Dans l’option Default Output Format, choisissez From Template &gt; Revealjs Presentation.\n\n\nChanger l’en-tête YAML\n\nEn image voici, un descriptif visuel des 04 petites étapes pour la création du fichier avec des images :\n\n\n\n\n\n\nEtape 1\n\n\n\n\n\n\n\nEtape 2\n\n\n\n\n\n\n\n\n\nEtape 3\n\n\n\n\n\n\n\nEtape 4\n\n\n\n\n\n\n\n\n\nExplication de l’en-tête YAML"
  },
  {
    "objectID": "FORMATIONS/presentations.html#informations-générales",
    "href": "FORMATIONS/presentations.html#informations-générales",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "Informations générales",
    "text": "Informations générales\n\ntitle : Titre principal de la présentation\n\nIci : “ANALYSE EXPLORATOIRE DES DONNEES MTCARS”. C’est ce qui s’affiche en haut de la première diapositive.\n\nauthor : Nom(s) des présentateur(s)\n\nIci : “Presented by Djamal Toe”.\n\ninstitute : Institution ou organisation associée\n\nIci : “National School for Statistic and Data Analysis”.\n-date : Date de la présentation\nIci, elle est générée dynamiquement avec : 2025-04-03. Cela affichera automatiquement la date du jour où le fichier est tricoté."
  },
  {
    "objectID": "FORMATIONS/presentations.html#format-et-personnalisation-reveal.js",
    "href": "FORMATIONS/presentations.html#format-et-personnalisation-reveal.js",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "Format et personnalisation (reveal.js)",
    "text": "Format et personnalisation (reveal.js)\nLa section format: revealjs: contient des options spécifiques à la bibliothèque reveal.js, permettant de personnaliser la présentation.\n\nVitesse de transition:  transition-speed: fast définit la vitesse des transitions entre les diapositives. Options possibles : slow, normal, fast.\nAspect ratio :  aspect_ratio: \"16:9\" spécifie le ratio largeur/hauteur des diapositives. Le ratio “16:9” est idéal pour les écrans modernes (écran large). Autres options possibles : “4:3”, “3:2”, etc.\nMarges : margin: 0.02 définit l’espace vide autour du contenu de chaque diapositive. Une valeur faible (comme 0.02) maximise l’espace utilisé sur chaque diapositive.\nCentrage : center: true permet de Centrer le contenu verticalement et horizontalement sur chaque diapositive.\nPied de page : footer: “English classes with Milonnet” : Ajoute un texte en bas de chaque diapositive, comme une signature ou une note de contexte.\nLogo : logo: \"logo_ensai.png\" affiche un logo en haut à droite de chaque diapositive. L’image doit être placée dans le répertoire spécifié ou un chemin relatif correct doit être utilisé.\nCSS personnalisé : css: style.css permet d’utiliser un fichier CSS externe pour personnaliser les styles. Exemple : changer les polices, couleurs, tailles, etc. Le fichier style.css doit être dans le même répertoire ou le chemin approprié doit être indiqué.\nGestion des figure : fig_caption: yes active l’affichage des légendes sous les graphiques insérés.\nTable des matières (ToC) : toc: true active l’affichage d’une table des matières, toc-expand: false exige que les sections de la table des matières ne soient pas développées par défaut, toc-depth: 1 définit la profondeur de la hiérarchie affichée dans la table des matières (seulement les titres principaux #)."
  },
  {
    "objectID": "FORMATIONS/presentations.html#prévisualition",
    "href": "FORMATIONS/presentations.html#prévisualition",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "Prévisualition",
    "text": "Prévisualition\n      Pendant que vous faites la présentations sur Rstudio, vous pouvez la présualiser. Regardez les images ci-après :\n\n\n\n\n\n\nPrevisualisation : etape 1\n\n\n\n\n\n\n\nPrevisualisation : etape 2\n\n\n\n\n\n\n\n\n\nCompilation et Previsualisation : etape 3\n\n\n\n\n\n\n\n\n\n\n\nViewer ou Presenation ?\n\n\n\nA l’étape 2 de la prévisualisation, il se peut que la prévisualisation apparaisse dans la partie Presentation juste à droite de l’onglet Viewer encerclé en rouge sur l’image."
  },
  {
    "objectID": "FORMATIONS/presentations.html#mise-en-forme-avec-le-fichier-css",
    "href": "FORMATIONS/presentations.html#mise-en-forme-avec-le-fichier-css",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "Mise en forme avec le fichier CSS",
    "text": "Mise en forme avec le fichier CSS\n      Pour cette section ne vous inquietez pas si vous n’avez pas de connaissance en html ou en css, nous utiliserons juste un code css pour la mise en forme du titre."
  },
  {
    "objectID": "FORMATIONS/presentations.html#télécharger-le-fichier-de-la-présentation",
    "href": "FORMATIONS/presentations.html#télécharger-le-fichier-de-la-présentation",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "Télécharger le fichier de la présentation",
    "text": "Télécharger le fichier de la présentation\nAvant de télécharger le fichier, vous pouvez voir ce qu’il donne en cliquant sur ce lien\nVous pouvez télécharger le fichier d’analyse exploratoire des données mtcars au format .qmd ci-dessous.\nTélécharger le fichier .qmd\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour à la page d’accueuil"
  },
  {
    "objectID": "FORMATIONS/SIG.html",
    "href": "FORMATIONS/SIG.html",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "",
    "text": "packages &lt;- c(\"ggplot2\",\"haven\", \"gtsummary\", \"corrr\", \"MASS\",\n              \"dplyr\",\"haven\", \"rstatix\", \"tidyverse\", \"ggpubr\",\n              \"glue\", \"dplyr\",\"ggspatial\", \"ggrepel\",\"marmap\", \n              \"readxl\", \"stringr\", \"colorspace\", \"sf\", \"viridis\",\n              \"tools\",\"ggspatial\",\"readxl\",\"openxlsx\",\"grid\",\n              \"outliers\",\"car\",\"ftExtra\",\"tibble\",\n              \"gtsummary\", \"wesanderson\", \"viridis\",\n              \"RColorBrewer\", \"knitr\", \"kableExtra\") \n            \nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, dependencies = T)\n  }\n  library(pkg, character.only = TRUE)\n}"
  },
  {
    "objectID": "FORMATIONS/SIG.html#comment-faire-des-cartes-choroplèthes-et-des-cartes-de-proportions-avec-r",
    "href": "FORMATIONS/SIG.html#comment-faire-des-cartes-choroplèthes-et-des-cartes-de-proportions-avec-r",
    "title": "Djamaldbz - DJAMAL TOE",
    "section": "Comment faire des cartes Choroplèthes et des cartes de proportions avec R ?",
    "text": "Comment faire des cartes Choroplèthes et des cartes de proportions avec R ?\n      Les cartes choroplèthes et les cartes de proportions sont des outils puissants pour visualiser des données géospatiales dans R. Ces cartes permettent de représenter des valeurs quantitatives (par exemple, des taux de population, des moyennes) sur des zones géographiques, souvent des régions administratives comme des départements, des communes, ou des zones géographiques personnalisées.\n\nIntroduction aux Cartes Choroplèthes et Cartes de Proportions\n\nLes cartes choroplèthes colorient les régions géographiques en fonction de valeurs numériques ou de proportions, facilitant l’analyse spatiale et la compréhension des variations géographiques. Elles sont couramment utilisées pour des données socio-économiques, de santé publique, ou des analyses environnementales.\nLes cartes de proportions sont similaires mais mettent davantage l’accent sur les ratios ou proportions par rapport à une valeur totale, comme des pourcentages ou des fractions de populations.\n\nNotions de Base : Polygones, Shapefiles et Coordonnées Avant de créer ces cartes, il est important de comprendre quelques notions de base, comme les polygones et les shapefiles :\n\n\n\n\n\n\n\nPolygones\n\n\n\nUne zone géographique est souvent représentée par un polygone, une forme géométrique fermée qui peut avoir plusieurs côtés. Par exemple, une commune ou un département sur une carte peut être représentée comme un polygone.\n\n\n\n\n\n\n\n\nShapefiles\n\n\n\nCe sont un format de fichier standard pour stocker des informations géospatiales, y compris les coordonnées de points, de lignes et de polygones. Ils peuvent contenir les géométries des entités géographiques ainsi que leurs attributs (valeurs associées à chaque région, comme le revenu moyen ou le taux de chômage).\n\n\n\n\n\n\n\n\nCoordonnées géographiques\n\n\n\nLes coordonnées (latitude et longitude) permettent de positionner ces polygones sur une carte. En R, on utilise des systèmes de coordonnées géographiques et projetées pour gérer et visualiser ces données.\n\n\nPlusieurs pakages permettent de visualiser les données avec les cartes, ici nous interessons aux packages glue et sf.\n\nZone d’étude\n\nSupposons que nous menions une étude au Burkina-Faso. Par exemple, nous mésurer des indicateurs tels que le taux de mortalité, la couverture sanitaire etc … Le Burkina Faso est un pays qui compte 13 regions, mais notre etude s’étend seulement sur 8 regions. Il convient de montrer toutes les regions, puis de mettre en exègue celles qui nous concernent.\n\nPlace au code\n\n\n\nvoir/cacher le code\n\n\n###---- Chargement des shapefiles src = GADM\nroot &lt;- getwd() ##-- la racine du repertoire\n\n##- La carte du pays sans les polygones des regions, communes et/ou departements\npath0 &lt;- paste0(root,\"/DATA_SIG/BFA2/gadm41_BFA_0.shp\")\n\n##- La carte du pays avec le polygone des regions, sans ceux des communes et/ou departements\npath1 &lt;- paste0(root,\"/DATA_SIG/BFA2/gadm41_BFA_1.shp\")\n\n##- La carte du pays avec le polygone des regions, sans ceux des communes et/ou departements\npath2 &lt;- paste0(\"/DATA_SIG/BFA2/gadm41_BFA_2.shp\")\n\n##- La carte du pays avec le polygone des regions, sans ceux des communes et/ou departements\npath3 &lt;- paste0(root,\"/DATA_SIG/BFA2/gadm41_BFA_3.shp\")\n\n\n##-- selection des regions concernées\n\nstudy.area &lt;-  c(\"Boucle du Mouhoun\", \"Centre-Est\", \"Centre-Nord\",\n             \"Centre-Ouest\", \"Nord\", \"Sud-Ouest\",\n             \"Haut-Bassins\", \"Cascades\")\n\n##-- lecture des shapefiles\npays_shp &lt;- read_sf(glue(path0), quiet = T)\nregion_shp &lt;- read_sf(glue(path1), quiet = T)\n#commune_shp &lt;- read_sf(glue(path2), quiet = T)\n#province_shp &lt;- read_sf(glue(path3), quiet = T)\n\n##-- création d'une sous base avec les polygones des regions sélectionnés\n\ndata_region &lt;- region_shp %&gt;% filter(NAME_1 %in% study.area)\n\n\n##-- Study area colors\nstudy_zone_colors &lt;- c(\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\",\n                       \"#3FE1B8\", \"#9467bd\", \"#8c564b\",\n                       \"#00008B\", \"#4B0082\")\n\nstudy_zone_map &lt;- ggplot() +\n  geom_sf(data = pays_shp, aes(linewidth = \"Burkina Faso\"),fill = \"white\", color = \"black\") +\n  geom_sf(data = region_shp, aes(fill = ifelse(\n    NAME_1 %in% study.area,\n    \"Regions d'études\",\n    \"Autres regions\"\n  ) )) +\n  geom_sf_text(data = region_shp, aes(label = ifelse(\n    NAME_1 %in% study.area,\n    study.area,\n    \"\"\n  )), size = 4)+\n  ggspatial::annotation_scale(\n    location = \"br\",\n    bar_cols = c(\"black\", \"white\")\n  )  +\n  theme_light()+\n  ggspatial::annotation_north_arrow(\n    location = \"tr\", which_north = \"true\",\n    pad_x = unit(0.05, \"in\"), pad_y = unit(0.05, \"in\"),\n    style = ggspatial::north_arrow_nautical(\n      fill = c(\"black\", \"white\"),\n      line_col = \"black\"\n    )\n  )+\n  xlab(\"\")+\n  ylab(\"\")+\n  scale_linewidth_manual(values = c(1.2), name = \"\")+\n  scale_fill_manual(values = c(\"white\",\"#1f77b4\"), name=\"Zone d'étude\")+\n  theme_light() + \n  guides(\n    linewidth = guide_legend(order = 1),\n    fill = guide_legend(order = 2),\n    color = guide_legend(order = 3)\n  )\n\n\n\nstudy_zone_map\n\n\n\n\nCartographie de la zone d’étude\n\n\n\n\n\nExpliquons le code à présent\n\n\nCharger les fichier shapefiles :\n\nglue : pour preparer la structure du format (optionnel)\nreadsf : pour lire les fichiers shapefiles\n\nDefinir la zone d’étude : les fichier shapefile devient comme un dataframe, donc est manipulable au même titre que les fichiers excel, csv etc …\nOn trace d’abord la carte du pays, ensuite on ajoute la couche des regions (c’est-à-dire le shapefile des regions). On pourrait le faire simplement avec le shapefile des regions sans celui du pays.\nEnsuite on ajoute la couleur pour la zone concernée et les noms des regions sélectionnées avec geom_sf_text\nannotation_scale permet d’ajouter une barre d’échelle (scale bar) à une carte avec la position br pour dire bottom rigth (en bas à droite)\nannotation_north_arrow est utilisée pour ajouter une flèche du nord sur une carte créée avec ggplot2\nPour le reste il s’agit des fonctions qu’on utilise couramment avec ggplot2\n\n\n\nAfficher/Masquer le tableau\n\n\n\n\n\nTableau 1 : Les 10 premières lignes du shapefile\n\n\nGID_1\nGID_0\nCOUNTRY\nNAME_1\nVARNAME_1\nNL_NAME_1\nTYPE_1\nENGTYPE_1\nCC_1\nHASC_1\nISO_1\ngeometry\n\n\n\n\nBFA.1_1\nBFA\nBurkina Faso\nBoucle du Mouhoun\nNA\nNA\nRégion\nRegion\nNA\nBF.BO\nNA\nPOLYGON ((-2,73901 11,71249...\n\n\nBFA.2_1\nBFA\nBurkina Faso\nCascades\nNA\nNA\nRégion\nRegion\nNA\nBF.CD\nNA\nPOLYGON ((-4,591742 9,70225...\n\n\nBFA.7_1\nBFA\nBurkina Faso\nCentre\nNA\nNA\nRégion\nRegion\nNA\nBF.CT\nNA\nPOLYGON ((-1,2786 12,13921,...\n\n\nBFA.3_1\nBFA\nBurkina Faso\nCentre-Est\nNA\nNA\nRégion\nRegion\nNA\nBF.CE\nNA\nPOLYGON ((0,4371 11,67655, ...\n\n\nBFA.4_1\nBFA\nBurkina Faso\nCentre-Nord\nNA\nNA\nRégion\nRegion\nNA\nBF.CN\nNA\nPOLYGON ((-0,7773 12,66989,...\n\n\nBFA.5_1\nBFA\nBurkina Faso\nCentre-Ouest\nNA\nNA\nRégion\nRegion\nNA\nBF.CO\nNA\nPOLYGON ((-2,360162 11,0081...\n\n\nBFA.6_1\nBFA\nBurkina Faso\nCentre-Sud\nNA\nNA\nRégion\nRegion\nNA\nBF.CS\nNA\nPOLYGON ((-0,8624911 10,985...\n\n\nBFA.8_1\nBFA\nBurkina Faso\nEst\nNA\nNA\nRégion\nRegion\nNA\nBF.ES\nNA\nPOLYGON ((1,384436 11,44223...\n\n\nBFA.9_1\nBFA\nBurkina Faso\nHaut-Bassins\nNA\nNA\nRégion\nRegion\nNA\nBF.HB\nNA\nPOLYGON ((-4,08994 10,79044...\n\n\nBFA.10_1\nBFA\nBurkina Faso\nNord\nNA\nNA\nRégion\nRegion\nNA\nBF.NO\nNA\nPOLYGON ((-1,96586 12,67774...\n\n\n\na Source des données : GADM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCartes choroplèthes\n\n      Les cartes choroplèthes sont des représentations graphiques qui utilisent des nuances de couleurs pour illustrer des données quantitatives ou qualitatives sur des zones géographiques. Chaque zone est remplie d’une couleur qui correspond à une valeur spécifique ou à une plage de valeurs, facilitant ainsi l’analyse des variations spatiales des données.\nLes cartes choroplèthes sont idéales pour représenter des indicateurs comme le taux de mortalité, le revenu moyen, l’accès à l’eau potable, ou encore la couverture sanitaire par région.\n\nExemple de carte choroplèthe\nDans cet exemple, nous allons créer une carte choroplèthe montrant la couverture sanitaire par région au Burkina Faso, en utilisant les données fictives créées plus haut. pour les données, vous pouvez me contacter par email.\n\nEtape 1 : Charger les shapefiles et les données\n\nIci nous nous assurons que les shapefiles des régions et les données sont correctement chargés et liés entre eux. Pour cela on fait une jointure externe.\n\n##-- Joindre les données au shapefile\nregion_data &lt;- region_shp %&gt;% \n  left_join(data, by = c(\"NAME_1\" = \"Region\"))\n\nAvant de passer à l’étape 2, affichons les données générées avant jointure et ceux aprés jointures.\n\n\nAfficher/cacher le code\n\n\ntbl.avant.jointure &lt;- kbl(head(data,10)) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\ntbl.apres.jointure &lt;- kbl(head(data,10)) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;% add_footnote(label = \"Source des données :  GADM\")\n\n\n\n\nAfficher/Masquer le tableau\n\n\ntbl.avant.jointure\ntbl.apres.jointure\n\n\nLes 10 premières lignes des tables\n\n\n\n\n\nAvant jointure\n\n\nRegion\nPopulation\nTaux_Mortalite\nCouverture_Sanitaire\nAcces_Eau_Potable\n\n\n\n\nBoucle du Mouhoun\n1456502\n6,15\n84,25\n76,95\n\n\nCascades\n1815966\n5,19\n72,12\n52,72\n\n\nCentre\n898804\n5,33\n88,35\n65,80\n\n\nCentre-Est\n2030204\n9,26\n77,28\n68,35\n\n\nCentre-Nord\n2378361\n5,62\n87,66\n54,85\n\n\nCentre-Ouest\n2234678\n13,25\n90,85\n70,82\n\n\nCentre-Sud\n2186632\n14,48\n76,81\n75,90\n\n\nEst\n2056293\n8,59\n72,28\n68,34\n\n\nHauts-Bassins\n983002\n5,21\n73,23\n79,62\n\n\nNord\n2135930\n12,97\n81,81\n70,19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAprès jointure\n\n\nRegion\nPopulation\nTaux_Mortalite\nCouverture_Sanitaire\nAcces_Eau_Potable\n\n\n\n\nBoucle du Mouhoun\n1456502\n6,15\n84,25\n76,95\n\n\nCascades\n1815966\n5,19\n72,12\n52,72\n\n\nCentre\n898804\n5,33\n88,35\n65,80\n\n\nCentre-Est\n2030204\n9,26\n77,28\n68,35\n\n\nCentre-Nord\n2378361\n5,62\n87,66\n54,85\n\n\nCentre-Ouest\n2234678\n13,25\n90,85\n70,82\n\n\nCentre-Sud\n2186632\n14,48\n76,81\n75,90\n\n\nEst\n2056293\n8,59\n72,28\n68,34\n\n\nHauts-Bassins\n983002\n5,21\n73,23\n79,62\n\n\nNord\n2135930\n12,97\n81,81\n70,19\n\n\n\na Source des données : GADM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEtape 2 : Créer la carte choroplèthe\n\nUtilisez ggplot2 et geom_sf() pour afficher les régions et les colorer en fonction de la couverture sanitaire.\n\n##-  Carte choroplèthe\nchoropleth_map &lt;- ggplot(region_data) +\n  geom_sf(aes(fill = Couverture_Sanitaire), color = \"black\") +\n  scale_fill_viridis_c(\n    option = \"C\",\n    name = \"Couverture Sanitaire (%)\"\n  ) +\n  ggtitle(\"Carte choroplèthe : Couverture sanitaire par région\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0.5, face = \"bold\")\n  )\n\nchoropleth_map\n\n\n\n\nCouverture sanitaire par région\n\n\n\n\n\nEtape 3 :  Ajouter des éléments décoratifs\n\nAjoutons une barre d’échelle et une flèche du nord pour rendre la carte plus informative.\n\n##- Ajout des éléments décoratifs\nchoropleth_map &lt;- choropleth_map +\n  ggspatial::annotation_scale(location = \"br\") +\n  ggspatial::annotation_north_arrow(\n    location = \"tl\", style = north_arrow_nautical()\n  ) ###-- tl pour top-left (en haut à gauche)\n\nchoropleth_map\n\n\n\n\n\nInterpréter les résultats\n\nExaminez la carte générée et répondez aux questions suivantes : - Quelles régions ont la meilleure couverture sanitaire ? - Quelles régions doivent faire l’objet d’une attention particulière pour améliorer les conditions de vie ?\n\nExtensions possibles\n\nRéalisez une carte choroplèthe pour le taux de mortalité.\nAjoutez des annotations pour les régions ayant les valeurs extrêmes.\nExpérimentez avec d’autres palettes de couleurs en utilisant scale_fill_brewer() ou scale_fill_manual() etc ….\n\n\n\n\n\n\n\nDonnées discrètes ?\n\n\n\nIl se peut qu’il n’y ait pas une variabilité importante dans les données dans ce cas, au lieu d’avoir une palette, nous aurons juste des cases de couleurs comme s’agissait d’un indicateur discrèt. Dans ce cas, recoder juste cet indicateur en un indicateur qualitatif (regrouper par classe) et ensuite utiliser scale_fill_manual() pour definir vos couleurs manuellement ou laisser R le faire tout seul. Le graphique ci-dessous en est un exemple.\n\n\n[Exemple de carte avec un indicateur recodé : Indisponible pour l’instant]\n\nCartes de proportions\n\n\n\n\nA suivre\n\n\n\n\nCartes de proportions avancées\n\n\n\n\n\n\nRetour à la page d’accueuil"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "",
    "text": "Bienvenue sur le site Djamaldbz, dédié à mes travaux en statistiques et informatique. Explorez mes projets, mes recherches, et mes publications."
  },
  {
    "objectID": "index.html#qui-suis-je",
    "href": "index.html#qui-suis-je",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Qui suis-je ?",
    "text": "Qui suis-je ?\nJe m’appelle Djamal Y. TOE, et je suis passionné par les statistiques et l’informatique. Mon parcours m’a permis de me concentrer sur l’analyse des données et la résolution de problèmes avec des méthodes quantitatives. Je suis toujours en apprentissage et cherche à découvrir de nouvelles approches. Mon objectif est d’apporter des solutions pratiques, tout en restant ouvert à l’apprentissage continu et à l’amélioration dans le domaine des statistiques et de la science des données.\n\nEn savoir plus sur moi ici"
  },
  {
    "objectID": "index.html#mon-approche",
    "href": "index.html#mon-approche",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Mon approche",
    "text": "Mon approche\nJ’aime explorer les statistiques, la programmation et l’analyse de données pour mieux comprendre des problématiques et proposer des solutions adaptées. Je m’intéresse particulièrement aux domaines suivants :\n\nAnalyses factorielles et visualisation\nModèles de régression et prévisions\nApplications statistiques interactives\nDéveloppement d’applications\nVision par ordinateur"
  },
  {
    "objectID": "index.html#mes-projets-récents",
    "href": "index.html#mes-projets-récents",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Mes projets récents",
    "text": "Mes projets récents\nVoici quelques-uns de mes projets récents dans le domaine des statistiques et de l’analyse de données :\n\nAnalyse Factorielle et Visualisation Avancée (disponible bientôt)\nModélisation de Régressions Mixtes pour des données complexes (disponible bientôt)\nDéveloppement d’outils interactifs pour la visualisation de données (disponible bientôt)\n\n\nVoir tous mes projets [Bientôt disponible]"
  },
  {
    "objectID": "index.html#dernières-publications",
    "href": "index.html#dernières-publications",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Dernières publications",
    "text": "Dernières publications\nVoici les dernières publications sur des sujets de statistiques et d’informatique que j’ai partagées :\n\nExploration des techniques d’analyse factorielle\nConseils pour réaliser des présentations avec R et Rstudio\nTutoriel sur la réalisation de cartes de proportions et de cartes choroplèthes avec R\nModélisation des données de comptages : Evaluation de l’impact d’une intervention sur le nombre de cas de paludisme\nModèles de regression à variables dépendantes binaires : Modéliser la probabilité d’être atteint du diabètes chez les femmes et faire prédiction sur la base des variables explicatives\nCrée ton assistant virtuel avec commandes vocalesen python\n\n\nVoir toutes les publications [Bientôt disponible]"
  },
  {
    "objectID": "index.html#publications-en-cours",
    "href": "index.html#publications-en-cours",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Publications en cours",
    "text": "Publications en cours\n\nUtilisation de la SVM pour la classification binaire : Classer des personnes par genre en fonction des images données en entrées avec python (disponible bientôt)"
  },
  {
    "objectID": "index.html#contactez-moi",
    "href": "index.html#contactez-moi",
    "title": "Djamaldbz - Explorations en Statistiques et en Informatiques",
    "section": "Contactez-moi",
    "text": "Contactez-moi\nSi vous avez des questions, des suggestions ou l’envie d’échanger autour d’un projet, je serais ravi de vous lire. N’hésitez pas à me contacter.\n\nEnvoyer un message\n\n\n\n\nA propos de nous\nElève en Science de données à l’Ecole Nationale de la Statistique et de l’Analyse de l’Information en France, titulaire d’une licence en Statistiques-informatique.\nRéseaux sociaux\n\nFacebook\nTwitter\nLinkedIn\n\n\nCoordonnées\n\nAdresse : Rennes, 35000, France\nEmail : djamaltoe2905@gmail.com\nTéléphone : +33 ** ** ** ** **\n\nHeures de services\n\n\n\nJour\nHoraire\n\n\n\n\nLundi\n8h30pm - 9:30pm\n\n\nMardi - Vendredi\n7pm - 8pm\n\n\nSamedi\n9:30am - 10:30am\n\n\nEn été\nTous les jours ouvrés de 7h à 16h\n\n\n\n\n\n\n\nInformations supplémentaires\n© 2024 DJAMAL DEV\nContact: djamaltoe2905@gmail.com\nLocalisation: Rennes, France\nTéléphone: +33 ** ** ** ** **"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "",
    "text": "Comment cela fonctionne\n\n\n\nJe tiens tout d’abord à rapperler que je n’utilise pas de modèle NLP pour créer ce petit assistant virtuel. J’avais écris ce progamme en 2021, donc bien evidemment les outils utilisés ont évolué et donc vous pourrez l’ajuster à votre guise. Le code source sera téléchargeable à la fin de la page."
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#wolfram-alpha-cest-quoi-et-à-quoi-ça-sert",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#wolfram-alpha-cest-quoi-et-à-quoi-ça-sert",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "Wolfram Alpha : C’est quoi et à quoi ça sert ?",
    "text": "Wolfram Alpha : C’est quoi et à quoi ça sert ?\nWolfram Alpha est un moteur de calcul et de réponse basé sur l’intelligence artificielle et les algorithmes symboliques. Contrairement à un moteur de recherche classique comme Google, qui fournit des liens vers des sites web, Wolfram Alpha génère directement des réponses précises basées sur des bases de connaissances et des algorithmes mathématiques avancés. Il est souvent utilisé pour des calculs, des questions scientifiques et des recherches basées sur des données structurées.\n\nUtilité :\n\nRésolution d’équations mathématiques et scientifiques\n\nRecherche et analyse de données (statistiques, physique, chimie, finance, etc.)\n\nInterprétation de requêtes en langage naturel\n\nGénération de graphiques et de simulations\n\n🔗 Créer un compte Wolfram Alpha :\nSi vous souhaitez utiliser l’API de Wolfram Alpha dans votre projet, vous devez créer un compte via ce lien :\n👉 Créer un compte Wolfram Alpha Je posterai une demo sur comment creer son compte et recupérer un id pour une application. Car en effet, il existe plusieurs type d’ID qui servent à différentes type d’applications. Il fonctionne en Anglais donc nous allons écrire une fonction pour la traduction du Francais en Anglais afin de poser des questions et une pour la traduction de l’Anglais en Français pour la reponse trouvée. Vous avez bien entendu besoin de connexion pour effectuer les recherches.\nExemple d’utilisation\n\nimport wolframalpha\nid_ = \"YOUR_WOLFRAMALPHA_ID\"\n##-- J'utliserai le mien que j'ai masqué\nid_ = r.id_\nclient = wolframalpha.Client(id_)\nqueries = [\"who is the president of France\",\n        \"2 times 2 times ln(2)\",\n        \"derivate xln(x)\",\n        \"integrate exponential of 2x between 2 and 4\"\n]\n\n\nans1 = client.query(queries[0])\nans1 = next(ans1.results).text\nans1\n\n'Emmanuel Macron (from 14/05/2017 to present)'\n\n\n\nans2 = client.query(queries[1])\nans2 = next(ans2.results).text\nans2\n\n'4 log(2)'\n\n\n\nans3 = client.query(queries[2])\nans3 = next(ans3.results).text\nans3\n\n'd/dx(x log(x)) = log(x) + 1'\n\n\n\nans4 = client.query(queries[3])\nans4 = next(ans4.results).text\nans4\n\n'integral_2^4 exp(2 x) dx = 1/2 e^4 (e^4 - 1)≈1463.2'\n\n\n\n\nExplication des parties techniques de votre code\nLe script commence par l’importation des bibliothèques nécessaires :\n\nimport datetime\nimport webbrowser\nimport sys\nimport pywhatkit\nimport speech_recognition as sr\nimport pyttsx3 as ttx\nimport wikipedia\nfrom googletrans import Translator\nimport wolframalpha\n\n\ndatetime : gestion des dates et heures.\nwebbrowser : ouverture des pages web.\nsys : gestion des fonctionnalités système.\npywhatkit : exécution de commandes interactives comme la recherche YouTube.\nspeech_recognition : reconnaissance vocale.\npyttsx3 : synthèse vocale.\nwikipedia : récupération d’informations depuis Wikipédia.\ngoogletrans : traduction de texte.\nwolframalpha : moteur de réponse à des questions scientifiques et mathématiques.\n\nIntaller les avec la commande :\n\nmodules = [\n    \"pywhatkit\", \"speechrecognition\", \"pyttsx3\",\n    \"wikipedia\", \"googletrans==4.0.0-rc1\", \"wolframalpha\", \"pyaudio\"\n]\n\nimport subprocess\nimport sys\ndef install_modules():\n    for module in modules:\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", module])\n        except:\n            print(\"Quelque chose s'est mal passée\")\n            \ninstall_modules()"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#configuration-du-moteur-de-synthèse-vocale",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#configuration-du-moteur-de-synthèse-vocale",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "2. Configuration du moteur de synthèse vocale",
    "text": "2. Configuration du moteur de synthèse vocale\nLe code initialise pyttsx3 et affiche les voix disponibles :\n\nmoteur = ttx.init()\nvoix_disponibles = moteur.getProperty(\"voices\")\n\nfor index, voix in enumerate(voix_disponibles):\n    print(f\"Index {index} - ID: {voix.id} - Langue: {voix.languages} - Nom: {voix.name}\")\n\nIndex 0 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0 - Langue: [] - Nom: Microsoft David Desktop - English (United States)\nIndex 1 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0 - Langue: [] - Nom: Microsoft Zira Desktop - English (United States)\nIndex 2 - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_FR-FR_HORTENSE_11.0 - Langue: [] - Nom: Microsoft Hortense Desktop - French\n\n\nEnsuite, une voix spécifique est sélectionnée et testée :\n\nmoteur.setProperty(\"voice\", voix_disponibles[2].id)\nmoteur.say(\"Bonjour, ceci est un test avec une autre voix.\")\nmoteur.runAndWait()"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#définition-de-la-classe-voxaassistant",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#définition-de-la-classe-voxaassistant",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "3. Définition de la classe voxaAssistant",
    "text": "3. Définition de la classe voxaAssistant\nLa classe voxaAssistant gère toutes les fonctionnalités de l’assistant vocal.\n\n3.1 Initialisation\n\nclass voxaAssistant:\n    def __init__(self):\n        self.ecouteur = sr.Recognizer()\n        self.moteur = ttx.init()\n        self.voix_disponibles = self.moteur.getProperty(\"voices\")\n        self.moteur.setProperty(\"voice\", self.voix_disponibles[2].id)\n        self.moteur.setProperty(\"rate\", 170)\n        self.app_id = r.id_\n        self.client = wolframalpha.Client(self.app_id)\n\nCette méthode : - Initialise le moteur de reconnaissance vocale (speech_recognition). - Configure la synthèse vocale avec pyttsx3. - Définit la clé API pour Wolfram Alpha.\n\n\n3.2 Fonction parler\nCette fonction génère une sortie vocale à partir d’un texte donné.\n\ndef parler(self, texte):\n    self.moteur.say(texte)\n    self.moteur.runAndWait()\n\n\n\n3.3 Fonction saluer\nCette fonction ajuste le message de salutation en fonction de l’heure.\n\ndef saluer(self):\n    heure_actuel = int(datetime.datetime.now().hour)\n    if 0 &lt;= heure_actuel &lt;= 12:\n        self.parler(\"Bonjour à vous Djamal\")\n    else:\n        self.parler(\"Bonsoir à vous Djamal\")"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#reconnaissance-et-traitement-des-requêtes-vocales",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#reconnaissance-et-traitement-des-requêtes-vocales",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "4. Reconnaissance et Traitement des Requêtes Vocales",
    "text": "4. Reconnaissance et Traitement des Requêtes Vocales\n\nFonction voxa_requete\nCette fonction écoute l’utilisateur et transcrit la parole en texte.\n\ndef voxa_requete(self):\n    with sr.Microphone() as parole:\n        print(\"Entrain d'écouter ...\")\n        self.ecouteur.adjust_for_ambient_noise(parole, duration=1)\n        self.ecouteur.pause_threshold = 1.5\n        try:\n            voix = self.ecouteur.listen(parole, timeout=5, phrase_time_limit=5)\n            command = self.ecouteur.recognize_google(voix, language=\"fr\").lower()\n            print(\"Vous avez dit .... : \", command)\n            return command\n        except sr.UnknownValueError:\n            print(\"Je n'ai pas compris, veuillez répéter.\")\n            return \"\"\n        except sr.RequestError:\n            print(\"Erreur avec le service de reconnaissance vocale.\")\n            return \"\"\n\n\n\nRecherche Google et YouTube\nSi l’utilisateur mentionne Google ou YouTube, la recherche est effectuée automatiquement.\n\nelif \"google\" in voix:\n    url = voix.split().index(\"google\")\n    elt_rechercher = voix.split()[url + 1:]\n    self.parler(\"D'accord, je lance la recherche\")\n    webbrowser.open(\"https://www.google.com/search?q=\" + \"+\".join(elt_rechercher), new=2)\n\n\nelif \"recherche sur youtube\" in voix or \"recherche sur youtube.com\" in voix:\n                url = voix.split().index(\"youtube\")\n                elt_rechercher = voix.split()[url + 1:]\n                self.parler(\"d'accord  je  lance  la  recherche\")\n                webbrowser.open(\n                    \"http://www.youtube.com/results?search_query=\"\n                    + \"+\".join(elt_rechercher),\n                    new=2,\n                )\n\n\n1️⃣ split() : Pourquoi l’utiliser ici ?\n\nurl = voix.split().index(\"google\")\nelt_rechercher = voix.split()[url + 1:]\n\n\nsplit() découpe une chaîne de caractères en une liste de mots.\nIci, on cherche l’index du mot “google” pour récupérer les mots suivants, qui correspondent à la requête de l’utilisateur.\n\nExemple :\n\nEntrée : \"cherche sur google c'est quoi la capitale de la France\"\n\nAprès split() : [\"cherche\",\"sur\", \"google\", \"c\", \"'\", \"est\", \"quoi\", \"la\", \"capitale\", \"de\", \"la\", \"France\"]\n\nIndex du mot “google” : 3\n\nCe qui est recherché : [\"c\", \"'\", \"est\", \"quoi\", \"la\", \"capitale\", \"de\", \"la\", \"France\"] → Ici, on devrait prendre les mots après “google”.\n\n\n\n\n2️⃣ Pourquoi y a-t-il des + dans l’URL de Google et YouTube ?\n\nwebbrowser.open(\"https://www.google.com/search?q=\" + \"+\".join(elt_rechercher), new=2)\n\n\nelif \"youtube\" in voix:\n    s = voix.replace(\"youtube\", \"\")\n    self.parler(\"D'accord sans soucis\")\n    pywhatkit.playonyt(s)\n\n\nExplication du +.\n\nDans une URL, un espace est souvent remplacé par + ou %20.\n\nExemple : Si l’utilisateur dit “recherche machine learning sur google”, on doit transformer \"machine learning\" en \"machine+learning\" pour que Google comprenne.\n\nAutre solution : \"%20\".join(elt_rechercher) aurait aussi pu être utilisé.\n\n\n\n\n\nRecherches Avancées avec Wolfram Alpha et Wikipédia\n\nUtilisation de Wolfram Alpha pour répondre aux questions générales\n\n    def question_generale(self, voix):\n        voix = self.translate_eng_fr(voix)\n        try:\n            reponse = self.client.query(voix)\n            res = next(reponse.results).text\n            res = self.translate_fr_eng(res)\n            print(\"Un instant ...\")\n            print(res)\n            self.parler(res)\n        except:\n            self.parler(\"Je n'ai pas trouvé de réponse.\")\n\n      Ici, l’assistant vocal envoie la requête à Wolfram Alpha, récupère la réponse et la traduit en français avant de la prononcer.\nSi aucune réponse n’est trouvée, une recherche est effectuée sur Wikipédia.\n\n\nUtilisation de wikipedia pour répondre aux questions générales\n\ntry:\n    wikipedia.set_lang(\"fr\")\n    info = wikipedia.summary(voix, 1)\n    self.parler(str(info))\nexcept:\n    self.parler(\"Je n'ai pas bien compris\")\n\n\n\n3️⃣ query : À quoi ça sert dans Wolfram Alpha*\n\nreponse = self.client.query(voix)\nres = next(reponse.results).text\n\n\n.query(voix) : envoie la question de l’utilisateur à Wolfram Alpha.\n\nnext(reponse.results).text : récupère la première réponse retournée et extrait le texte.\n\nSi Wolfram Alpha trouve une réponse pertinente, elle est lue à haute voix.\n\n\n\n\nRésumé des concepts clés :\n\n\n\n\n\n\n\nÉlément\nExplication\n\n\n\n\nWolfram Alpha\nMoteur de calcul intelligent répondant à des requêtes scientifiques et analytiques\n\n\nsplit()\nDécoupe une phrase en liste de mots\n\n\nquery()\nEnvoie une requête à Wolfram Alpha\n\n\njoin(“+”)\nTransforme une liste de mots en requête lisible par un moteur de recherche"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#test-du-code",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#test-du-code",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "Test du code",
    "text": "Test du code"
  },
  {
    "objectID": "INFO_MINI_PROJETS/assistant_virtuel.html#conclusion",
    "href": "INFO_MINI_PROJETS/assistant_virtuel.html#conclusion",
    "title": "Djamaldbz - Crée ton assistant virtuel en python !!!",
    "section": "Conclusion",
    "text": "Conclusion\nCe code met en place un assistant vocal capable de reconnaître et d’exécuter des commandes vocales en français, d’effectuer des recherches sur le web, et de répondre aux questions grâce à Wolfram Alpha et Wikipédia. Il constitue une base quelque peu solide pour un assistant personnel plus ou moins intelligent.\nTélécharger le fichier .python\nTélécharger la vidéo\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour à la page d’accueuil"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "",
    "text": "Comment cela fonctionne\n\n\n\nJe tiens tout d’abord à rapperler que je n’utilise pas de modèle NLP pour créer ce petit assistant virtuel. J’avais écris ce progamme en 2021, donc bien evidemment les outils utilisés ont évolué et donc vous pourrez l’ajuster à votre guise. Le code source sera téléchargeable à la fin de la page."
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html#la-svm-cest-quoi-et-à-quoi-ça-sert",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html#la-svm-cest-quoi-et-à-quoi-ça-sert",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "LA SVM : C’est quoi et à quoi ça sert ?",
    "text": "LA SVM : C’est quoi et à quoi ça sert ?\n\nUtilité :\n\n\nPrincipe de la SVM pour la classification binaire"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html#test-du-code",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html#test-du-code",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "Test du code",
    "text": "Test du code"
  },
  {
    "objectID": "INFO_MINI_PROJETS/classification_binaire_svm.html#conclusion",
    "href": "INFO_MINI_PROJETS/classification_binaire_svm.html#conclusion",
    "title": "Djamaldbz - Classification binaire en utilisant la SVM !!!",
    "section": "Conclusion",
    "text": "Conclusion\nCe code met en place un assistant vocal capable de reconnaître et d’exécuter des commandes vocales en français, d’effectuer des recherches sur le web, et de répondre aux questions grâce à Wolfram Alpha et Wikipédia. Il constitue une base quelque peu solide pour un assistant personnel plus ou moins intelligent.\nTélécharger le fichier .python\nTélécharger la vidéo\nSi vous avez des questions, vous pouvez me contacter !!!\nRetour à la page d’accueuil"
  }
]