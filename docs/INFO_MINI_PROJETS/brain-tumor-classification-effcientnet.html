<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Djamal TOE">
<meta name="dcterms.date" content="2025-06-11">

<title>DJAMAL WEBSITE - Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="index.qmd" class="navbar-brand navbar-brand-logo">
    <img src="../images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="index.qmd">
    <span class="navbar-title">DJAMAL WEBSITE</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-statistics--machine-learning" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Statistics &amp; Machine Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-statistics--machine-learning">    
        <li>
    <a class="dropdown-item" href="../INFO_MINI_PROJETS/classi_bin_acp_kmeans_knn_logit/Breast-Tumor-Article.html" rel="" target="">
 <span class="dropdown-text">Diagnostic assisté par ACP et régression logistique</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../INFO_MINI_PROJETS/brain-tumor-classification-effcientnet.html" rel="" target="">
 <span class="dropdown-text">Diagnostic tumeurs cérébrales - Reseau de neurones</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ANALYSES_FACTORIELLES/acp-kmeans.html" rel="" target="">
 <span class="dropdown-text">Reduction de dimensionnalité et clustering non supervisé</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../FORMATIONS/logistic_regression_diabetes.html" rel="" target="">
 <span class="dropdown-text">Modélisation des données à variables dépendantes binaires</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../projet-traitement-donnees/report_writing/synthese-des-travaux.html" rel="" target="">
 <span class="dropdown-text">Prédire la durée de carrière des joueurs NBA</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../INFO_MINI_PROJETS/anomaly-detection-in-transactions-GMM.html" rel="" target="">
 <span class="dropdown-text">Détection d’anomalies dans les transactions à l’aide de modèles de mélange gaussien</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../FORMATIONS/poisson_paludisme.html" rel="" target="">
 <span class="dropdown-text">Modélisation des données de comptage</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../INFO_MINI_PROJETS/shifumi-cnn-yolov8.html" rel="" target="">
 <span class="dropdown-text">Classification des gestes de la main avec Yolo et CNN</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-programming" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Programming</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-programming">    
        <li>
    <a class="dropdown-item" href="../INFO_MINI_PROJETS/assistant_virtuel.html" rel="" target="">
 <span class="dropdown-text">Crée ton assistant virtuel avec pyhton</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../INFO_MINI_PROJETS/JavaApp/desktop-app-java-mysql.html" rel="" target="">
 <span class="dropdown-text">Application desktop avec Java et Mysql</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-various" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Various</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-various">    
        <li>
    <a class="dropdown-item" href="../FORMATIONS/machine-learning/gradient-descent-linear-reg.html" rel="" target="">
 <span class="dropdown-text">Régression linéaire par descente de gradient - théorie et application</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../publications.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About me</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://www.linkedin.com/in/djamal-toe-7a18432b0" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text">LinkedIn</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://https://github.com/Djamal029" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#résumé" id="toc-résumé" class="nav-link" data-scroll-target="#résumé">Résumé</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#méthodologie" id="toc-méthodologie" class="nav-link" data-scroll-target="#méthodologie">Méthodologie</a>
  <ul class="collapse">
  <li><a href="#source-des-données" id="toc-source-des-données" class="nav-link" data-scroll-target="#source-des-données">Source des données</a></li>
  <li><a href="#traitement-des-images" id="toc-traitement-des-images" class="nav-link" data-scroll-target="#traitement-des-images">Traitement des images</a></li>
  <li><a href="#modèle-utilisé" id="toc-modèle-utilisé" class="nav-link" data-scroll-target="#modèle-utilisé">Modèle utilisé</a></li>
  <li><a href="#evaluation-du-modèle" id="toc-evaluation-du-modèle" class="nav-link" data-scroll-target="#evaluation-du-modèle">Evaluation du modèle</a></li>
  </ul></li>
  <li><a href="#résultats" id="toc-résultats" class="nav-link" data-scroll-target="#résultats">Résultats</a>
  <ul class="collapse">
  <li><a href="#echantillons-des-images-téléchargées" id="toc-echantillons-des-images-téléchargées" class="nav-link" data-scroll-target="#echantillons-des-images-téléchargées">Echantillons des images téléchargées</a></li>
  <li><a href="#résultats-et-validation-du-modèle" id="toc-résultats-et-validation-du-modèle" class="nav-link" data-scroll-target="#résultats-et-validation-du-modèle">Résultats et validation du modèle</a></li>
  <li><a href="#discussions-des-résultats" id="toc-discussions-des-résultats" class="nav-link" data-scroll-target="#discussions-des-résultats">Discussions des résultats</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#annexes" id="toc-annexes" class="nav-link" data-scroll-target="#annexes">Annexes</a></li>
  <li><a href="#annexes-1-data-augmentation" id="toc-annexes-1-data-augmentation" class="nav-link" data-scroll-target="#annexes-1-data-augmentation">Annexes 1 : Data augmentation</a></li>
  <li><a href="#annexe-2-construction-du-modèle" id="toc-annexe-2-construction-du-modèle" class="nav-link" data-scroll-target="#annexe-2-construction-du-modèle">Annexe 2 : Construction du modèle</a></li>
  <li><a href="#annexe-3-détails-de-calculs-des-métriques" id="toc-annexe-3-détails-de-calculs-des-métriques" class="nav-link" data-scroll-target="#annexe-3-détails-de-calculs-des-métriques">Annexe 3: Détails de calculs des métriques</a></li>
  <li><a href="#moyennes-globales" id="toc-moyennes-globales" class="nav-link" data-scroll-target="#moyennes-globales">Moyennes globales</a></li>
  <li><a href="#liste-des-sigles-et-abréviations" id="toc-liste-des-sigles-et-abréviations" class="nav-link" data-scroll-target="#liste-des-sigles-et-abréviations">LISTE DES SIGLES ET ABRÉVIATIONS</a></li>
  <li><a href="#references-bibliographiques" id="toc-references-bibliographiques" class="nav-link" data-scroll-target="#references-bibliographiques">REFERENCES BIBLIOGRAPHIQUES</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Classification des tumeurs cérébrales à partir d’IRM : Modélisation et évaluation</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Djamal TOE </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 11, 2025</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>Accurate classification of brain tumors from magnetic resonance imaging (<code>MRI</code>) is essential for guiding therapeutic decisions and improving patient outcomes. In this study, we propose a deep learning approach based on transfer learning with the pre-trained <code>EfficientNetB5 convolutional neural network</code>. A dataset of <code>6 012</code> T1-weighted MR images comprising <code>gliomas</code> (2 004 images), <code>meningiomas</code> (2 004 images), and <code>other tumor types</code> (2 004 images) was split into <strong>training (70 %)</strong>, <strong>validation (15 %)</strong>, and <strong>test (15 %) sets</strong>. Images were resized to <strong>224 × 224 pixels</strong>, <strong>normalized</strong>, and <strong>augmented</strong> through random rotations (±15°), zooms (±20 %), and horizontal flips. EfficientNetB5’s convolutional base was frozen up to layer 95, and a custom classifier head (GlobalAveragePooling2D → Dense(128, ReLU) → Dense(3, softmax)) was fine-tuned using the Adam optimizer (learning rate 1 × 10⁻⁵) with early stopping on validation loss.</p>
<p>On the independent test set (907 images), our fine-tuned EfficientNetB5 achieved an <code>overall accuracy of 99.78 %</code>, with only three misclassifications (<code>2/906</code>). Per-class metrics were as follows:</p>
<ul>
<li><p><code>Glioma</code>: <strong>Precision 99,67 %</strong>, <strong>Recall 100 %</strong>, <strong>F1-score 99,83 %</strong> (support 302)</p></li>
<li><p><code>Meningioma</code>: <strong>Precision 100 %</strong>, <strong>Recall 99,34 %</strong>, <strong>F1-score 99,67 %</strong> (support 302)</p></li>
<li><p><code>Other tumors</code>: <strong>Precision 99,67 %</strong>, <strong>Recall 100 %</strong>, <strong>F1-score 99,83 %</strong> (support 302)</p></li>
</ul>
<hr>
</section>
<section id="résumé" class="level2">
<h2 class="anchored" data-anchor-id="résumé">Résumé</h2>
<p>La classification précise des tumeurs cérébrales à partir de l’imagerie par résonance magnétique (IRM) est essentielle pour guider les décisions thérapeutiques et améliorer les résultats pour les patients. Dans cette étude, nous proposons une approche d’apprentissage profond basée sur l’apprentissage par transfert avec le réseau neuronal convolutionnel <code>EfficientNetB5</code> pré-entraîné. Un ensemble de données de <code>6 012</code> images RM pondérées en T1 comprenant des <code>gliomes</code> (2 004 images), des <code>méningiomes</code> (2 004 images), et des <code>autres types de tumeurs</code> (2 004 images) a été divisé en <strong>ensembles d’entraînement (70 %)</strong>, <strong>ensembles de validation (15 %)</strong>, et <strong>ensembles de test (15 %)</strong>. Les images ont été redimensionnées à <strong>224 × 224 pixels</strong>, <strong>normalisées</strong> et <strong>augmentées</strong> par des rotations aléatoires (±15°), des zooms (±20 %) et des retournements horizontaux. La base convolutive d’EfficientNetB5 a été gelée jusqu’à la couche 95, et une tête de classificateur personnalisée (GlobalAveragePooling2D → Dense(128, ReLU) → Dense(3, softmax)) a été affinée à l’aide de l’optimiseur Adam (taux d’apprentissage 1 × 10-⁵) avec arrêt anticipé sur la perte de validation.</p>
<p>Sur l’ensemble de test indépendant (907 images), notre EfficientNetB5 affiné a atteint une <code>précision globale de 99,78 %</code>, avec seulement trois erreurs de classification (<code>2/906</code>). Les mesures par classe sont les suivantes :</p>
<ul>
<li><p><code>Gliome</code> : <strong>Précision 99,67 %</strong>, <strong>Rappel 100 %</strong>, <strong>F1-score 99,83 %</strong> (support 302)</p></li>
<li><p>Méningiome` : <strong>Précision 100 %</strong>, <strong>Rappel 99,34 %</strong>, <strong>Score F1 99,67 %</strong> (support 302)</p></li>
<li><p><code>Autres tumeurs</code> : <strong>Précision 99,67 %</strong>, <strong>Rappel 100 %</strong>, <strong>F1-score 99,83 %</strong> (support 302)</p></li>
</ul>
<hr>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Les tumeurs cérébrales représentent un enjeu majeur de santé publique en raison de leur complexité diagnostique et de leurs implications cliniques graves. Classifier précisément ces tumeurs, notamment les méningiomes, les gliomes et les tumeurs hypophysaires, est essentiel pour guider les décisions thérapeutiques et améliorer le pronostic des patients <span class="citation" data-cites="WHO2021">(<a href="#ref-WHO2021" role="doc-biblioref"><strong>WHO2021?</strong></a>)</span>.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Selon la 5e édition de la classification de l’Organisation Mondiale de la Santé (<code>OMS</code>), une approche intégrée reposant à la fois sur des critères histopathologiques et moléculaires est désormais recommandée pour le diagnostic des tumeurs du système nerveux central <span class="citation" data-cites="WHO2021">(<a href="#ref-WHO2021" role="doc-biblioref"><strong>WHO2021?</strong></a>)</span>. Cependant, l’interprétation des images médicales, en particulier des <code>IRM</code> cérébrales, reste un défi complexe et chronophage pour les professionnels de santé. Dans ce contexte, les méthodes d’intelligence artificielle, notamment les réseaux de neurones convolutifs (<code>CNN</code>), ont montré un potentiel prometteur pour automatiser la classification des tumeurs à partir d’images <code>IRM</code>.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Une revue menée par Xie et al. <span class="citation" data-cites="Xie2022">(<a href="#ref-Xie2022" role="doc-biblioref">Xie et al. 2022</a>)</span> souligne les avancées récentes dans l’application des <code>CNN</code> à la classification des tumeurs cérébrales, en insistant sur les défis techniques rencontrés comme le surapprentissage, le déséquilibre des classes, ou encore la nécessité d’intégrer la classification moléculaire. D’autres travaux, tels que celui de Rasheed et al. <span class="citation" data-cites="Rasheed2023">(<a href="#ref-Rasheed2023" role="doc-biblioref">Rasheed et al. 2023</a>)</span>, proposent un modèle <code>CNN</code> personnalisé pour différencier automatiquement les <code>IRM</code> de trois types de tumeurs avec une grande précision, tout en mettant en avant l’importance du prétraitement des images pour améliorer la performance du modèle.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;En parallèle, Tummala et al. <span class="citation" data-cites="Tummala2022">(<a href="#ref-Tummala2022" role="doc-biblioref">Tummala et al. 2022</a>)</span> introduisent une approche combinée utilisant les transformeurs visuels (Vision Transformers, ViT) avec les <code>CNN</code> pour augmenter la robustesse et la précision du modèle, démontrant ainsi la pertinence des modèles hybrides. Dans le même esprit, Srinivasan et al. <span class="citation" data-cites="Srinivasan2024">(<a href="#ref-Srinivasan2024" role="doc-biblioref">Srinivasan et al. 2024</a>)</span> conçoivent un modèle profond et hybride adapté à la classification multi-classes, en combinant plusieurs architectures <code>CNN</code> avec des stratégies d’optimisation.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dans cette étude, nous proposons de développer un modèle basé sur un réseau de neurones convolutif (<code>CNN</code>) pour classifier les <code>IRM</code> cérébrales en trois types de tumeurs : <code>gliomes</code>, <code>méningiomes</code> et <code>tumeurs hypophysaires</code>. Cette approche vise à fournir un outil efficace d’aide au diagnostic, en s’appuyant sur les méthodes récentes les plus performantes issues de la littérature.</p>
<hr>
</section>
<section id="méthodologie" class="level2">
<h2 class="anchored" data-anchor-id="méthodologie">Méthodologie</h2>
<section id="source-des-données" class="level3">
<h3 class="anchored" data-anchor-id="source-des-données">Source des données</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Les données ont été téléchargées sous forme d’images depuis la plateforme <code>Kaggle</code> (en accès libre <a href="https://www.kaggle.com/datasets/orvile/brain-cancer-mri-dataset">Cliquez ici pour accéder à la page</a>). Elles sont réparties en trois sous-groupes :</p>
<ul>
<li><p><code>brain_menin</code> (<strong><em>2004 images</em></strong>) pour la <strong>méningiome</strong>, une tumeur généralement bénigne des méninges (les membranes entourant le cerveau);</p></li>
<li><p><code>brain_glioma</code> (<strong><em>2004 images</em></strong>) pour le <strong>gliome</strong>, une tumeur maligne issue des cellules gliales, souvent infiltrante et agressive;</p></li>
<li><p><code>brain_tumor</code> (<strong><em>2048 images</em></strong>) pour, éventuellement, les <strong>autres types de tumeurs cérébrales</strong>, souvent malignes, incluant diverses localisations et origines cellulaires.</p></li>
</ul>
<p>Au total, la base de données contient donc <strong>6056 images</strong>.</p>
</section>
<section id="traitement-des-images" class="level3">
<h3 class="anchored" data-anchor-id="traitement-des-images">Traitement des images</h3>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p><strong>Gestion des doublons</strong></p>
</blockquote>
</blockquote>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Il est crucial, avant d’entraîner un modèle sur une base de données d’images, de vérifier qu’elle ne contient pas d’images en double. Cela permet d’éviter de nombreux problèmes, notamment une évaluation faussée du modèle. Après vérification, nous avons identifié <code>44 doublons</code> dans la base de données. Ceux-ci ont donc été supprimés de la base locale, ce qui porte le nombre total d’images à <code>6012</code> au lieu de <code>6056</code>.</p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p><strong>Partionnement des données</strong></p>
</blockquote>
</blockquote>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Avant de commencer la phase de classification, les images ont été réparties aléatoirement dans trois répertoires selon les proportions suivantes:</p>
<ul>
<li><strong>train</strong> : <strong>70 %</strong> des images (entraînement)</li>
<li><strong>val</strong> : <strong>15 %</strong> des images (validation)</li>
<li><strong>test</strong> : <strong>15 %</strong> des images (test)</li>
</ul>
<p>Plus explicitement :</p>
<ul>
<li>Le dossier <code>train</code> sert à <strong>entraîner</strong> le modèle.</li>
<li>Le dossier <code>val</code> est utilisé pour <strong>valider</strong> le modèle à chaque itération, ce qui permet d’ajuster les paramètres et de <strong>minimiser</strong> le score de perte (calculé à partir de la fonction de perte/fonction objective) grâce à l’optimiseur (ici, <strong>Adam</strong>).</li>
<li>Le dossier <code>test</code> permet d’évaluer la performance finale du modèle sur des données qu’il n’a jamais vues.</li>
</ul>
<p>Chaque répertoire contient les trois classes de tumeurs cérébrales: <code>brain_menin</code>, <code>brain_tumor</code> et <code>brain_glioma</code> que les données nous fournissaient.</p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p><strong>Verification des doublons dans les différents dossiers (Train/Val/Test)</strong></p>
</blockquote>
</blockquote>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Après avoir réparti les images dans les différents dossiers, nous avons effectué un test supplémentaire afin de vérifier qu’aucune image ne se retrouvait à la fois dans les ensembles <code>train</code>, <code>val</code> ou <code>test</code>, que ce soit en double ou dans plusieurs ensembles simultanément. Les vérifications ont confirmé que tout était en ordre.</p>
<p>Cela a permis de s’assurer que chaque dossier contient des images uniques, garantissant ainsi une séparation stricte des données pour un entraînement, une validation et un test fiables du modèle.</p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p><strong>Préparation et chargement des images</strong></p>
</blockquote>
</blockquote>
</blockquote>
<p>Pour l’entraînement et la validation, les images sont traitées à l’aide de générateurs Keras (<code>ImageDataGenerator</code>):</p>
<ol type="1">
<li><strong>Dimensionnement</strong> : toutes les images sont redimensionnées à <strong>224×224 pixels</strong>, taille d’entrée standard pour de nombreux réseaux pré-entraînés.</li>
<li><strong>Batch size</strong> : on fixe le nombre d’images traitées simultanément à chaque pas d’entraînement.</li>
<li><strong>Data augmentation</strong> :
<ul>
<li><strong>Entraînement</strong> :
<ul>
<li>Normalisation des pixels : passage de l’échelle [0, 255] à l’échelle [0,1]</li>
<li>Rotation aléatoire jusqu’à ±15° (<code>rotation_range=15</code>)</li>
<li>Zoom aléatoire jusqu’à 20 % (<code>zoom_range=0.2</code>)</li>
<li>Flip horizontal aléatoire (<code>horizontal_flip=True</code>)</li>
</ul></li>
<li><strong>Validation et test</strong> :
<ul>
<li>Seule la normalisation des pixels (de [0, 255] à [0, 1]), afin d’évaluer le modèle sur des images aux orientations et échelles réelles.</li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="modèle-utilisé" class="level3">
<h3 class="anchored" data-anchor-id="modèle-utilisé">Modèle utilisé</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pour répondre à la problématique de classification des tumeurs cérébrales à partir d’images, nous avons opté pour l’utilisation d’un réseau de neurones convolutifs (<code>CNN</code>). Plutôt que de construire un modèle à partir de zéro — ce qui aurait été risqué compte tenu de la taille relativement modeste du jeu de données et des ressources de calcul disponibles —, nous avons choisi de recourir à une approche de transfert d’apprentissage.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Plus précisément, nous avons utilisé le modèle <code>EfficientNetB5</code>, un <code>CNN</code> préentraîné sur le vaste ensemble de données ImageNet. Ce modèle présente un excellent compromis entre performance, rapidité et taille du modèle, ce qui le rend particulièrement adapté pour des tâches de classification d’images médicales où les ressources peuvent être limitées.</p>
<p>Dans le cadre de cette approche :</p>
<ul>
<li><p>Les couches convolutionnelles profondes du modèle ont été conservées pour exploiter leur capacité à extraire des caractéristiques visuelles de bas niveau (bords, textures, formes, etc.);</p></li>
<li><p>Les couches supérieures (à partir de la <code>95e</code> couche dans notre cas) ont été désactivées (non gelées) et réentraînées sur notre propre base de données, afin d’adapter le modèle aux spécificités des tumeurs cérébrales.</p></li>
</ul>
<p>Cette technique permet de bénéficier des connaissances générales acquises par le modèle tout en l’adaptant finement à notre problème spécifique. En effet, les modèles préentraînés comme EfficientNet ne sont pas directement adaptés aux tâches ciblées des data scientists. Il est donc crucial de les affiner (<code>fine-tuning</code>) sur des données spécifiques pour améliorer leur capacité à détecter des motifs propres au domaine médical, tels que les contours et anomalies propres aux IRM cérébrales.</p>
<p>Enfin, construire un réseau de neurones entièrement personnalisé aurait pu exposer notre solution à des risques de surapprentissage ou à des difficultés d’optimisation, sans compter les contraintes computationnelles qui auraient ralenti considérablement le processus.</p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p><strong>Construction du modèle avec EfficientNetB5</strong></p>
</blockquote>
</blockquote>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pour la phase de modélisation, nous avons utilisé le modèle <strong>EfficientNetB5</strong>, préentraîné sur <code>ImageNet</code>. Ce modèle est particulièrement performant pour la classification d’images complexes et convient bien à des tâches médicales exigeantes en précision.</p>
<p>Nous avons chargé EfficientNetB5 sans ses couches de sortie (paramètre <code>include_top=False</code>) afin de pouvoir personnaliser l’architecture en sortie. L’entrée du modèle est spécifiée avec la taille <strong>(224, 224, 3) et le 3 correspond aux cannaux de couleurs (RGB : Rouge-Vert-Bleu)</strong> correspondant à nos images redimensionnées. Cependant <code>IRM</code> sont affichées en niveaux de gris souvent pour mieux visualiser les structure de cerveau. Or <code>EfficientNet</code> attend normalement des images de la forme <code>(3, H, W)</code> et les images de niveaux gris sont de la forme <code>(1, H, W)</code>. On serait donc tenté de les convertir en “faux RGB” (3 canaux identiques). Toutefois, le mode des images a été vérifié et celles-ci sont bien en <code>RGB</code>. Elle sont en noires blancs, mais elles ont trois cannaux et chaque canal contiendrait les mêmes valeurs ou une version identique. Ainsi les images ont été laissées telles quelles.</p>
<p>Nous avons ensuite :</p>
<ul>
<li><p><strong>Gelé les poids du modèle préentraîné</strong> pour ne pas altérer les connaissances acquises sur ImageNet lors d’un premier entraînement ;</p></li>
<li><p>Ajouté un <strong>GlobalAveragePooling2D</strong>, qui réduit la dimensionnalité tout en conservant les caractéristiques importantes ;</p></li>
<li><p>Ajouté une couche dense de <strong>128 neurones avec la fonction d’activation ReLU</strong> ;</p></li>
<li><p>Et enfin une <strong>couche de sortie avec 3 neurones</strong>, activée par une fonction <code>softmax</code> pour la classification des trois types de tumeurs : <em>méningiome</em>, <em>gliome</em> et <em>autres tumeurs cérébrales</em>.</p></li>
</ul>
<p>Le modèle a été compilé avec :</p>
<ul>
<li><p>L’<strong>optimiseur Adam</strong>, très utilisé pour sa rapidité de convergence,</p></li>
<li><p>Une <strong>taux d’apprentissage très faible (0.00001)</strong> pour éviter les grandes variations de poids à cause du gel partiel,</p></li>
<li><p>La <strong>fonction de perte <code>categorical_crossentropy</code></strong>, adaptée à une classification multiclasse,</p></li>
<li><p>Et comme métrique de performance : <strong>l’accuracy</strong>.</p></li>
</ul>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p><strong>Phase de fine-tuning (dégel progressif)</strong></p>
</blockquote>
</blockquote>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pour mieux adapter le modèle aux spécificités de nos données, nous avons procédé à un fine-tuning partiel :</p>
<ul>
<li><p>Le modèle a été rendu entièrement entraînable (<code>base_model.trainable = True</code>) ;</p></li>
<li><p>Afin d’éviter une modification brutale des poids et une possible dégradation des performances, les <code>95 premières couches</code> ont été gelées, et seules les couches à partir de la 96e ont été entraînées. Cette technique permet au modèle de conserver ses caractéristiques basiques tout en affinant ses couches supérieures pour s’adapter à notre tâche spécifique ;</p></li>
<li><p>Le modèle a été compilé avec le même taux d’apprentissage très faible (<code>1e-5</code>) afin de permettre une phase de fine-tuning progressive, en évitant des modifications brusques des poids et en assurant une convergence stable ;</p></li>
<li><p>Un entraînement initialement prévu sur <code>50 époques</code> a été lancé, avec des <code>callback</code> :</p>
<ul>
<li><p><strong>EarlyStopping</strong> (monitor=<code>val_loss</code>, <code>patience=3</code>, <code>restore_best_weights=True</code>) pour arrêter automatiquement l’apprentissage au <strong>meilleur point</strong> de validation (après trois époques consécutifs sans que la valeur du score de perte de la validation ne soit inférieuer à sa plus pétite valeure), et un <strong>ModelCheckpoint</strong> pour sauvegarder le modèle de <code>val_loss</code> minimal.</p></li>
<li><p><strong>ReduceLROnPlateau</strong> : un scheduler (planificateur) de taux d’apprentissage qui réduit le learning rate lorsque la performance du modèle ne s’améliore plus après un certain nombre d’époques (patience = 3)</p></li>
</ul></li>
</ul>
</section>
<section id="evaluation-du-modèle" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-du-modèle">Evaluation du modèle</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Une fois l’entraînement terminé, le modèle sera évalué sur un jeu de données de <strong>test indépendant</strong> afin de mesurer sa capacité à classer correctement les différentes classes de tumeurs cérébrales. Cette évaluation repose sur plusieurs métriques standard qui quantifient la performance du modèle en termes de justesse, précision et rappel.</p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>Définitions des métriques de classification</p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li><strong><code>TP</code></strong> (<code>Vrais positifs</code>) : nombre d’images bien classées dans leur vraie classe.<br>
</li>
<li><strong><code>FP</code></strong> (Faux positifs) : nombre d’images mal classées dans cette classe alors qu’elles n’y appartiennent pas.<br>
</li>
<li><strong><code>FN</code></strong> (<code>Faux négatifs</code>) : nombre d’images appartenant à cette classe mais mal classées dans une autre.<br>
</li>
<li><strong><code>TN</code></strong> (<code>Vrais négatifs</code>) : nombre d’images bien exclues de cette classe.</li>
</ul>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>Formules</p>
</blockquote>
</blockquote>
</blockquote>
<p><span class="math display">\[
\text{Accuracy} = \frac{TP + TN}{TP + FP + FN + TN}
\]</span></p>
<p><span class="math display">\[
\text{Precision} = \frac{TP}{TP + FP} \quad
\]</span></p>
<p><span class="math display">\[
\text{Recall} = \frac{TP}{TP + FN} \quad
\]</span></p>
<p><span class="math display">\[
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]</span></p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>Correspondance des classes</p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li><strong>Classe 1</strong> : <code>Gliome</code><br>
</li>
<li><strong>Classe 2</strong> : <code>Méningiome</code><br>
</li>
<li><strong>Classe 3</strong> : <code>Autre tumeur</code></li>
</ul>
<hr>
</section>
</section>
<section id="résultats" class="level2">
<h2 class="anchored" data-anchor-id="résultats">Résultats</h2>
<section id="echantillons-des-images-téléchargées" class="level3">
<h3 class="anchored" data-anchor-id="echantillons-des-images-téléchargées">Echantillons des images téléchargées</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Les images ci-dessous sont des échantillons de celles qui serviront de base pour l’entraînement, la validation et le test du modèle. Celles affichées sont choisies aléatoirement dans au sein de chaque classe.</p>
<ul>
<li><strong><code>Tumeur méningiome</code></strong></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-menin" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="data_brain_tumor/menin.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Echantillons des images de la méningiome collectées</figcaption>
</figure>
</div>
</div>
</div>
<ul>
<li><strong><code>Tumeur gliome</code></strong></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-giom" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="data_brain_tumor/gioma.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Echantillons des images de la gliome collectées</figcaption>
</figure>
</div>
</div>
</div>
<ul>
<li><strong><code>Autres types de tumeurs</code></strong></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-tm" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="data_brain_tumor/tumor.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Echantillons des images des autres types de tumeurs collectées</figcaption>
</figure>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A l’oeil nu, il m’est personnellement impossible de pouvoir classer ses images sur la base de critères solides.</p>
</section>
<section id="résultats-et-validation-du-modèle" class="level3">
<h3 class="anchored" data-anchor-id="résultats-et-validation-du-modèle">Résultats et validation du modèle</h3>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>Evolution des pertes et des exacatidues durant l’entraînement</p>
</blockquote>
</blockquote>
</blockquote>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-evolution-training-validation" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="data_brain_tumor/validation_train_metrics.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Évolution de l’exactitude et de la perte pendant l’entraînement</figcaption>
</figure>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Le modèle s’est arrêté à l’<code>époque 44</code>, mais les meilleurs poids (paramètres) ont été restaurés à l’<code>époque 41</code>, conformément au mécanisme d’<code>early stopping</code> avec une <code>patience fixée à 3</code>. Sur la figure ci-dessus, on observe qu’au cours des premières époques (environ jusqu’à la troisième), l’<strong>exactitude de l’entraînement</strong> (courbe verte) était légèrement <strong>supérieure</strong> à celle de la <strong>validation</strong> (courbe bleue). Toutefois, entre la quatrième et la onzième époque, l’<strong>exactitude de validation</strong> a dépassé de manière notable celle de l’entraînement. Cette progression peut s’expliquer par l’ajustement progressif des poids du modèle, sous l’effet de l’optimiseur, qui améliore les performances globales, y compris sur les données de validation.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sur la suite de l’entraînement, on constate que l’<strong>exactitude de validation</strong> reste globalement <strong>légèrement supérieure</strong> à celle de l’entraînement, tandis que la <strong>perte de validation</strong> (courbe orange) reste <strong>plus basse</strong> que la perte d’entraînement (courbe rouge). Ce comportement, bien que contre-intuitif, peut s’expliquer par un ensemble de validation plus homogène ou moins bruité, ou encore par des effets de régularisation implicites induits par la structure du modèle ou les callbacks utilisés.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Enfin, à partir de la trentième époque environ, toutes les courbes se stabilisent autour de valeurs proches de <code>1</code> pour les exactitudes, et un peu proches de <code>0</code> pour les pertes, ce qui témoigne d’une excellente capacité de généralisation du modèle <strong>sans signe apparent de surapprentissage</strong> (<strong>overfitting</strong>).</p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>Matrice de confusion (test)</p>
</blockquote>
</blockquote>
</blockquote>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-cm" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="data_brain_tumor/cm.png" style="width:90.0%;height:80.0%" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Matrice de confusion</figcaption>
</figure>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La matrice de confusion montre une excellente performance du modèle, avec seulement <strong>trois erreurs de classification</strong> sur <strong>904 images</strong> de test. Le modèle atteint un <strong>rappel parfait (100 %)</strong> pour les classes <em>gliome</em> et <em>autres tumeurs</em>, et une <strong>précision parfaite (100 %)</strong> pour la classe <em>méningiome</em>.</p>
<p>Les <strong>F1-scores</strong> dépassent <strong>99 %</strong> dans chaque cas, confirmant une capacité remarquable à différencier les types de tumeurs cérébrales.</p>
<p>Ces résultats témoignent d’un modèle bien entraîné, capable de généraliser efficacement sur des données de validation, même dans un contexte de classification multiclasse sensible comme celui des diagnostics de tumeurs cérébrales.</p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>Résultats par classe</p>
</blockquote>
</blockquote>
</blockquote>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-metrics-simple" class="anchored">
<table data-quarto-postprocess="true" class="table table-sm table-striped small">
<caption>Table&nbsp;1: Métriques en pourentage par classe</caption>
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Classe</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Précision</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Rappel</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">F1.score</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Gliome</td>
<td style="text-align: right;">99.67</td>
<td style="text-align: right;">100.00</td>
<td style="text-align: right;">99.83</td>
<td style="text-align: right;">302</td>
</tr>
<tr class="even">
<td style="text-align: left;">Méningiome</td>
<td style="text-align: right;">100.00</td>
<td style="text-align: right;">99.34</td>
<td style="text-align: right;">99.67</td>
<td style="text-align: right;">302</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Autre tumeur</td>
<td style="text-align: right;">99.67</td>
<td style="text-align: right;">100.00</td>
<td style="text-align: right;">99.83</td>
<td style="text-align: right;">302</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-metrics-globale" class="anchored">
<table data-quarto-postprocess="true" class="table table-sm table-striped small">
<caption>Table&nbsp;2: Métriques globales en pourentage</caption>
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Métrique</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Précision</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Rappel</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">F1.score</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Exactitude (Accuracy)</td>
<td style="text-align: right;">99.78</td>
<td style="text-align: right;">99.78</td>
<td style="text-align: right;">99.78</td>
<td style="text-align: right;">906</td>
</tr>
<tr class="even">
<td style="text-align: left;">Moyenne macro</td>
<td style="text-align: right;">99.67</td>
<td style="text-align: right;">99.67</td>
<td style="text-align: right;">99.67</td>
<td style="text-align: right;">906</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Moyenne pondérée</td>
<td style="text-align: right;">99.67</td>
<td style="text-align: right;">99.67</td>
<td style="text-align: right;">99.67</td>
<td style="text-align: right;">906</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p><strong>Interprétation des métriques</strong></p>
</blockquote>
</blockquote>
</blockquote>
<p>Les résultats obtenus montrent que le modèle de classification des tumeurs cérébrales atteint une excellente performance sur l’ensemble de <code>test</code>, avec des scores de <strong>précision</strong>, <strong>rappel</strong> et <strong>F1-score</strong> supérieurs à <strong>99 %</strong> pour chaque classe.</p>
<ul>
<li><p><strong>Classe 1 (<code>Gliome</code>)</strong> :</p>
<ul>
<li><p>La <strong>précision</strong> de <strong>99,67 %</strong> indique que lorsque le modèle prédit un gliome, il se trompe très rarement (environ <em>0,33 %</em> de faux positifs).</p></li>
<li><p>Le <strong>rappel</strong> parfait à <strong>100 %</strong> signifie que <em>toutes</em> les images de gliome ont été correctement détectées, sans <em>aucun</em> faux négatif.</p></li>
<li><p>Le <strong>F1-score</strong> élevé de <strong>99,83 %</strong> traduit un excellent compromis entre précision et rappel, assurant une classification fiable pour cette classe.</p></li>
</ul></li>
<li><p><strong>Classe 2 (<code>Méningiome</code>)</strong> :</p>
<ul>
<li><p>Une <strong>précision</strong> parfaite de <strong>100 %</strong> montre que <em>toutes</em> les images classées comme méningiome sont effectivement correctes, sans aucun faux positif.</p></li>
<li><p>Un <strong>rappel</strong> de <strong>99,34 %</strong> indique que quelques images de méningiome ont été classées à tort dans une autre catégorie (<em>quelques faux négatifs</em>).</p></li>
<li><p>Le <strong>F1-score</strong> de <strong>99,67 %</strong> confirme une très bonne performance globale, avec un bon équilibre entre détection et exactitude.</p></li>
</ul></li>
<li><p><strong>Classe 3 (<code>Autre tumeur</code>)</strong> :</p>
<ul>
<li><p>La <strong>précision</strong> de <strong>99,67 %</strong> montre que le modèle fait <em>très peu</em> d’erreurs positives pour cette classe.</p></li>
<li><p>Le <strong>rappel</strong> parfait à <strong>100 %</strong> signifie qu’aucune image de cette classe n’a été manquée (<em>pas de faux négatifs</em>).</p></li>
<li><p>Le <strong>F1-score</strong> de <strong>99,83 %</strong> met en évidence la très haute qualité de la classification pour cette catégorie.</p></li>
</ul></li>
<li><p><strong>Moyennes globales (<code>macro</code> et <code>pondérée</code>)</strong> :</p>
<ul>
<li><p>Les scores globaux supérieurs à <strong>99,67 %</strong> en précision, rappel et F1-score montrent que le modèle est <em>à la fois robuste et équilibré</em> dans sa performance.</p></li>
<li><p>Le fait que précision et rappel soient très proches dans toutes les classes indique une capacité du modèle à <em>détecter les tumeurs avec fiabilité</em> tout en <em>limitant les fausses alertes</em>.</p></li>
</ul></li>
</ul>
<hr>
</section>
<section id="discussions-des-résultats" class="level3">
<h3 class="anchored" data-anchor-id="discussions-des-résultats">Discussions des résultats</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Notre modèle de classification des tumeurs cérébrales a atteint une exactitude de <strong>99,78 %</strong> sur les données de test, avec une valeur de perte (<code>loss</code>) très faible de <strong>1,35 %</strong>. La matrice de confusion révèle une excellente performance, avec très peu d’erreurs entre les classes. Par exemple, seules quelques images ont été mal classées, et aucune confusion n’a été observée pour la classe des autres tumeurs.</p>
<p>Au cours de l’entraînement, le score de perte diminuait de manière continue à chaque itération, aussi bien pour l’ensemble d’apprentissage que pour celui de validation. Cette évolution parallèle et cohérente des courbes de perte constitue un <strong>indice fort d’absence de surapprentissage (overfitting)</strong>. Le modèle semble ainsi avoir trouvé un bon compromis entre mémorisation des données d’entraînement et capacité de généralisation.</p>
<p>Ces résultats se comparent favorablement à ceux présentés dans la littérature. Tummala et al.&nbsp;(<span class="citation" data-cites="Tummala2022">(<a href="#ref-Tummala2022" role="doc-biblioref">Tummala et al. 2022</a>)</span>), utilisant un ensemble de Vision Transformers, rapportent une précision de 99,12 %, tandis que <span class="citation" data-cites="Rasheed2023">(<a href="#ref-Rasheed2023" role="doc-biblioref">Rasheed et al. 2023</a>)</span> obtiennent <strong>98,72 %</strong> avec un modèle <code>CNN</code>. D’autres études, comme celles de <span class="citation" data-cites="Srinivasan2024">(<a href="#ref-Srinivasan2024" role="doc-biblioref">Srinivasan et al. 2024</a>)</span> et <span class="citation" data-cites="Xie2022">(<a href="#ref-Xie2022" role="doc-biblioref">Xie et al. 2022</a>)</span>, rapportent également des précisions comprises entre <strong>97 %</strong> et <strong>99 %</strong>, mais sur des volumes de données souvent plus restreints.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Enfin, il convient de souligner que notre jeu de données comportait environ <code>6000</code> images, soit un volume environ deux fois plus important que dans certaines des études précédentes, ce qui pourrait contribuer à renforcer la fiabilité de l’évaluation. Toutefois, bien que ces résultats soient très encourageants, il reste important de rester prudent. <strong><code>Des facteurs tels que la diversité des images, la qualité des annotations, ou encore la sélection des hyperparamètres peuvent influencer les performances.</code></strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ainsi, même si notre approche semble compétitive par rapport à certaines méthodes récentes, une validation sur des jeux de données externes ou en conditions cliniques réelles serait nécessaire pour évaluer pleinement sa robustesse et sa généralisabilité. Notre objectif n’est pas tant de surpasser les méthodes existantes que de proposer une solution fiable, reproductible, et adaptée au contexte spécifique de notre étude.</p>
<hr>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Les résultats obtenus à l’issue de l’entraînement et de la validation du modèle indiquent que celui-ci :</p>
<ul>
<li><p>Apprend efficacement les motifs caractéristiques des différentes classes de tumeurs cérébrales à partir des images <code>IRM</code> ;</p></li>
<li><p>Généralise correctement sur des données non vues, ce qui est essentiel dans une perspective d’application clinique ;</p></li>
<li><p>Ne présente pas de signe manifeste de surapprentissage, comme en témoigne la faible différence entre les métriques d’entraînement et de validation (accuracy et loss).</p></li>
</ul>
<p>La cohérence de l’évolution des scores de perte durant l’entraînement, tant sur les données d’apprentissage que de validation, confirme la stabilité du modèle et son bon ajustement au problème de classification multiclasse.</p>
<p>Le modèle est très performant pour distinguer les différentes classes de tumeurs cérébrales sur les images <code>IRM</code>, avec très peu d’erreurs, ce qui est crucial dans un contexte clinique. Les faux positifs et faux négatifs sont très faibles, ce qui minimise le risque d’erreur de diagnostic.</p>
<hr>
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>À nuancer</p>
</blockquote>
</blockquote>
<ul>
<li><p>La très haute performance obtenue peut être en partie liée à la taille importante du jeu de données, qui a permis un apprentissage plus robuste. En effet, nous avons environ deux fois plus d’images que dans certaines études comparables.</p></li>
<li><p>Cela rend la comparaison directe avec les résultats des articles précédents plus délicate, car un jeu de données plus grand favorise généralement de meilleures performances, mais peut aussi cacher des variations dans la qualité ou la diversité des images.</p></li>
<li><p>Enfin, malgré ces résultats encourageants, il est essentiel de tester le modèle sur des données externes indépendantes pour confirmer sa capacité à généraliser en conditions réelles.</p></li>
</ul>
<hr>
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>Perspectives</p>
</blockquote>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Un prolongement naturel de ce travail consisterait à explorer la localisation de la tumeur en complément de sa classification. En ce sens, l’entraînement d’un modèle de type <code>YOLO</code> (You Only Look Once) pourrait permettre d’identifier automatiquement les zones suspectes sur une IRM en encadrant précisément la position de la tumeur.</p>
<p>Cependant, la mise en œuvre d’un tel modèle demanderait des ressources de calcul importantes, notamment en raison de la complexité des architectures de détection et de la nécessité de disposer d’annotations spatiales précises (bounding boxes). Cela constitue un défi technique, mais également une étape prometteuse vers un outil d’aide au diagnostic plus complet.</p>
<hr>
</section>
<section id="annexes" class="level2">
<h2 class="anchored" data-anchor-id="annexes">Annexes</h2>
</section>
<section id="annexes-1-data-augmentation" class="level2">
<h2 class="anchored" data-anchor-id="annexes-1-data-augmentation">Annexes 1 : Data augmentation</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>train_datagen <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    rescale<span class="op">=</span><span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    rotation_range<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    zoom_range<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">True</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>val_datagen <span class="op">=</span> ImageDataGenerator(rescale<span class="op">=</span><span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>test_datagen <span class="op">=</span> ImageDataGenerator(rescale<span class="op">=</span><span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_datagen.flow_from_directory(</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'data_ml_efficient_net/train'</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>     target_size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>     batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>     class_mode<span class="op">=</span><span class="st">'categorical'</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>val_gen <span class="op">=</span> val_datagen.flow_from_directory(</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'data_ml_efficient_net/val'</span>,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">'categorical'</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>test_gen <span class="op">=</span> train_gen.flow_from_directory(</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'data_ml_efficient_net/val'</span>,</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">'categorical'</span>,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    shuffle <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="annexe-2-construction-du-modèle" class="level2">
<h2 class="anchored" data-anchor-id="annexe-2-construction-du-modèle">Annexe 2 : Construction du modèle</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Construction du modèle</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> EfficientNetB5(weights<span class="op">=</span><span class="st">'imagenet'</span>, include_top<span class="op">=</span><span class="va">False</span>, input_shape<span class="op">=</span>(<span class="op">*</span>image_size, <span class="dv">3</span>))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Dégel des couches du modèle </span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>base_model.trainable <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Gel les premières couches pour ne pas tout ré-entraîner</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> base_model.layers[:<span class="dv">95</span>]:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    layer.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajout la tête de classification</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> base_model.output</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GlobalAveragePooling2D()(x)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> Dense(<span class="dv">3</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>base_model.<span class="bu">input</span>, outputs<span class="op">=</span>predictions)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Compilation avec un LR très bas pour le fine-tuning</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">1e-5</span>),</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Callbacks</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>callbacks <span class="op">=</span> [</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    EarlyStopping(</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">'val_loss'</span>,</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        patience<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        restore_best_weights<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    ReduceLROnPlateau(</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">'val_loss'</span>,</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        factor<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        patience<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        min_lr<span class="op">=</span><span class="fl">1e-7</span>,</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    ModelCheckpoint(</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        <span class="st">'best_model.h5'</span>,</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">'val_accuracy'</span>,</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        save_best_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Entraînement</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>val_gen,</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>callbacks</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="annexe-3-détails-de-calculs-des-métriques" class="level2">
<h2 class="anchored" data-anchor-id="annexe-3-détails-de-calculs-des-métriques">Annexe 3: Détails de calculs des métriques</h2>
<ul>
<li><strong>Matrice de confusion (test)</strong></li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
302 &amp; 0 &amp; 0 \\
1 &amp; 300 &amp; 1 \\
0 &amp; 0 &amp; 302 \\
\end{bmatrix}
\]</span></p>
<ul>
<li><p><strong>Classe 1 (Gliome)</strong> :</p>
<ul>
<li><span class="math inline">\(TP = 302\)</span><br>
</li>
<li><span class="math inline">\(FP = 0\)</span> (images d’autres classes classées comme Gliome)<br>
</li>
<li><span class="math inline">\(FN = 1\)</span> (images Gliome mal classées)</li>
</ul>
<p><span class="math display">\[
\text{Précision} = \frac{302}{302 + 1} = 0.9967
\]</span></p>
<p><span class="math display">\[
\text{Rappel} = \frac{302}{302 + 0} = 1.0
\]</span></p>
<p><span class="math display">\[
F1 = 2 \times \frac{0.9967 \times 1.0}{0.9967 + 1.0} = 0.9983
\]</span></p></li>
<li><p><strong>Classe 2 (Méningiome)</strong> :</p>
<ul>
<li><span class="math inline">\(TP = 300\)</span><br>
</li>
<li><span class="math inline">\(FP = 2\)</span><br>
</li>
<li><span class="math inline">\(FN = 0\)</span></li>
</ul>
<p><span class="math display">\[
\text{Précision} = \frac{300}{300} = 1
\]</span></p>
<p><span class="math display">\[
\text{Rappel} = \frac{300}{300 + 2} = 0.9934
\]</span></p>
<p><span class="math display">\[
F1 = 2 \times \frac{1.0 \times 0.9934}{1.0 + 0.9934} = 0.9967
\]</span></p></li>
<li><p><strong>Classe 3 (Autre tumeur)</strong> :</p>
<ul>
<li><span class="math inline">\(TP = 302\)</span><br>
</li>
<li><span class="math inline">\(FP = 0\)</span><br>
</li>
<li><span class="math inline">\(FN = 1\)</span></li>
</ul>
<p><span class="math display">\[
\text{Précision} = \frac{302}{302 + 1} = 0.9967
\]</span></p>
<p><span class="math display">\[
\text{Rappel} = \frac{302}{302 + 0} = 1.0
\]</span></p>
<p><span class="math display">\[
F1 = 2 \times \frac{0.9967 \times 1.0}{0.9967 + 1.0} = 0.9983
\]</span></p></li>
</ul>
<hr>
</section>
<section id="moyennes-globales" class="level2">
<h2 class="anchored" data-anchor-id="moyennes-globales">Moyennes globales</h2>
<p>Support total : <span class="math inline">\(300 + 300 + 307 = 907\)</span></p>
<ul>
<li><strong>Macro-average</strong> (moyenne simple) :</li>
</ul>
<p><span class="math display">\[
\text{Précision}_{macro} = \frac{0.9967 + 1.0 + 0.9967}{3} = 0.9978
\]</span></p>
<p><span class="math display">\[
\text{Rappel}_{macro} = \frac{1.0 + 0.9934 + 1.0}{3} = 0.9978
\]</span></p>
<p><span class="math display">\[
F1_{macro} = \frac{0.9983 + 0.9967 + 0.9983}{3} = 0.9978
\]</span></p>
<ul>
<li><strong>Weighted-average</strong> (moyenne pondérée) :</li>
</ul>
<p><span class="math display">\[
\text{Précision}_{weighted} = \frac{(302 \times 0.9967) + (302 \times 1.0) + (302 \times 0.9967)}{906} = 0.9978
\]</span></p>
<p><span class="math display">\[
\text{Rappel}_{weighted} = \frac{(302 \times 1.0) + (302 \times 0.9934) + (302 \times 1.0)}{906} = 0.9978
\]</span></p>
<p><span class="math display">\[
F1_{weighted} = \frac{(302 \times 0.9983) + (302 \times 0.9967) + (302 \times 0.9983)}{906} = 0.9978
\]</span></p>
<hr>
</section>
<section id="liste-des-sigles-et-abréviations" class="level2">
<h2 class="anchored" data-anchor-id="liste-des-sigles-et-abréviations">LISTE DES SIGLES ET ABRÉVIATIONS</h2>
<table class="table">
<colgroup>
<col style="width: 17%">
<col style="width: 82%">
</colgroup>
<thead>
<tr class="header">
<th>Sigle</th>
<th>Signification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>CNN</code></td>
<td>Convolutional Neural Network (Réseau de Neurones Convolutif)</td>
</tr>
<tr class="even">
<td><code>IRM</code></td>
<td>Imagerie par Résonance Magnétique</td>
</tr>
<tr class="odd">
<td><code>OMS</code></td>
<td>Organisation Mondiale de la Santé</td>
</tr>
<tr class="even">
<td><code>ViT</code></td>
<td>Vision Transformer</td>
</tr>
<tr class="odd">
<td><code>ReLU</code></td>
<td>Rectified Linear Unit (fonction d’activation)</td>
</tr>
<tr class="even">
<td><code>RGB</code></td>
<td>Rouge, Vert, Bleu (canaux de couleur)</td>
</tr>
<tr class="odd">
<td><code>YOLO</code></td>
<td>You Only Look Once</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="references-bibliographiques" class="level2">
<h2 class="anchored" data-anchor-id="references-bibliographiques">REFERENCES BIBLIOGRAPHIQUES</h2>
<p><strong>LIVRE</strong> : L’apprentissage Profond avec Python, <em>Les meilleures pratiques</em> de François Chollet (Une base en optimisation et méthodes de calculs numériques pourrait être utile pour une compréhension moins superficielle du conténu du livre)</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Rasheed2023" class="csl-entry" role="listitem">
Rasheed, Zahid, Yong-Kui Ma, Inam Ullah, Tamara Al Shloul, Ahsan Bin Tufail, Yazeed Yasin Ghadi, Muhammad Zubair Khan, and Heba G. Mohamed. 2023. <span>“Automated Classification of Brain Tumors from Magnetic Resonance Imaging Using Deep Learning.”</span> <em>Brain Sciences</em> 13 (4): 602. <a href="https://doi.org/10.3390/brainsci13040602">https://doi.org/10.3390/brainsci13040602</a>.
</div>
<div id="ref-Srinivasan2024" class="csl-entry" role="listitem">
Srinivasan, Saravanan, Divya Francis, Sandeep Kumar Mathivanan, Hariharan Rajadurai, Basu Dev Shivahare, and Mohd Asif Shah. 2024. <span>“A Hybrid Deep CNN Model for Brain Tumor Image Multi-Classification.”</span> <em>BMC Medical Imaging</em> 24: 21. <a href="https://doi.org/10.1186/s12880-024-01195-7">https://doi.org/10.1186/s12880-024-01195-7</a>.
</div>
<div id="ref-Tummala2022" class="csl-entry" role="listitem">
Tummala, Sudhakar, Seifedine Kadry, Syed Ahmad Chan Bukhari, and Hafiz Tayyab Rauf. 2022. <span>“Classification of Brain Tumor from Magnetic Resonance Imaging Using Vision Transformers Ensembling.”</span> <em>Current Oncology</em> 29 (10): 7498–7511. <a href="https://doi.org/10.3390/curroncol29100590">https://doi.org/10.3390/curroncol29100590</a>.
</div>
<div id="ref-Xie2022" class="csl-entry" role="listitem">
Xie, Yuting, Fulvio Zaccagna, Leonardo Rundo, Claudia Testa, Raffaele Agati, Raffaele Lodi, David Neil Manners, and Caterina Tonon. 2022. <span>“Convolutional Neural Network Techniques for Brain Tumor Classification (from 2015 to 2022): Review, Challenges, and Future Perspectives.”</span> <em>Diagnostics</em> 12 (8): 1850. <a href="https://doi.org/10.3390/diagnostics12081850">https://doi.org/10.3390/diagnostics12081850</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>