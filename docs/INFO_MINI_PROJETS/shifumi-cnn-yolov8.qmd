---
title: "ShiFuMi IA â€“ Reconnaissance de gestes"
author: "Djamal TOE"
date: "May 20, 2025"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE)
library(reticulate)
use_python("C:/Users/Djamal TOE/AppData/Local/Programs/Python/Python311", required=TRUE)
```


# ğŸ¯ Contexte

Ce projet vise Ã  crÃ©er une intelligence artificielle capable de jouer au jeu **ShiFuMi** (*pierre-papier-ciseaux*) contre un humain. Lâ€™objectif est de reconnaÃ®tre automatiquement les gestes d'une main via une **webcam**, et de rÃ©pondre en temps rÃ©el. Ce prototype est dÃ©veloppÃ© **seul**, en utilisant les outils suivants :

- **CNN prÃ©-entraÃ®nÃ© (MobileNetV2)** pour classifier les images de mains,
- **TensorFlow/Keras** pour l'entraÃ®nement,
- **YOLOv8** (en prÃ©paration) pour la dÃ©tection en direct via webcam,

---

# ğŸ§  Objectif technique

- CrÃ©er un modÃ¨le de **classification dâ€™images** pour reconnaÃ®tre trois gestes : `pierre`, `papier`, `ciseaux`,
- Utiliser un modÃ¨le lÃ©ger et rapide adaptÃ© Ã  la webcam : **MobileNetV2**,
- PrÃ©parer l'intÃ©gration en live avec **dÃ©tection par YOLOv8** + **reconnaissance par CNN**.

---

# âš™ï¸ DonnÃ©es pour le CNN

Les images sont triÃ©es dans des dossiers selon leur classe :

```rmd
dataset/
â”œâ”€â”€ paper
|   â”œâ”€â”€ train
â”‚   â””â”€â”€ val
â”œâ”€â”€ rock/
|   â”œâ”€â”€ train
â”‚   â””â”€â”€ val
â”œâ”€â”€ scissors/
|   â”œâ”€â”€ train
â”‚   â””â”€â”€ val
```

|       Ici je ne fais pas de test, car au cours de l'entrainement j'Ã©value le modÃ¨le avec les images du dossier `val`. Cependant il est toutefois possible possible de crÃ©er un dossier test via la classe que j'ai Ã©crite pour l'entraienement du modÃ¨le. Les images utilisÃ©es pour l'entraienement du modÃ¨le vienne d'une compilation d'images tÃ©lÃ©chargÃ©es sur Kaggle. Etant volumineuses, je ne pourrai pas les mettre sur le dÃ©pot `git public`. Parcontre les `notebook` et les `script .py` y seront.

# âš™ï¸ DonnÃ©es pour yolov8

Les donnÃ©es pour l'entrainement du modÃ¨le `yolov8` proviennent de la plateforme `ROBOT FLOW`. L'idÃ©e derriÃ¨re le fait d'entraÃ®ner le modÃ¨le `YOLO` vient du fait que je veux pouvoir detecter les mains dans une images et ensuite les passer au modÃ¨le `CNN` pour la classification. 
|       La structure des donnÃ©es `YOLO` est un peu particuliÃ¨re. En effet, elle a bÃ©soin d'images mais pas que, il faut qu'elles soient **annontÃ©es**. De maniÃ¨re claire, il faut en plus des images, les labels qui lui sont associÃ©s (coordonnÃ©es des images). 

# ğŸ”® DonnÃ©es d'entraÃ®nement pour YOLOv8

Les donnÃ©es utilisÃ©es pour entraÃ®ner le modÃ¨le **YOLOv8** proviennent de la plateforme **[Robot Flow](https://robotflow.ai)**. Cette plateforme propose une interface conviviale pour la collecte et l'annotation de donnÃ©es, ce qui en fait un outil idÃ©al pour les projets de vision par ordinateur.

## ğŸ¤” Pourquoi entraÃ®ner YOLOv8 ?

|       L'objectif principal de l'entraÃ®nement de ce modÃ¨le est de **dÃ©tecter les mains** dans des images issues d'une webcam ou d'une vidÃ©o. Cette dÃ©tection constitue une premiÃ¨re Ã©tape essentielle avant de transmettre la rÃ©gion d'intÃ©rÃªt (ROI), c'est-Ã -dire la main dÃ©tectÃ©eÃ  un modÃ¨le **CNN** pour effectuer la **classification du geste** (pierre, feuille, ciseaux).

Cette stratÃ©gie en deux Ã©tapes permet de :

* RÃ©duire le bruit visuel autour de la main (fond, visage, objets parasites)
* Augmenter la prÃ©cision du classifieur `CNN` en se concentrant uniquement sur la main

## ğŸ“š Structure des donnÃ©es YOLO

Le format d'attente de `YOLO` est spÃ©cifique : chaque image d'entraÃ®nement doit Ãªtre accompagnÃ©e d'un **fichier d'annotation `.txt`** contenant les informations de localisation des objets (ici, la main).

L'organisation des donnÃ©es se prÃ©sente gÃ©nÃ©ralement ainsi :

```rmd
datasets/yolo_hand/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ img_001.jpg
â”‚   â”‚   â”œâ”€â”€ img_002.jpg
â”‚   â””â”€â”€ val/
â”‚       â”œâ”€â”€ img_101.jpg
â”‚       â”œâ”€â”€ img_102.jpg
â”œâ”€â”€ labels/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ img_001.txt
â”‚   â”‚   â”œâ”€â”€ img_002.txt
â”‚   â””â”€â”€ val/
â”‚       â”œâ”€â”€ img_101.txt
â”‚       â”œâ”€â”€ img_102.txt
```

Chaque fichier `.txt` contient une ou plusieurs lignes correspondant aux objets dÃ©tectÃ©s dans l'image, selon le format suivant :

```
<class_id> <x_center> <y_center> <width> <height>
```

* Toutes les valeurs sont **normalisÃ©es** entre 0 et 1 relativement Ã  la taille de l'image.
* `class_id` correspond ici Ã  la main, donc souvent 0 dans le cadre d'un problÃ¨me mono-classe.

## ğŸš€ EntraÃ®nement avec Ultralytics (Exemple)

Pour entraÃ®ner le modÃ¨le YOLOv8, on utilise la commande suivante :

```bash
yolo task=detect mode=train \
  model=yolov8n.pt \
  data=config.yaml \
  epochs=50 \
  imgsz=640
```

OÃ¹ le fichier `config.yaml` contient :

```yaml
path: datasets/yolo_hand
train: images/train
val: images/val

names:
  0: main
```

::: {#wolframalfa .callout-important} 
# Pourquoi le notebook n'est pas publiÃ© ?

L'outil de dÃ©vÃ©loppement du site ne supporte pas la bibliothÃ¨que `tensorflow`, du coup cela rend impossible le dÃ©ploiement du site car l'exÃ©cution des code du notebook ne passe pas. Toutefois, une fois les `notebooks` et `scripts` entiÃ¨rement mis au propres, je les mettrai Ã  disposition sur un dÃ©pot public `github`
:::

# Premiers rÃ©sultats (entrainement du CNN)

|       Au lieu de partir de zÃ©ro pour l'entraÃ®nement du modÃ¨le, j'ai prÃ©fÃ©rÃ© utiliser un modÃ¨le dÃ©jÃ  prÃ©entraÃ®nÃ© afin d'amÃ©liorer la prÃ©cision et accÃ©lÃ©rer la convergence. A cet effet j'ai lancÃ© l'entrainement avec `20 itÃ©rations`. Cependant, il est important de souligner que le fait dâ€™utiliser un modÃ¨le prÃ©entraÃ®nÃ© ne signifie pas que celui-ci peut directement classifier correctement les gestes de la main sans un entraÃ®nement spÃ©cifique sur mon jeu de donnÃ©es.

En effet, le modÃ¨le prÃ©entraÃ®nÃ©, ici `MobileNetV2`, a Ã©tÃ© initialement entraÃ®nÃ© sur une large base dâ€™images gÃ©nÃ©riques (ImageNet) et connaÃ®t bien les caractÃ©ristiques gÃ©nÃ©rales des images, comme les formes et les textures. Mais pour reconnaÃ®tre prÃ©cisÃ©ment des gestes spÃ©cifiques (pierre, papier, ciseaux), il faut le reformer (`fine-tuning`[^1]) avec mes images annotÃ©es.


[^1]: Le fine-tuning est une technique dâ€™apprentissage supervisÃ© qui consiste Ã  prendre un modÃ¨le prÃ©entraÃ®nÃ© sur un grand jeu de donnÃ©es gÃ©nÃ©ral (comme `ImageNet`) et Ã  le rÃ©entraÃ®ner sur un jeu de donnÃ©es spÃ©cifique Ã  un problÃ¨me particulier.

En dâ€™autres termes, au lieu de dÃ©marrer lâ€™entraÃ®nement dâ€™un rÃ©seau de neurones Ã  partir de zÃ©ro (ce qui nÃ©cessite beaucoup de donnÃ©es et de temps), on part dâ€™un modÃ¨le qui connaÃ®t dÃ©jÃ  des caractÃ©ristiques gÃ©nÃ©rales (par exemple, dÃ©tecter des formes, des textures, des couleurs). Ensuite, on "ajuste" ou "affine" ce modÃ¨le en lui apprenant Ã  reconnaÃ®tre des classes plus spÃ©cifiques, comme ici les gestes de la main (pierre, feuille, ciseaux).

> RÃ©sultats obtenus

- **PrÃ©cision sur les donnÃ©es d'entraÃ®nement et de validation** : aprÃ¨s quelques epochs, le modÃ¨le atteint une bonne exactitude (`accuracy rate`) (99%), ce qui indique quâ€™il a bien appris Ã  diffÃ©rencier les classes sur mes images.

- **Sur-apprentissage (overfitting) :** grÃ¢ce Ã  lâ€™utilisation de techniques comme la rÃ©gularisation et lâ€™augmentation des donnÃ©es (data augmentation), jâ€™ai limitÃ© le sur-apprentissage, ce qui permet au modÃ¨le de mieux gÃ©nÃ©raliser sur de nouvelles images.

- **Limites Ã©ventuelles** : malgrÃ© ces rÃ©sultats encourageants, la classification en conditions rÃ©elles (ex. en direct via webcam) peut Ãªtre plus complexe en raison des variations dâ€™Ã©clairage, de position des mains, et du fond. Etant donnÃ©e qu'Ã  ce stade, l'entraÃ®nement du modÃ¨le `YOLO` n'est pas encore terminÃ© je ne peux pas me prononcer avec certiude sur ces limites. 

> Test des rÃ©sultats obtenus

## EntraÃ®nement du modÃ¨le YOLO (Epochs=33/100)

|       Etant donnÃ© que le modÃ¨le Ã©tait entrainÃ© local et du fait qu'il mettait du temps, j'ai volontairement stoppÃ© l'entraÃ®nement Ã  la 33-iÃ¨me itÃ©ration. Car Ã  ce stade les rÃ©sultats de la dÃ©tections des mains Ã©taient satisfaisants (**MAP**[^2] environ Ã©gale Ã  0,94). Voici un extrait du rÃ©sultat :

{{< video https://djamal2905.github.io/djamal_website/INFO_MINI_PROJETS/results_cnn_shifumi/yolo.mp4 >}}


## EntraÃ®nement du modÃ¨le CNN

|       Une fois le modÃ¨le `YOLO` prÃªt Ã  detecter les mains dans une image, le `boxe` (pour dire le cadre/rectangle contenant la main) est envoyÃ© au modÃ¨le `CNN` afin qu'il puisse classer la main parmis les trois gestes : `paper` pour ***papier***, `rock` pour ***pierre*** et `scissors` pour ***ciseau***. 

La vidÃ©o ci-aprÃ¨s illustre le rÃ©sultat.

{{< video https://djamal2905.github.io/djamal_website/INFO_MINI_PROJETS/results_cnn_shifumi/cnn.mp4 >}}



[^2]: **Mean Average Precision** : C'est la mÃ©trique principale utilisÃ©e pour Ã©valuer les performances d'un modÃ¨le de dÃ©tection comme `YOLO`. 
**PrÃ©cision (Precision)**

# Annexes 1 : CNN - entraÃ®nement et validation

```{python, echo=FALSE, warning = FALSE}
#| fig-cap: Ã‰volution de la prÃ©cision et de la perte sur les donnÃ©es de validation
import numpy as np
import matplotlib.pyplot as plt

# DonnÃ©es
val_accuracy = np.asarray([
    0.9969306588172913, 0.9979537725448608, 0.9977491497993469,
    0.997544527053833, 0.9981583952903748, 0.9983630180358887,
    0.9981583952903748, 0.9981583952903748, 0.9979537725448608,
    0.997544527053833, 0.9973399043083191, 0.9973399043083191,
    0.9973399043083191, 0.9977491497993469, 0.9979537725448608,
    0.9981583952903748, 0.9977491497993469, 0.9977491497993469,
    0.9981583952903748, 0.9979537725448608
])

val_loss = np.asarray([
    0.011523569002747536, 0.008804770186543465, 0.009383113123476505,
    0.009934342466294765, 0.00778602110221982, 0.008130679838359356,
    0.00829746387898922, 0.008368044160306454, 0.009568408131599426,
    0.009984266944229603, 0.012473036535084248, 0.010387655347585678,
    0.009567142464220524, 0.010067718103528023, 0.009568578563630581,
    0.01169647928327322, 0.011526044458150864, 0.01234098244458437,
    0.011368398554623127, 0.010415228083729744
])

epochs = np.arange(1, 21)

# Tracer les courbes
plt.figure(figsize=(10, 5))
plt.plot(epochs, val_accuracy, label='Validation Accuracy', marker='o')
plt.plot(epochs, val_loss, label='Validation Loss', marker='s')
plt.title("Ã‰volution de l'exactitude et de la perte sur les donnÃ©es de validation")
plt.xlabel('Ã‰poques')
plt.ylabel('Valeur')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
```

# Annexe 2 : MAP

La MAP (**Mean Average Precision**) : C'est la mÃ©trique principale utilisÃ©e pour Ã©valuer les performances d'un modÃ¨le de dÃ©tection comme `YOLO`. 
**PrÃ©cision (Precision)**

La prÃ©cision indique combien de prÃ©dictions faites par le modÃ¨le sont *correctes*.

$$
\text{PrÃ©cision} = \frac{\text{Vrais positifs}}{\text{Vrais positifs} + \text{Faux positifs}}
$$

- **Rappel (Recall)**

Le rappel indique combien dâ€™objets rÃ©els ont Ã©tÃ© *correctement dÃ©tectÃ©s*.

$$
\text{Rappel} = \frac{\text{Vrais positifs}}{\text{Vrais positifs} + \text{Faux nÃ©gatifs}}
$$

- **AP : Average Precision**

Lâ€™*AP (Average Precision)* est la *moyenne* de la prÃ©cision Ã  diffÃ©rents niveaux de rappel pour *une seule classe d'objet*.

- **mAP : mean Average Precision**

Le *mAP* est la moyenne des *AP* pour *toutes les classes*. Si le modÃ¨le dÃ©tecte plusieurs objets (main, poing, doigt pointÃ©), on calcule lâ€™AP pour chacune, puis on fait la moyenne.
