---
title: "D√©tection d'anomalies dans les transactions banquaires"
date: "June 23, 2025"
author: "Djamal TOE"
link-citations: true
bibliography: anomaly_bib.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE)
library(reticulate)
use_python("C:/Users/Djamal TOE/AppData/Local/Programs/Python/Python311")
```


## Introduction

|       La d√©tection de fraudes sur les transactions bancaires est un enjeu majeur pour les institutions financi√®res. Les m√©thodes traditionnelles bas√©es sur des r√®gles statiques peinent √† s‚Äôadapter aux sch√©mas de fraude de plus en plus sophistiqu√©s. D√®s 2002, Bolton & Hand ont propos√© un mod√®le statistique pour identifier les anomalies transactionnelles [@Bolton2002]. Depuis, de nombreuses approches ont √©merg√©, √† la fois supervis√©es et non supervis√©es, offrant des performances variables selon la disponibilit√© de donn√©es √©tiquet√©es, la diversit√© des comportements normaux et la capacit√© √† d√©tecter de nouvelles formes de fraude [@Phua2010; @Ngai2011; @Chalapathy2019].


## M√©thodologie

|       Pour ce mini-projet, nous adopterons une approche non supervis√©e utilisant un Gaussian Mixture Model (`GMM`). Le `GMM` permet de mod√©liser la distribution sous-jacente des transactions l√©gitimes par une combinaison de gaussiennes, et d'identifier les observations pr√©sentant une faible vraisemblance comme anomalies [@Bishop2006].

Les √©tapes principales sont:

- **Pr√©traitement des donn√©es**:

  - S√©lection des variables pertinentes (montant, temporalit√©, etc.);

  - Nettoyage, transformation et mise √† l‚Äô√©chelle [@Hastie2009]

- **Estimation du GMM**:

  - Choix du nombre de composantes par crit√®res AIC/BIC [@Schwarz1978]

  - Ajustement du mod√®le sur les donn√©es normalis√©es

- **D√©tection des anomalies**:

  - Calcul de la log-vraisemblance pour chaque transaction

  - D√©finition d‚Äôun seuil bas√© sur un percentile (par exemple 1%) pour isoler les transactions suspectes [@Ngai2011]

## Pratique

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# print(plt.style.available)
plt.style.use('seaborn-v0_8-whitegrid')
df = pd.read_csv('bank_transactions_data_2.csv')
```

```{python}
df.head()
```


**About Dataset** (From kaggle ([link](https://www.kaggle.com/datasets/valakhorasani/bank-transaction-dataset-for-fraud-detection)))

$\quad$ This dataset provides a detailed look into transactional behavior and financial activity patterns, ideal for exploring fraud detection and anomaly identification. It contains 2,512 samples of transaction data, covering various transaction attributes, customer demographics, and usage patterns. Each entry offers comprehensive insights into transaction behavior, enabling analysis for financial security and fraud detection applications.

**Key Features:**

- `TransactionID`: Unique alphanumeric identifier for each transaction.

- `AccountID`: Unique identifier for each account, with multiple transactions per account.

- `TransactionAmount`: Monetary value of each transaction, ranging from small everyday expenses to larger purchases.

- `TransactionDate`: Timestamp of each transaction, capturing date and time.

- `TransactionType`: Categorical field indicating 'Credit' or 'Debit' transactions.

- `Location`: Geographic location of the transaction, represented by U.S. city names.

- `DeviceID`: Alphanumeric identifier for devices used to perform the transaction.

- `IP Address`: IPv4 address associated with the transaction, with occasional changes for some accounts.

- `MerchantID`: Unique identifier for merchants, showing preferred and outlier merchants for each account.

- `AccountBalance`: Balance in the account post-transaction, with logical correlations based on transaction type and amount.

- `PreviousTransactionDate`: Timestamp of the last transaction for the account, aiding in calculating transaction frequency.

- `Channel`: Channel through which the transaction was performed (e.g., Online, ATM, Branch).

- `CustomerAge`: Age of the account holder, with logical groupings based on occupation.

- `CustomerOccupation`: Occupation of the account holder (e.g., Doctor, Engineer, Student, Retired), reflecting income patterns.

- `TransactionDuration`: Duration of the transaction in seconds, varying by transaction type.

- `LoginAttempts`: Number of login attempts before the transaction, with higher values indicating potential anomalies.

This dataset is ideal for data scientists, financial analysts, and researchers looking to analyze transactional patterns, detect fraud, and build predictive models for financial security applications. The dataset was designed for machine learning and pattern analysis tasks and is not intended as a primary data source for academic publications.

```{python}
df.info()
```

```{python}
# checking if there are some NA values
(df.isnull().sum() | df.isna().sum())
```

$\quad$ Certaines variables, bien que nettoy√©es et sans valeurs manquantes, n‚Äôont pas √©t√© exploit√©es dans l‚Äôanalyse principale¬†:  
- **TransactionID**  
- **AccountID**  
- **DeviceID**  
- **IP Address**  
- **MerchantID**  

Toutefois, selon l‚Äôobjectif vis√©, certaines d‚Äôentre elles pourraient s‚Äôav√©rer tr√®s pertinentes:  
- **AccountID**: d√©tection de comptes √† risque, suivi des comportements de chaque titulaire,  
- **IP Address**: analyse spatiale et tra√ßage g√©ographique des connexions,  
- **MerchantID**: √©tude du comportement des commer√ßants et d√©tection d‚Äôanomalies sp√©cifiques √† certains points de vente. 

Pour des raisons √©thiques, ces d√©tails ne seront pas explor√©es dans le cadre de cet **mini-projet**, car nous ne savons pas si les ID sont r√©els ou pas.


```{python}
selected_variables = list(df.columns[~df.columns.isin(['TransactionID', 'IP Address', 'AccountID', 'MerchantID', 'DeviceID', 'Unnamed: 0'])])
df_reduced = df[selected_variables]
df_reduced.head()
```

|       Les variables `TransactionDate` et `PreviousTransactionDate` peuvent nous servir √† calculer une variables plus informative et utilisable qui est `TimeBetweenThisTransactionAndTheLastOne` qui pourrait √™tre en heures ou en secondes en fonction des valeurs obtenues de la diff√©rence entre ces deux dates.


```{python}
# converting TransactionDate and PreviousTransactionDate into datetime format
df_reduced[['TransactionDate', 'PreviousTransactionDate']] = df_reduced[['TransactionDate', 'PreviousTransactionDate']].apply(pd.to_datetime)

# computing 'TimeBetweenThisTransactionAndTheLastOne'

df_reduced['TimeBetweenThisTransactionAndTheLastOne'] = (
    (df_reduced['TransactionDate'] - df_reduced['PreviousTransactionDate'])
    .dt.total_seconds()
    .abs() # taking the abs because PreviousTransactionDate is later than TransactionDate
    .div(3600) # 1 hour = 3600 secs
    .round(2)
)
# just removing the to columns of dates
df_reduced.drop(['TransactionDate', 'PreviousTransactionDate'], axis=1, inplace=True)

df_reduced.head()
```
```{python}
df_reduced['TimeBetweenThisTransactionAndTheLastOne'].describe()
```
On pourrait m√™me convertir cela en nombre de jours car le minimum est de `7381.75` heures.


```{python}
selected_variables = list(df.columns[~df.columns.isin(['TransactionID', 'IP Address', 'AccountID', 'MerchantID', 'DeviceID', 'Unnamed: 0.1', 'Unnamed: 0'])])
df_reduced = df[selected_variables]
# converting TransactionDate and PreviousTransactionDate into datetime format
df_reduced[['TransactionDate', 'PreviousTransactionDate']] = df_reduced[['TransactionDate', 'PreviousTransactionDate']].apply(pd.to_datetime)

# computing 'TimeBetweenThisTransactionAndTheLastOne'

df_reduced['DaysBetweenThisTransactionAndTheLastOne'] = (
    (df_reduced['TransactionDate'] - df_reduced['PreviousTransactionDate'])
    .dt.total_seconds()
    .abs() # taking the abs because PreviousTransactionDate is later than TransactionDate
    .div(3600*24) # 1 hour = 3600 secs
    .round(0)
)
# just removing the to columns of dates
df_reduced.drop(['TransactionDate', 'PreviousTransactionDate'], axis=1, inplace=True)
df_reduced.head()

```


```{python}
df_reduced['DaysBetweenThisTransactionAndTheLastOne'].describe()
```


|       Il est appara√Æt surprenant que qu'il y'ai autant de jour entre deux transaction. Nous n'avons pas fait d'erreur de calcul car si vous monter un peu plus haut et en regardant les colonnes `TransactionDate` and `PreviousTransactionDate` vous verrez que cet √©cart peut s'expliquer par diverses raisons sauf par une erreur de calcul de notre part. 



```{python}
def plot_bar(df, var_name, color='#1f77b4', ax=None):
    """
    Plot a bar chart of value counts for a categorical variable.

    Parameters
    ----------
    df : pandas.DataFrame
        DataFrame containing the data.
    var_name : str
        Name of the column to plot.
    color : str, optional
        Bar color (default: '#1f77b4').
    ax : matplotlib.axes.Axes, optional
        Axes on which to draw the plot. If None, a new figure and axes are created.

    Returns
    -------
    matplotlib.axes.Axes
        The axes object containing the bar plot, for further customization.

    Notes
    -----
    - Displays count labels above each bar.
    - Rotates x-axis labels by 45¬∞ for readability.
    """
    counts = df[var_name].value_counts()
    x = counts.index.tolist()
    height_ = counts.values

    if ax is None:
        fig, ax = plt.subplots(1, 1)

    ax.bar(x=x, height=height_, width=0.4, color=color)

    for i, value in enumerate(height_):
        ax.text(i, value + value / 100, str(value), ha='center')

    ax.tick_params(axis='x', labelrotation=45)
    ax.set_xlabel(var_name.capitalize())
    ax.set_ylabel(f'Number of {var_name.lower()}')
    ax.set_title(f'Barplot of {var_name.capitalize()}')
    return ax


def plot_hist(df, var_name, bins=10, color='#1f77b4', ax=None, transform_=None):
    """
    Plot a histogram for a numeric variable, with an optional transformation.

    Parameters
    ----------
    df : pandas.DataFrame
        DataFrame containing the data.
    var_name : str
        Name of the numeric column to plot.
    bins : int, optional
        Number of histogram bins (default: 10).
    color : str, optional
        Bar color for the histogram (default: '#1f77b4').
    ax : matplotlib.axes.Axes, optional
        Axes on which to draw the plot. If None, a new figure and axes are created.
    transform_ : callable, optional
        Function to apply to the data before plotting (e.g., np.log).
        Must accept and return an array-like.

    Returns
    -------
    matplotlib.axes.Axes
        The axes object containing the histogram, for further customization.

    Notes
    -----
    - Missing values are automatically dropped.
    - If a transformation is applied, the plot title reflects it.
    - Rotates x-axis labels by 45¬∞ for readability.
    """
    y = df[var_name].dropna()

    if transform_ is not None:
        y = transform_(y)

    if ax is None:
        fig, ax = plt.subplots(1, 1)

    ax.hist(y, bins=bins, color=color, edgecolor='black')

    ax.tick_params(axis='x', labelrotation=45)
    title = f'Histogram of {var_name.capitalize()}'
    if transform_ is not None:
        title += ' (transformed)'
    ax.set_title(title)
    ax.set_xlabel('Bins')
    ax.set_ylabel(f'Frequency of {var_name.lower()}')
    return ax
```


```{.python}
import math
var_list = list(df_reduced.columns.values)
figsize = (12, 8)
n_vars = len(var_list)
# calcul automatique du nombre de colonnes si non fourni
ncols = int(math.ceil(math.sqrt(n_vars)))
nrows = int(math.ceil(n_vars / ncols))
fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)
axes = axes.flatten() 
plt.subplots_adjust(hspace=0.4, wspace=0.4)
bins_ = 20
for i in range(len(var_list)):
    var_name = var_list[i]
    if df_reduced[var_name].dtype in ['categorical', 'object']:
        plot_bar(df=df_reduced, var_name=var_name, ax=axes[i])
    else:
        plot_hist(df=df_reduced, var_name=var_name, ax=axes[i])

for j in range(len(var_list), len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
#plt.suptitle('Variables distribution')
plt.show()
```

```{r, fig.align='center', echo= FALSE, out.width='95%', out.height='90%'}
#| label: fig-var-dist
#| fig-cap: Selecting the optimum number of components

knitr::include_graphics('dist-var.png')
```


|       On arrive √† voir que nous avons √©norm√©ment de zones ou regions de transactions. On peut aussi constater que le nombre de tentatives de connexion est discret avec des valeurs faibles (√† prendre en compte dans la mod√©lisation future).

```{python}
df_reduced['Location'].value_counts()
```


|       Imaginons que nous n‚Äôayons que des donn√©es continues pour detecter les anomalies. Appliquons un mod√®le de m√©lange gaussien. 


### Log-vraisemblance compl√®te dans un GMM

Soit :

- $X = \{x_1, \ldots, x_n\}$ : les donn√©es observ√©es,
- $Z = \{z_1, \ldots, z_n\}$ : les variables latentes (composantes d‚Äôappartenance),
- $\Theta = \{ \pi_k, \mu_k, \Sigma_k \}_{k=1}^K$ : les param√®tres du mod√®le,
- $z_{ik} = 1$ si $x_i$ appartient √† la composante $k$, sinon $0$.

- **Vraisemblance compl√®te**

On suppose que l‚Äôobservation $x_i$ vient de la composante $k$ avec une probabilit√© $\pi_k$, et que la distribution conditionnelle est gaussienne :

$$
p(X, Z \mid \Theta) = \prod_{i=1}^n \prod_{k=1}^K \left[ \pi_k \, \mathcal{N}(x_i \mid \mu_k, \Sigma_k) \right]^{z_{ik}}
$$

- **Log-vraisemblance augment√©e**

En prenant le logarithme, on obtient la log-vraisemblance augment√©e :

$$
\log p(X, Z \mid \Theta) = \sum_{i=1}^n \sum_{k=1}^K z_{ik} \left( \log \pi_k + \log \mathcal{N}(x_i \mid \mu_k, \Sigma_k) \right)
$$

- L‚Äôalgorithme EM maximise l‚Äôesp√©rance de cette quantit√© (appel√©e **Q-fonction**) dans l‚Äô√©tape E :
  
$$
Q(\Theta \mid \Theta^{\text{old}}) = \mathbb{E}_{Z \mid X, \Theta^{\text{old}}} [ \log p(X, Z \mid \Theta) ]
$$


- **Algorithme EM pour un mod√®le de m√©lange gaussien (GMM)**

|       Soit un jeu de donn√©es $X = \{ x_1, x_2, \ldots, x_n \}$ avec $n$ observations,  et un GMM avec $K$ composantes. En vous √©pargnant de la r√©solution du probl√®me : $\Theta = argmax \log p(X, Z \mid \Theta)$

- **Initialisation**

Initialiser les param√®tres du mod√®le pour chaque composante $k = 1, \ldots, K$ :

- Les poids : $\pi_k$, avec $\sum_{k=1}^K \pi_k = 1$ et $\pi_k > 0$,
- Les moyennes : $\mu_k \in \mathbb{R}^d$,
- Les matrices de covariance : $\Sigma_k \in \mathbb{R}^{d \times d}$.


- **√âtape 1 : Expectation (E-step)**

Pour chaque observation $x_i$, calculer la responsabilit√© $\gamma_{ik}$ qui est la probabilit√© que $x_i$ appartienne √† la composante $k$, donn√©e les param√®tres actuels :

$$
\gamma_{ik} = \frac{\pi_k \, \mathcal{N}(x_i \mid \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \, \mathcal{N}(x_i \mid \mu_j, \Sigma_j)}
$$

o√π $\mathcal{N}(x \mid \mu, \Sigma)$ est la densit√© de la loi normale multivari√©e.


- **√âtape 2 : Maximisation (M-step)**

Mettre √† jour les param√®tres $\pi_k$, $\mu_k$, $\Sigma_k$ en fonction des responsabilit√©s calcul√©es :

- Mise √† jour des poids :

$$
\pi_k = \frac{1}{n} \sum_{i=1}^n \gamma_{ik}
$$

- Mise √† jour des moyennes :

$$
\mu_k = \frac{\sum_{i=1}^n \gamma_{ik} x_i}{\sum_{i=1}^n \gamma_{ik}}
$$

- Mise √† jour des covariances :

$$
\Sigma_k = \frac{\sum_{i=1}^n \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^n \gamma_{ik}}
$$


- **R√©p√©ter**

R√©p√©ter les √©tapes **E** et **M** jusqu'√† convergence, c‚Äôest-√†-dire jusqu‚Äô√† ce que la variation de la log-vraisemblance soit tr√®s faible ou qu‚Äôun nombre maximal d‚Äôit√©rations soit atteint.


- **R√©sum√©**

| √âtape          | Description                                             |
|----------------|---------------------------------------------------------|
| Initialisation | Fixer $\pi_k, \mu_k, \Sigma_k$ pour $k=1,\ldots,K$      |
| E-step         | Calculer les responsabilit√©s $\gamma_{ik}$              |
| M-step         | Mettre √† jour $\pi_k, \mu_k, \Sigma_k$                   |
| R√©p√©ter       | Jusqu‚Äô√† convergence                                      |

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
RANDOM_STATE = 42
scaler = StandardScaler()
# selecting continuous variables
excluded_variables = [
    'TransactionType', 
    'Location', 
    'Channel',
    'LoginAttempts',
    'CustomerOccupation'
]
df_continuous = df_reduced.drop(excluded_variables, axis=1)
df_continuous.head()
```

|       Nous disposons des variables suivantes : le montant de la transaction, l'√¢ge du client, la dur√©e de la transaction, le solde du compte, ainsi que le nombre de jours √©coul√©s depuis la derni√®re transaction.  

Afin d'appliquer un mod√®le de m√©lange gaussien (GMM) √† ces donn√©es, il est n√©cessaire de choisir un nombre optimal de composantes. En pratique, ce choix repose souvent sur des crit√®res d'information tels que le **Crit√®re d'Information d'Akaike (AIC)** ou le **Crit√®re d'Information Bay√©sien (BIC)**. Plus ces crit√®res sont faibles, meilleur est le mod√®le. Toutefois, il convient de rester vigilant face au risque de **surapprentissage** : un mod√®le trop complexe (avec trop de composantes) peut s'ajuster parfaitement aux donn√©es d'apprentissage mais perdre en capacit√© de g√©n√©ralisation.  

Dans le cadre de l‚Äôapprentissage **non supervis√©**, l‚Äô√©valuation du mod√®le est plus d√©licate, car nous ne disposons pas de labels permettant de valider la qualit√© de la segmentation. Dans certains cas, un petit √©chantillon d‚Äôexemples √©tiquet√©s comme anomalies est disponible, ce qui permet une √©valuation cibl√©e du mod√®le entra√Æn√©. Mais dans notre situation, aucune √©tiquette n‚Äôest fournie, ce qui rend l‚Äô√©valuation enti√®rement d√©pendante de crit√®res internes tels que l‚Äô**AIC** (Akaike Information Criterion) et le **BIC** (Bayesian Information Criterion).

En compl√©ment, nous pouvons √©galement calculer les **log-vraisemblances compl√®tes** pour diff√©rents nombres de composantes, afin de visualiser la qualit√© d‚Äôajustement du mod√®le. Enfin, une strat√©gie consiste √† d√©finir un **seuil d‚Äôanomalie** √† partir de la distribution des log-vraisemblances : par exemple, en retenant le **5e percentile**, les observations les moins vraisemblables (c‚Äôest-√†-dire situ√©es dans les 5 % les plus faibles) seront consid√©r√©es comme potentiellement anormales.


```{.python}
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Pr√©paration des donn√©es
X_scaled = scaler.fit_transform(df_continuous)

# Param√®tres
n_components_range = range(1, 6)
aic, bic, log_lik = [], [], []

# Entra√Ænement et √©valuation du mod√®le
for n in n_components_range:
    gmm = GaussianMixture(n_components=n, random_state=RANDOM_STATE)
    gmm.fit(X_scaled)
    aic.append(gmm.aic(X_scaled))
    bic.append(gmm.bic(X_scaled))
    log_lik.append(gmm.score(X_scaled) * len(X_scaled))

# Visualisation
ig, axes = plt.subplots(1, 2, figsize=(12, 6))

# AIC & BIC
axes[0].plot(n_components_range, aic, label='AIC', linestyle='-', marker='o')
axes[0].plot(n_components_range, bic, label='BIC', linestyle='-', marker='s')
axes[0].set_title("AIC & BIC vs. nombre de composantes", fontsize=10)
axes[0].set_xlabel("Nombre de composantes")
axes[0].set_ylabel("Score AIC/BIC")
axes[0].legend()
axes[0].grid(True)

# Log-vraisemblance
axes[1].plot(n_components_range, log_lik, label='Log-vraisemblance', color='green', marker='^')
axes[1].set_title("Log-vraisemblance vs. nombre de composantes", fontsize=10)
axes[1].set_xlabel("Nombre de composantes")
axes[1].set_ylabel("Log-vraisemblance")
axes[1].grid(True)

plt.tight_layout(pad=4)
plt.show()

```

```{r, fig.align='center', echo= FALSE, out.width='95%', out.height='90%'}
#| label: fig-selecting-components
#| fig-cap: Selecting the optimum number of components

knitr::include_graphics('components.png')
```


|       On observe que plus le nombre de composantes augmente, plus les scores d‚ÄôAIC et de BIC diminuent. Cependant, le **BIC** se stabilise √† partir de la troisi√®me composante, tandis que l‚Äô**AIC** continue de diminuer l√©g√®rement jusqu‚Äô√† la cinqui√®me. Cette divergence sugg√®re qu‚Äôau-del√† de trois composantes, le mod√®le pourrait √™tre sujet √† un **surapprentissage**.

Par ailleurs, l‚Äô√©volution de la **log-vraisemblance compl√®te** montre une augmentation nette entre une et trois composantes, suivie d‚Äôune progression beaucoup plus faible au-del√†. Ces observations concordantes justifient le choix d‚Äôun mod√®le avec **trois composantes**, qui repr√©sente un bon compromis entre qualit√© d‚Äôajustement et complexit√©.

```{python, echo=FALSE}
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Pr√©paration des donn√©es
X_scaled = scaler.fit_transform(df_continuous)
```


```{python}
opt_components = 3
# specifying the model
model = GaussianMixture(n_components=opt_components, random_state=RANDOM_STATE)

# ajusting the model to the scaled data
model.fit(X_scaled)

# Probability of belonging to the distribution
log_probs = model.score_samples(X_scaled)

# responsabilities
responsabilities = model.predict_proba(X_scaled)

# anomalies detection : the lowest log probabilities are considered anomalies
threshold = np.percentile(log_probs, 3) # anomaly threshold
anomalies = log_probs < threshold
```

> Le 3·µâ percentile est la valeur en dessous de laquelle se trouvent 3 % des observations dans un ensemble de donn√©es.

> hue dans sns.pairplot ‚Äî Qu‚Äôest-ce que c‚Äôest ?
> L‚Äôargument hue sert √† colorer les points en fonction d‚Äôune variable cat√©gorielle (g√©n√©ralement une √©tiquette ou un groupe). Cela permet de visualiser les diff√©rences entre groupes dans les nuages de points.


```{.python}
from copy import deepcopy
df_continuous_with_anomalies_obs = deepcopy(df_continuous)
df_continuous_with_anomalies_obs['Is anomaly'] = anomalies

sns.pairplot(df_continuous_with_anomalies_obs, hue="Is anomaly", palette={False: "blue", True: "red"}, corner=False, height=2.5) #diag_kind="hist" ou kde pour la courbe
#plt.suptitle('Anomalies detection with a Gaussian Mixture Model')
plt.show()
```

```{r, fig.align='center', echo= FALSE, out.width='95%', out.height='90%'}
#| label: fig-pairplot
#| fig-cap: Anomalies detection with a Gaussian Mixture Model

knitr::include_graphics('pair-plot.png')
```


# üìä Analyse du Pairplot des Anomalies (GMM)

Ce graphique pr√©sente une **visualisation crois√©e des variables continues** du jeu de donn√©es, color√©e selon l‚Äôappartenance √† une anomalie d√©tect√©e par un **mod√®le de m√©lange gaussien (GMM)**.

- üîµ **Bleu** : observations consid√©r√©es comme **normales**
- üî¥ **Rouge** : observations identifi√©es comme **anomalies**


## üéØ Variables analys√©es

- `CustomerAge` (√Çge du client)
- `TransactionAmount` (Montant de la transaction)
- `TransactionDuration` (Dur√©e de la transaction)
- `AccountBalance` (Solde du compte)
- `DaysBetweenThisTransactionAndTheLastOne` (Jours entre deux transactions)


## Lecture du graphique

- La **diagonale** contient les **distributions marginales** estim√©es de chaque variable :
  - En **bleu** : la densit√© des observations normales
  - En **rouge** : la densit√© des anomalies (souvent plus discr√®te car peu nombreuses)

- Les **graphiques hors-diagonale** sont des nuages de points croisant deux variables √† la fois :
  - Les **points rouges** se situent souvent dans des zones de **faible densit√© bleue**, indiquant leur caract√®re atypique dans l‚Äôespace multivari√©.

> üí° **Remarque** : Un point rouge m√©lang√© √† du bleu ne signifie pas une erreur du mod√®le, mais une **anomalie faible**, difficile √† s√©parer par les seules combinaisons bivari√©es. L‚Äôanalyse multidimensionnelle du GMM est ici essentielle.

---

## Interpr√©tation variable par variable

- **1. `TransactionAmount` (Montant de la transaction)**

  - **Distribution** : Asym√©trique √† droite (valeurs √©lev√©es peu fr√©quentes).

  - **Anomalies** :

    - Montants **tr√®s √©lev√©s (> 1 000)** souvent identifi√©s comme atypiques.
  
  - **Hypoth√®se** : Retraits importants ou virements massifs peuvent signaler des comportements inhabituels (fraude, op√©ration exceptionnelle).


- **2. `CustomerAge` (Age du client)**

  - **Distribution** : Potentiellement bimodale (ex. : jeunes adultes et seniors).
  
  - **Anomalies** :
  
    - Clients **tr√®s jeunes (< 18 ans)** ou **tr√®s √¢g√©s (> 75 ans)**.
  
  - **Hypoth√®se** : Ces tranches sont minoritaires et peuvent √™tre li√©es √† des profils atypiques ou vuln√©rables.


- **3. `TransactionDuration` (Dur√©e de la transaction)**

  - **Distribution** : Relativement √©tal√©e.
  
  - **Anomalies** :
  
    - **Tr√®s longues (> 250 s)** ou **tr√®s courtes (< 5 s)**.

  - **Hypoth√®se** : Dur√©es extr√™mes peuvent refl√©ter des probl√®mes techniques ou des manipulations suspectes.


- **4. `AccountBalance` (Solde du compte)**

  - **Distribution** : Concentr√©e vers les faibles soldes, avec une queue √† droite.

  - **Anomalies** :

    - **Soldes tr√®s √©lev√©s (> 12 000)** ou **tr√®s bas (‚âà 0)**.
  
  - **Hypoth√®se** : Les extr√™mes financiers peuvent attirer l‚Äôattention en d√©tection d‚Äôanomalies.


- **5. `DaysBetweenThisTransactionAndTheLastOne` (Jours entre deux transactions)**

  - **Distribution** : Dispers√©e.

  - **Anomalies** :
  
    - P√©riodes **tr√®s courtes (< 100 jours)** ou **tr√®s longues (> 700 jours)**.
  
  - **Hypoth√®se** : Des √©carts extr√™mes dans la fr√©quence peuvent indiquer une activit√© inhabituelle.

---

## Interactions cl√©s entre variables

- **`TransactionAmount` √ó `AccountBalance`**

  - **Zone √† risque** : Montants et soldes simultan√©ment √©lev√©s.
  
  - **Interpr√©tation** : Le retrait de sommes importantes depuis un compte bien rempli peut correspondre √† un comportement rare ou √† surveiller.


- **`TransactionAmount` √ó `TransactionDuration`**

  - Anomalies dans les cas de **montants √©lev√©s + dur√©es longues**.
  
  - **Interpr√©tation** : Transactions longues et co√ªteuses peuvent signaler un traitement manuel, un bug ou une tentative malveillante.


- **`CustomerAge` √ó Autres variables**

  - Moins de patterns nets, mais les **jeunes ou tr√®s √¢g√©s** combin√©s √† **des comportements extr√™mes** (ex. : gros montant ou d√©lai long) ressortent souvent comme anomalies.


---

## Cas particulier visible dans le coin bas-gauche

On observe que des **transactions nulles ou tr√®s faibles (`TransactionAmount` ‚âà 0)** mais avec une **dur√©e de traitement tr√®s longue (`TransactionDuration` > 250 s)** sont fr√©quemment class√©es comme anomalies.

> üß© Cela peut correspondre √† une attente anormale sans transaction effective ‚Äì ce qui peut indiquer une erreur technique, une fraude ou une activit√© suspecte.


---


## Conclusion

|       Le mod√®le `GMM` a permis de mettre en √©vidence des **observations atypiques**, d√©finies comme ayant une **faible probabilit√© d‚Äôappartenance** √† l‚Äôun des groupes dominants dans l‚Äôespace des variables continues.

- **Les anomalies d√©tect√©es refl√®tent :**

- Des **valeurs extr√™mes univari√©es**

- Des **combinaisons de comportements rares**, parfois imperceptibles dans les projections bivari√©es

- **Utilit√© :**

- Surveillance des fraudes

- Ajustement des r√®gles de s√©curit√©

- Compr√©hension des profils inhabituels


> üî¨ **Limite** : Cette analyse repose uniquement sur des variables num√©riques continues. Int√©grer des variables cat√©gorielles ou discr√®tes (ex. : type de transaction, canal utilis√©) permettrait d‚Äôaffiner la d√©tection.


## Annexes

> **`Histogramme :`**

```
|        ‚ñÜ
|        ‚ñÜ    ‚ñÜ
|    ‚ñÜ   ‚ñÜ    ‚ñÜ   ‚ñÜ
|‚ñÜ   ‚ñÜ   ‚ñÜ‚ñÜ  ‚ñÜ‚ñÜ‚ñÜ ‚ñÜ‚ñÜ
+--------------------------> valeur
```

>**`KDE (courbe liss√©e) :`**

```
          /\
         /  \     /\
        /    \   /  \
_______/      \_/    \_____
```
